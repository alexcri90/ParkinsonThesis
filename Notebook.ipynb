{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torchvision in c:\\users\\alexc\\local github\\parkinsonthesis\\.venv\\lib\\site-packages (0.20.1+cu121)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\alexc\\local github\\parkinsonthesis\\.venv\\lib\\site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: numpy in c:\\users\\alexc\\local github\\parkinsonthesis\\.venv\\lib\\site-packages (from torchvision) (2.1.2)\n",
      "Requirement already satisfied: torch==2.5.1+cu121 in c:\\users\\alexc\\local github\\parkinsonthesis\\.venv\\lib\\site-packages (from torchvision) (2.5.1+cu121)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\alexc\\local github\\parkinsonthesis\\.venv\\lib\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\alexc\\local github\\parkinsonthesis\\.venv\\lib\\site-packages (from torch==2.5.1+cu121->torchvision) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\alexc\\local github\\parkinsonthesis\\.venv\\lib\\site-packages (from torch==2.5.1+cu121->torchvision) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\alexc\\local github\\parkinsonthesis\\.venv\\lib\\site-packages (from torch==2.5.1+cu121->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\alexc\\local github\\parkinsonthesis\\.venv\\lib\\site-packages (from torch==2.5.1+cu121->torchvision) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\alexc\\local github\\parkinsonthesis\\.venv\\lib\\site-packages (from torch==2.5.1+cu121->torchvision) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\alexc\\local github\\parkinsonthesis\\.venv\\lib\\site-packages (from torch==2.5.1+cu121->torchvision) (75.3.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\alexc\\local github\\parkinsonthesis\\.venv\\lib\\site-packages (from torch==2.5.1+cu121->torchvision) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\alexc\\local github\\parkinsonthesis\\.venv\\lib\\site-packages (from sympy==1.13.1->torch==2.5.1+cu121->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\alexc\\local github\\parkinsonthesis\\.venv\\lib\\site-packages (from jinja2->torch==2.5.1+cu121->torchvision) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%pip install -r requirements.txt\n",
    "%pip install torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU: NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Test GPU usage\n",
    "import torch\n",
    "\n",
    "def check_gpu():\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA is available. Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        print(\"CUDA is not available. Using CPU.\")\n",
    "\n",
    "check_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Searching for DICOM files in Images_Test\\dicom\n",
      "INFO:__main__:Found 6 DICOM files\n",
      "INFO:__main__:Loaded 6 images\n",
      "INFO:__main__:First image shape: (91, 109, 91)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data loader module for DATSCAN DICOM images.\n",
    "This module provides functionality to load and preprocess DATSCAN images\n",
    "for the Parkinson's Disease analysis project.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple, Union\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pydicom\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class DATSCANMetadata:\n",
    "    \"\"\"Data class to store relevant DICOM metadata.\"\"\"\n",
    "    patient_id: str\n",
    "    exam_date: str\n",
    "    exam_id: str\n",
    "    image_shape: Tuple[int, ...]\n",
    "    pixel_spacing: Optional[Tuple[float, ...]] = None\n",
    "    \n",
    "class DATSCANDataLoader:\n",
    "    \"\"\"Class to handle loading and preprocessing of DATSCAN DICOM images.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_path: Union[str, Path], use_gpu: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize the data loader.\n",
    "        \n",
    "        Args:\n",
    "            base_path: Root directory containing the DATSCAN images\n",
    "            use_gpu: Whether to use GPU for processing (if available)\n",
    "        \"\"\"\n",
    "        self.base_path = Path(base_path)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() and use_gpu else 'cpu')\n",
    "        self.dicom_paths: List[Path] = []\n",
    "        self.metadata: Dict[str, DATSCANMetadata] = {}\n",
    "        \n",
    "    def find_dicom_files(self) -> None:\n",
    "        \"\"\"\n",
    "        Recursively traverse directory structure to find all DICOM files.\n",
    "        Updates self.dicom_paths with found files.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Searching for DICOM files in {self.base_path}\")\n",
    "        try:\n",
    "            for root, _, files in os.walk(self.base_path):\n",
    "                for file in files:\n",
    "                    if file.endswith('.dcm'):\n",
    "                        path = Path(root) / file\n",
    "                        self.dicom_paths.append(path)\n",
    "            \n",
    "            logger.info(f\"Found {len(self.dicom_paths)} DICOM files\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error while searching for DICOM files: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def load_single_dicom(self, path: Path) -> Tuple[np.ndarray, DATSCANMetadata]:\n",
    "        \"\"\"\n",
    "        Load a single DICOM file and extract relevant metadata.\n",
    "        \n",
    "        Args:\n",
    "            path: Path to the DICOM file\n",
    "            \n",
    "        Returns:\n",
    "            Tuple containing:\n",
    "                - np.ndarray: The image data\n",
    "                - DATSCANMetadata: Associated metadata\n",
    "        \"\"\"\n",
    "        try:\n",
    "            dcm = pydicom.dcmread(path)\n",
    "            \n",
    "            # Extract image data\n",
    "            image_data = dcm.pixel_array.astype(float)\n",
    "            \n",
    "            # Extract metadata\n",
    "            patient_id = str(path).split(os.sep)[3]  # Based on folder structure\n",
    "            exam_date = str(path).split(os.sep)[5]   # Based on folder structure\n",
    "            exam_id = str(path).split(os.sep)[6]     # Based on folder structure\n",
    "            \n",
    "            metadata = DATSCANMetadata(\n",
    "                patient_id=patient_id,\n",
    "                exam_date=exam_date,\n",
    "                exam_id=exam_id,\n",
    "                image_shape=image_data.shape,\n",
    "                pixel_spacing=tuple(float(x) for x in dcm.PixelSpacing) if hasattr(dcm, 'PixelSpacing') else None\n",
    "            )\n",
    "            \n",
    "            return image_data, metadata\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading DICOM file {path}: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def load_all_data(self) -> Tuple[List[np.ndarray], Dict[str, DATSCANMetadata]]:\n",
    "        \"\"\"\n",
    "        Load all DICOM files found in the directory structure.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple containing:\n",
    "                - List of image arrays\n",
    "                - Dictionary mapping file paths to metadata\n",
    "        \"\"\"\n",
    "        if not self.dicom_paths:\n",
    "            self.find_dicom_files()\n",
    "            \n",
    "        images = []\n",
    "        metadata_dict = {}\n",
    "        \n",
    "        for path in self.dicom_paths:\n",
    "            try:\n",
    "                image, metadata = self.load_single_dicom(path)\n",
    "                images.append(image)\n",
    "                metadata_dict[str(path)] = metadata\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Skipping file {path} due to error: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "        return images, metadata_dict\n",
    "\n",
    "class DATSCANDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset class for DATSCAN images.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_path: Union[str, Path], transform=None):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        \n",
    "        Args:\n",
    "            base_path: Root directory containing the DATSCAN images\n",
    "            transform: Optional transform to be applied to the images\n",
    "        \"\"\"\n",
    "        self.loader = DATSCANDataLoader(base_path)\n",
    "        self.transform = transform\n",
    "        self.images, self.metadata = self.loader.load_all_data()\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "        \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, DATSCANMetadata]:\n",
    "        \"\"\"\n",
    "        Get a single item from the dataset.\n",
    "        \n",
    "        Args:\n",
    "            idx: Index of the item to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            Tuple containing:\n",
    "                - torch.Tensor: The image data\n",
    "                - DATSCANMetadata: Associated metadata\n",
    "        \"\"\"\n",
    "        image = self.images[idx]\n",
    "        metadata = list(self.metadata.values())[idx]\n",
    "        \n",
    "        # Convert to torch tensor\n",
    "        image_tensor = torch.from_numpy(image).float()\n",
    "        \n",
    "        if self.transform:\n",
    "            image_tensor = self.transform(image_tensor)\n",
    "            \n",
    "        return image_tensor, metadata\n",
    "\n",
    "def main():\n",
    "    \"\"\"Example usage of the data loader.\"\"\"\n",
    "    # Example path - replace with actual path\n",
    "    base_path = Path(\"Images_Test/dicom\")\n",
    "    \n",
    "    # Initialize data loader\n",
    "    loader = DATSCANDataLoader(base_path)\n",
    "    \n",
    "    # Find all DICOM files\n",
    "    loader.find_dicom_files()\n",
    "    \n",
    "    # Load all data\n",
    "    images, metadata = loader.load_all_data()\n",
    "    \n",
    "    # Print summary\n",
    "    logger.info(f\"Loaded {len(images)} images\")\n",
    "    logger.info(f\"First image shape: {images[0].shape}\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Following cell should be run after the data loading cell\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Tuple, Union\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class DATSCANPreprocessor:\n",
    "    \"\"\"Class to handle preprocessing of DATSCAN images.\"\"\"\n",
    "    \n",
    "    def __init__(self, device: Optional[torch.device] = None):\n",
    "        \"\"\"\n",
    "        Initialize the preprocessor.\n",
    "        \n",
    "        Args:\n",
    "            device: torch.device to use for computation\n",
    "        \"\"\"\n",
    "        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def normalize_intensity(self, image: torch.Tensor, method: str = 'z_score') -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Normalize image intensity using specified method.\n",
    "        \n",
    "        Args:\n",
    "            image: Input image tensor\n",
    "            method: Normalization method ('z_score', 'min_max', or 'percentile')\n",
    "            \n",
    "        Returns:\n",
    "            Normalized image tensor\n",
    "        \"\"\"\n",
    "        if method == 'z_score':\n",
    "            # Compute mean and std on flattened tensor\n",
    "            mean = image.mean()\n",
    "            std = image.std()\n",
    "            normalized = (image - mean) / (std + 1e-8)\n",
    "            \n",
    "        elif method == 'min_max':\n",
    "            min_val = image.min()\n",
    "            max_val = image.max()\n",
    "            normalized = (image - min_val) / (max_val - min_val + 1e-8)\n",
    "            \n",
    "        elif method == 'percentile':\n",
    "            # Use percentiles to be more robust to outliers\n",
    "            p_low = torch.quantile(image, 0.01)\n",
    "            p_high = torch.quantile(image, 0.99)\n",
    "            normalized = torch.clamp((image - p_low) / (p_high - p_low + 1e-8), 0, 1)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown normalization method: {method}\")\n",
    "            \n",
    "        return normalized\n",
    "        \n",
    "    def resize_volume(self, image: torch.Tensor, target_size: Tuple[int, ...]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Resize image to target dimensions using trilinear interpolation.\n",
    "        \n",
    "        Args:\n",
    "            image: Input image tensor (should be 3D)\n",
    "            target_size: Desired output size\n",
    "            \n",
    "        Returns:\n",
    "            Resized image tensor\n",
    "        \"\"\"\n",
    "        # Add batch and channel dimensions for F.interpolate\n",
    "        if image.dim() == 3:\n",
    "            image = image.unsqueeze(0).unsqueeze(0)\n",
    "            \n",
    "        resized = F.interpolate(image, size=target_size, mode='trilinear', align_corners=False)\n",
    "        \n",
    "        # Remove batch and channel dimensions\n",
    "        return resized.squeeze(0).squeeze(0)\n",
    "        \n",
    "    def create_brain_mask(self, image: torch.Tensor, threshold: float = 0.1) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Create a binary mask for brain region based on intensity thresholding.\n",
    "        \n",
    "        Args:\n",
    "            image: Input image tensor\n",
    "            threshold: Threshold value for binary mask (relative to max intensity)\n",
    "            \n",
    "        Returns:\n",
    "            Binary mask tensor\n",
    "        \"\"\"\n",
    "        # Normalize image to 0-1 range for thresholding\n",
    "        normalized = self.normalize_intensity(image, method='min_max')\n",
    "        \n",
    "        # Create binary mask\n",
    "        mask = (normalized > threshold).float()\n",
    "        \n",
    "        # Apply morphological operations to clean up the mask\n",
    "        kernel_size = 3\n",
    "        kernel = torch.ones(1, 1, kernel_size, kernel_size, kernel_size).to(self.device)\n",
    "        \n",
    "        # Add dimensions for conv3d\n",
    "        mask = mask.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        # Closing operation (dilation followed by erosion)\n",
    "        mask = F.conv3d(mask, kernel, padding=kernel_size//2) > 0\n",
    "        mask = F.conv3d(mask.float(), kernel, padding=kernel_size//2) >= kernel.sum()\n",
    "        \n",
    "        return mask.squeeze(0).squeeze(0)\n",
    "        \n",
    "    def apply_mask(self, image: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply binary mask to image.\n",
    "        \n",
    "        Args:\n",
    "            image: Input image tensor\n",
    "            mask: Binary mask tensor\n",
    "            \n",
    "        Returns:\n",
    "            Masked image tensor\n",
    "        \"\"\"\n",
    "        return image * mask\n",
    "        \n",
    "    def preprocess_single_image(self, \n",
    "                              image: torch.Tensor,\n",
    "                              target_size: Optional[Tuple[int, ...]] = None,\n",
    "                              normalize_method: str = 'z_score',\n",
    "                              apply_brain_mask: bool = True) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply full preprocessing pipeline to single image.\n",
    "        \n",
    "        Args:\n",
    "            image: Input image tensor\n",
    "            target_size: Optional target size for resizing\n",
    "            normalize_method: Method for intensity normalization\n",
    "            apply_brain_mask: Whether to apply brain masking\n",
    "            \n",
    "        Returns:\n",
    "            Preprocessed image tensor\n",
    "        \"\"\"\n",
    "        # Move to device\n",
    "        image = image.to(self.device)\n",
    "        \n",
    "        # Resize if target size is specified\n",
    "        if target_size is not None:\n",
    "            image = self.resize_volume(image, target_size)\n",
    "            \n",
    "        # Create and apply brain mask if requested\n",
    "        if apply_brain_mask:\n",
    "            mask = self.create_brain_mask(image)\n",
    "            image = self.apply_mask(image, mask)\n",
    "            \n",
    "        # Normalize intensity\n",
    "        image = self.normalize_intensity(image, method=normalize_method)\n",
    "        \n",
    "        return image\n",
    "        \n",
    "    def preprocess_batch(self, \n",
    "                        images: List[torch.Tensor],\n",
    "                        target_size: Optional[Tuple[int, ...]] = None,\n",
    "                        normalize_method: str = 'z_score',\n",
    "                        apply_brain_mask: bool = True) -> List[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Apply preprocessing to a batch of images.\n",
    "        \n",
    "        Args:\n",
    "            images: List of input image tensors\n",
    "            target_size: Optional target size for resizing\n",
    "            normalize_method: Method for intensity normalization\n",
    "            apply_brain_mask: Whether to apply brain masking\n",
    "            \n",
    "        Returns:\n",
    "            List of preprocessed image tensors\n",
    "        \"\"\"\n",
    "        preprocessed_images = []\n",
    "        \n",
    "        for image in tqdm(images, desc=\"Preprocessing images\"):\n",
    "            preprocessed = self.preprocess_single_image(\n",
    "                image,\n",
    "                target_size=target_size,\n",
    "                normalize_method=normalize_method,\n",
    "                apply_brain_mask=apply_brain_mask\n",
    "            )\n",
    "            preprocessed_images.append(preprocessed)\n",
    "            \n",
    "        return preprocessed_images\n",
    "        \n",
    "    def visualize_preprocessing(self, \n",
    "                              original_image: torch.Tensor,\n",
    "                              preprocessed_image: torch.Tensor,\n",
    "                              slice_idx: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        Visualize original and preprocessed images side by side.\n",
    "        \n",
    "        Args:\n",
    "            original_image: Original image tensor\n",
    "            preprocessed_image: Preprocessed image tensor\n",
    "            slice_idx: Index of slice to visualize (if None, uses middle slice)\n",
    "        \"\"\"\n",
    "        if slice_idx is None:\n",
    "            slice_idx = original_image.shape[0] // 2\n",
    "            \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "        \n",
    "        # Plot original image\n",
    "        ax1.imshow(original_image[slice_idx].cpu().numpy(), cmap='gray')\n",
    "        ax1.set_title('Original Image')\n",
    "        ax1.axis('off')\n",
    "        \n",
    "        # Plot preprocessed image\n",
    "        ax2.imshow(preprocessed_image[slice_idx].cpu().numpy(), cmap='gray')\n",
    "        ax2.set_title('Preprocessed Image')\n",
    "        ax2.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Example usage\n",
    "def preprocess_example():\n",
    "    # Assuming we have loaded some images using DATSCANDataLoader\n",
    "    loader = DATSCANDataLoader(base_path)\n",
    "    images, metadata = loader.load_all_data()\n",
    "    \n",
    "    # Convert to torch tensors\n",
    "    image_tensors = [torch.from_numpy(img).float() for img in images]\n",
    "    \n",
    "    # Initialize preprocessor\n",
    "    preprocessor = DATSCANPreprocessor()\n",
    "    \n",
    "    # Define target size (example dimensions)\n",
    "    target_size = (128, 128, 128)\n",
    "    \n",
    "    # Preprocess images\n",
    "    preprocessed_images = preprocessor.preprocess_batch(\n",
    "        image_tensors,\n",
    "        target_size=target_size,\n",
    "        normalize_method='z_score',\n",
    "        apply_brain_mask=True\n",
    "    )\n",
    "    \n",
    "    # Visualize first image before and after preprocessing\n",
    "    preprocessor.visualize_preprocessing(image_tensors[0], preprocessed_images[0])\n",
    "    \n",
    "    return preprocessed_images, metadata\n",
    "\n",
    "# Run example if needed\n",
    "# preprocessed_images, metadata = preprocess_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'base_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m preprocessed_images, metadata \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 219\u001b[0m, in \u001b[0;36mpreprocess_example\u001b[1;34m()\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_example\u001b[39m():\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# Assuming we have loaded some images using DATSCANDataLoader\u001b[39;00m\n\u001b[1;32m--> 219\u001b[0m     loader \u001b[38;5;241m=\u001b[39m DATSCANDataLoader(\u001b[43mbase_path\u001b[49m)\n\u001b[0;32m    220\u001b[0m     images, metadata \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload_all_data()\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# Convert to torch tensors\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'base_path' is not defined"
     ]
    }
   ],
   "source": [
    "preprocessed_images, metadata = preprocess_example()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
