{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pydicom\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import concurrent.futures\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ScanInfo:\n",
    "    \"\"\"Class to store information about each scan\"\"\"\n",
    "    patient_id: str\n",
    "    exam_date: str\n",
    "    file_path: Path\n",
    "    group: str  # 'PD', 'SWEDD', or 'Control'\n",
    "    \n",
    "class DATSCANDataset:\n",
    "    \"\"\"Class to handle DATSCAN image dataset\"\"\"\n",
    "    def __init__(self, base_path: str, device: str = 'cuda'):\n",
    "        self.base_path = Path(base_path)\n",
    "        self.device = device\n",
    "        self.scans: Dict[str, List[ScanInfo]] = {\n",
    "            'PD': [],\n",
    "            'SWEDD': [],\n",
    "            'Control': []\n",
    "        }\n",
    "        \n",
    "    def discover_files(self):\n",
    "        \"\"\"Discover all DICOM files in the directory structure\"\"\"\n",
    "        group_folders = {\n",
    "            'PD': 'PPMI_Images_PD',\n",
    "            'SWEDD': 'PPMI_Images_SWEDD',\n",
    "            'Control': 'PPMI_Images_Cont'\n",
    "        }\n",
    "        \n",
    "        for group, folder in group_folders.items():\n",
    "            group_path = self.base_path / 'Images' / folder\n",
    "            if not group_path.exists():\n",
    "                logger.warning(f\"Group path does not exist: {group_path}\")\n",
    "                continue\n",
    "                \n",
    "            logger.info(f\"Discovering files for group: {group}\")\n",
    "            for patient_folder in tqdm(list(group_path.iterdir()), desc=f\"Processing {group} patients\"):\n",
    "                if not patient_folder.is_dir():\n",
    "                    continue\n",
    "                    \n",
    "                datscan_path = patient_folder / 'Reconstructed_DaTSCAN'\n",
    "                if not datscan_path.exists():\n",
    "                    continue\n",
    "                    \n",
    "                for exam_date_folder in datscan_path.iterdir():\n",
    "                    for exam_id_folder in exam_date_folder.iterdir():\n",
    "                        dicom_files = list(exam_id_folder.glob('*.dcm'))\n",
    "                        for dicom_file in dicom_files:\n",
    "                            scan_info = ScanInfo(\n",
    "                                patient_id=patient_folder.name,\n",
    "                                exam_date=exam_date_folder.name,\n",
    "                                file_path=dicom_file,\n",
    "                                group=group\n",
    "                            )\n",
    "                            self.scans[group].append(scan_info)\n",
    "                            \n",
    "        return self._summarize_dataset()\n",
    "    \n",
    "    def _summarize_dataset(self) -> Dict:\n",
    "        \"\"\"Summarize the discovered dataset\"\"\"\n",
    "        summary = {\n",
    "            'total_scans': sum(len(scans) for scans in self.scans.values()),\n",
    "            'scans_per_group': {group: len(scans) for group, scans in self.scans.items()},\n",
    "            'unique_patients': {\n",
    "                group: len(set(scan.patient_id for scan in scans))\n",
    "                for group, scans in self.scans.items()\n",
    "            }\n",
    "        }\n",
    "        return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Define image loading and preprocessing functions\n",
    "def load_dicom(file_path: Path) -> Optional[np.ndarray]:\n",
    "    \"\"\"Load a DICOM file and return as numpy array\"\"\"\n",
    "    try:\n",
    "        dcm = pydicom.dcmread(str(file_path))\n",
    "        return dcm.pixel_array\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading DICOM file {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def preprocess_image(image: np.ndarray) -> torch.Tensor:\n",
    "    \"\"\"Preprocess the image and convert to tensor\"\"\"\n",
    "    # Normalize to [0,1] range\n",
    "    image = (image - image.min()) / (image.max() - image.min())\n",
    "    \n",
    "    # Convert to tensor and add batch & channel dimensions\n",
    "    tensor = torch.from_numpy(image).float()\n",
    "    if len(tensor.shape) == 2:\n",
    "        tensor = tensor.unsqueeze(0).unsqueeze(0)  # Add batch and channel dims\n",
    "    return tensor\n",
    "\n",
    "class DATSCANLoader:\n",
    "    \"\"\"Class to handle batch loading of DATSCAN images\"\"\"\n",
    "    def __init__(self, dataset: DATSCANDataset, batch_size: int = 32):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def load_group(self, group: str, max_samples: Optional[int] = None) -> torch.Tensor:\n",
    "        \"\"\"Load all images for a specific group\"\"\"\n",
    "        scans = self.dataset.scans[group]\n",
    "        if max_samples:\n",
    "            scans = scans[:max_samples]\n",
    "            \n",
    "        all_images = []\n",
    "        for i in tqdm(range(0, len(scans), self.batch_size), desc=f\"Loading {group} images\"):\n",
    "            batch_scans = scans[i:i + self.batch_size]\n",
    "            batch_images = self._load_batch(batch_scans)\n",
    "            all_images.extend(batch_images)\n",
    "            \n",
    "        return torch.stack(all_images).to(self.dataset.device)\n",
    "    \n",
    "    def _load_batch(self, batch_scans: List[ScanInfo]) -> List[torch.Tensor]:\n",
    "        \"\"\"Load a batch of images in parallel\"\"\"\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            futures = [executor.submit(load_dicom, scan.file_path) for scan in batch_scans]\n",
    "            images = []\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                image = future.result()\n",
    "                if image is not None:\n",
    "                    tensor = preprocess_image(image)\n",
    "                    images.append(tensor)\n",
    "        return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Initialize the dataset and discover files\n",
    "dataset = DATSCANDataset(base_path='.', device='cuda')\n",
    "summary = dataset.discover_files()\n",
    "print(\"Dataset Summary:\")\n",
    "print(f\"Total number of scans: {summary['total_scans']}\")\n",
    "print(\"\\nScans per group:\")\n",
    "for group, count in summary['scans_per_group'].items():\n",
    "    print(f\"{group}: {count} scans\")\n",
    "print(\"\\nUnique patients per group:\")\n",
    "for group, count in summary['unique_patients'].items():\n",
    "    print(f\"{group}: {count} patients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Create the data loader\n",
    "loader = DATSCANLoader(dataset, batch_size=32)  # Adjust batch size based on GPU memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
