{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydicom>=2.4.0 in c:\\users\\alexc\\local github\\parkinsonthesis\\.venv\\lib\\site-packages (from -r requirements.txt (line 9)) (3.0.1)\n",
      "Requirement already satisfied: numpy>=1.24.0 in c:\\users\\alexc\\local github\\parkinsonthesis\\.venv\\lib\\site-packages (from -r requirements.txt (line 10)) (2.1.2)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in c:\\users\\alexc\\local github\\parkinsonthesis\\.venv\\lib\\site-packages (from -r requirements.txt (line 11)) (3.9.2)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\alexc\\local github\\parkinsonthesis\\.venv\\lib\\site-packages (from -r requirements.txt (line 12)) (4.67.0)\n",
      "Requirement already satisfied: pathlib>=1.0.1 in c:\\users\\alexc\\local github\\parkinsonthesis\\.venv\\lib\\site-packages (from -r requirements.txt (line 13)) (1.0.1)\n",
      "Requirement already satisfied: nibabel in c:\\users\\alexc\\local github\\parkinsonthesis\\.venv\\lib\\site-packages (from -r requirements.txt (line 14)) (5.3.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\alexc\\local github\\parkinsonthesis\\.venv\\lib\\site-packages (from -r requirements.txt (line 15)) (1.14.1)\n",
      "Requirement already satisfied: seaborn in c:\\users\\alexc\\local github\\parkinsonthesis\\.venv\\lib\\site-packages (from -r requirements.txt (line 16)) (0.13.2)\n",
      "Collecting scikit-learn (from -r requirements.txt (line 17))\n",
      "  Using cached scikit_learn-1.5.2-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\alexc\\local github\\parkinsonthesis\\.venv\\lib\\site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 11)) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\alexc\\local github\\parkinsonthesis\\.venv\\lib\\site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 11)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\alexc\\local github\\parkinsonthesis\\.venv\\lib\\site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 11)) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\alexc\\local github\\parkinsonthesis\\.venv\\lib\\site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 11)) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\alexc\\local github\\parkinsonthesis\\.venv\\lib\\site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 11)) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\alexc\\local github\\parkinsonthesis\\.venv\\lib\\site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 11)) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\alexc\\local github\\parkinsonthesis\\.venv\\lib\\site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 11)) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\alexc\\local github\\parkinsonthesis\\.venv\\lib\\site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 11)) (2.9.0.post0)\n",
      "Requirement already satisfied: colorama in c:\\users\\alexc\\local github\\parkinsonthesis\\.venv\\lib\\site-packages (from tqdm>=4.65.0->-r requirements.txt (line 12)) (0.4.6)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in c:\\users\\alexc\\local github\\parkinsonthesis\\.venv\\lib\\site-packages (from nibabel->-r requirements.txt (line 14)) (4.12.2)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\alexc\\local github\\parkinsonthesis\\.venv\\lib\\site-packages (from seaborn->-r requirements.txt (line 16)) (2.2.3)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->-r requirements.txt (line 17))\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->-r requirements.txt (line 17))\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\alexc\\local github\\parkinsonthesis\\.venv\\lib\\site-packages (from pandas>=1.2->seaborn->-r requirements.txt (line 16)) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\alexc\\local github\\parkinsonthesis\\.venv\\lib\\site-packages (from pandas>=1.2->seaborn->-r requirements.txt (line 16)) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\alexc\\local github\\parkinsonthesis\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->-r requirements.txt (line 11)) (1.16.0)\n",
      "Using cached scikit_learn-1.5.2-cp312-cp312-win_amd64.whl (11.0 MB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.2 threadpoolctl-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Searching for DICOM files in Images_Test\\dicom\n",
      "INFO:__main__:Found 6 DICOM files\n",
      "INFO:__main__:Loaded 6 images\n",
      "INFO:__main__:First image shape: (91, 109, 91)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data loader module for DATSCAN DICOM images.\n",
    "This module provides functionality to load and preprocess DATSCAN images\n",
    "for the Parkinson's Disease analysis project.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple, Union\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pydicom\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class DATSCANMetadata:\n",
    "    \"\"\"Data class to store relevant DICOM metadata.\"\"\"\n",
    "    patient_id: str\n",
    "    exam_date: str\n",
    "    exam_id: str\n",
    "    image_shape: Tuple[int, ...]\n",
    "    pixel_spacing: Optional[Tuple[float, ...]] = None\n",
    "    \n",
    "class DATSCANDataLoader:\n",
    "    \"\"\"Class to handle loading and preprocessing of DATSCAN DICOM images.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_path: Union[str, Path], use_gpu: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize the data loader.\n",
    "        \n",
    "        Args:\n",
    "            base_path: Root directory containing the DATSCAN images\n",
    "            use_gpu: Whether to use GPU for processing (if available)\n",
    "        \"\"\"\n",
    "        self.base_path = Path(base_path)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() and use_gpu else 'cpu')\n",
    "        self.dicom_paths: List[Path] = []\n",
    "        self.metadata: Dict[str, DATSCANMetadata] = {}\n",
    "        \n",
    "    def find_dicom_files(self) -> None:\n",
    "        \"\"\"\n",
    "        Recursively traverse directory structure to find all DICOM files.\n",
    "        Updates self.dicom_paths with found files.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Searching for DICOM files in {self.base_path}\")\n",
    "        try:\n",
    "            for root, _, files in os.walk(self.base_path):\n",
    "                for file in files:\n",
    "                    if file.endswith('.dcm'):\n",
    "                        path = Path(root) / file\n",
    "                        self.dicom_paths.append(path)\n",
    "            \n",
    "            logger.info(f\"Found {len(self.dicom_paths)} DICOM files\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error while searching for DICOM files: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def load_single_dicom(self, path: Path) -> Tuple[np.ndarray, DATSCANMetadata]:\n",
    "        \"\"\"\n",
    "        Load a single DICOM file and extract relevant metadata.\n",
    "        \n",
    "        Args:\n",
    "            path: Path to the DICOM file\n",
    "            \n",
    "        Returns:\n",
    "            Tuple containing:\n",
    "                - np.ndarray: The image data\n",
    "                - DATSCANMetadata: Associated metadata\n",
    "        \"\"\"\n",
    "        try:\n",
    "            dcm = pydicom.dcmread(path)\n",
    "            \n",
    "            # Extract image data\n",
    "            image_data = dcm.pixel_array.astype(float)\n",
    "            \n",
    "            # Extract metadata\n",
    "            patient_id = str(path).split(os.sep)[3]  # Based on folder structure\n",
    "            exam_date = str(path).split(os.sep)[5]   # Based on folder structure\n",
    "            exam_id = str(path).split(os.sep)[6]     # Based on folder structure\n",
    "            \n",
    "            metadata = DATSCANMetadata(\n",
    "                patient_id=patient_id,\n",
    "                exam_date=exam_date,\n",
    "                exam_id=exam_id,\n",
    "                image_shape=image_data.shape,\n",
    "                pixel_spacing=tuple(float(x) for x in dcm.PixelSpacing) if hasattr(dcm, 'PixelSpacing') else None\n",
    "            )\n",
    "            \n",
    "            return image_data, metadata\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading DICOM file {path}: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def load_all_data(self) -> Tuple[List[np.ndarray], Dict[str, DATSCANMetadata]]:\n",
    "        \"\"\"\n",
    "        Load all DICOM files found in the directory structure.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple containing:\n",
    "                - List of image arrays\n",
    "                - Dictionary mapping file paths to metadata\n",
    "        \"\"\"\n",
    "        if not self.dicom_paths:\n",
    "            self.find_dicom_files()\n",
    "            \n",
    "        images = []\n",
    "        metadata_dict = {}\n",
    "        \n",
    "        for path in self.dicom_paths:\n",
    "            try:\n",
    "                image, metadata = self.load_single_dicom(path)\n",
    "                images.append(image)\n",
    "                metadata_dict[str(path)] = metadata\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Skipping file {path} due to error: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "        return images, metadata_dict\n",
    "\n",
    "class DATSCANDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset class for DATSCAN images.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_path: Union[str, Path], transform=None):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        \n",
    "        Args:\n",
    "            base_path: Root directory containing the DATSCAN images\n",
    "            transform: Optional transform to be applied to the images\n",
    "        \"\"\"\n",
    "        self.loader = DATSCANDataLoader(base_path)\n",
    "        self.transform = transform\n",
    "        self.images, self.metadata = self.loader.load_all_data()\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "        \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, DATSCANMetadata]:\n",
    "        \"\"\"\n",
    "        Get a single item from the dataset.\n",
    "        \n",
    "        Args:\n",
    "            idx: Index of the item to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            Tuple containing:\n",
    "                - torch.Tensor: The image data\n",
    "                - DATSCANMetadata: Associated metadata\n",
    "        \"\"\"\n",
    "        image = self.images[idx]\n",
    "        metadata = list(self.metadata.values())[idx]\n",
    "        \n",
    "        # Convert to torch tensor\n",
    "        image_tensor = torch.from_numpy(image).float()\n",
    "        \n",
    "        if self.transform:\n",
    "            image_tensor = self.transform(image_tensor)\n",
    "            \n",
    "        return image_tensor, metadata\n",
    "\n",
    "def main():\n",
    "    \"\"\"Example usage of the data loader.\"\"\"\n",
    "    # Example path - replace with actual path\n",
    "    base_path = Path(\"Images_Test/dicom\")\n",
    "    \n",
    "    # Initialize data loader\n",
    "    loader = DATSCANDataLoader(base_path)\n",
    "    \n",
    "    # Find all DICOM files\n",
    "    loader.find_dicom_files()\n",
    "    \n",
    "    # Load all data\n",
    "    images, metadata = loader.load_all_data()\n",
    "    \n",
    "    # Print summary\n",
    "    logger.info(f\"Loaded {len(images)} images\")\n",
    "    logger.info(f\"First image shape: {images[0].shape}\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Following cell should be run after the data loading cell\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Tuple, Union\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class DATSCANPreprocessor:\n",
    "    \"\"\"Class to handle preprocessing of DATSCAN images.\"\"\"\n",
    "    \n",
    "    def __init__(self, device: Optional[torch.device] = None):\n",
    "        \"\"\"\n",
    "        Initialize the preprocessor.\n",
    "        \n",
    "        Args:\n",
    "            device: torch.device to use for computation\n",
    "        \"\"\"\n",
    "        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def normalize_intensity(self, image: torch.Tensor, method: str = 'z_score') -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Normalize image intensity using specified method.\n",
    "        \n",
    "        Args:\n",
    "            image: Input image tensor\n",
    "            method: Normalization method ('z_score', 'min_max', or 'percentile')\n",
    "            \n",
    "        Returns:\n",
    "            Normalized image tensor\n",
    "        \"\"\"\n",
    "        if method == 'z_score':\n",
    "            # Compute mean and std on flattened tensor\n",
    "            mean = image.mean()\n",
    "            std = image.std()\n",
    "            normalized = (image - mean) / (std + 1e-8)\n",
    "            \n",
    "        elif method == 'min_max':\n",
    "            min_val = image.min()\n",
    "            max_val = image.max()\n",
    "            normalized = (image - min_val) / (max_val - min_val + 1e-8)\n",
    "            \n",
    "        elif method == 'percentile':\n",
    "            # Use percentiles to be more robust to outliers\n",
    "            p_low = torch.quantile(image, 0.01)\n",
    "            p_high = torch.quantile(image, 0.99)\n",
    "            normalized = torch.clamp((image - p_low) / (p_high - p_low + 1e-8), 0, 1)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown normalization method: {method}\")\n",
    "            \n",
    "        return normalized\n",
    "        \n",
    "    def resize_volume(self, image: torch.Tensor, target_size: Tuple[int, ...]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Resize image to target dimensions using trilinear interpolation.\n",
    "        \n",
    "        Args:\n",
    "            image: Input image tensor (should be 3D)\n",
    "            target_size: Desired output size\n",
    "            \n",
    "        Returns:\n",
    "            Resized image tensor\n",
    "        \"\"\"\n",
    "        # Add batch and channel dimensions for F.interpolate\n",
    "        if image.dim() == 3:\n",
    "            image = image.unsqueeze(0).unsqueeze(0)\n",
    "            \n",
    "        resized = F.interpolate(image, size=target_size, mode='trilinear', align_corners=False)\n",
    "        \n",
    "        # Remove batch and channel dimensions\n",
    "        return resized.squeeze(0).squeeze(0)\n",
    "        \n",
    "    def create_brain_mask(self, image: torch.Tensor, threshold: float = 0.1) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Create a binary mask for brain region based on intensity thresholding.\n",
    "        \n",
    "        Args:\n",
    "            image: Input image tensor\n",
    "            threshold: Threshold value for binary mask (relative to max intensity)\n",
    "            \n",
    "        Returns:\n",
    "            Binary mask tensor\n",
    "        \"\"\"\n",
    "        # Normalize image to 0-1 range for thresholding\n",
    "        normalized = self.normalize_intensity(image, method='min_max')\n",
    "        \n",
    "        # Create binary mask\n",
    "        mask = (normalized > threshold).float()\n",
    "        \n",
    "        # Apply morphological operations to clean up the mask\n",
    "        kernel_size = 3\n",
    "        kernel = torch.ones(1, 1, kernel_size, kernel_size, kernel_size).to(self.device)\n",
    "        \n",
    "        # Add dimensions for conv3d\n",
    "        mask = mask.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        # Closing operation (dilation followed by erosion)\n",
    "        mask = F.conv3d(mask, kernel, padding=kernel_size//2) > 0\n",
    "        mask = F.conv3d(mask.float(), kernel, padding=kernel_size//2) >= kernel.sum()\n",
    "        \n",
    "        return mask.squeeze(0).squeeze(0)\n",
    "        \n",
    "    def apply_mask(self, image: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply binary mask to image.\n",
    "        \n",
    "        Args:\n",
    "            image: Input image tensor\n",
    "            mask: Binary mask tensor\n",
    "            \n",
    "        Returns:\n",
    "            Masked image tensor\n",
    "        \"\"\"\n",
    "        return image * mask\n",
    "        \n",
    "    def preprocess_single_image(self, \n",
    "                              image: torch.Tensor,\n",
    "                              target_size: Optional[Tuple[int, ...]] = None,\n",
    "                              normalize_method: str = 'z_score',\n",
    "                              apply_brain_mask: bool = True) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply full preprocessing pipeline to single image.\n",
    "        \n",
    "        Args:\n",
    "            image: Input image tensor\n",
    "            target_size: Optional target size for resizing\n",
    "            normalize_method: Method for intensity normalization\n",
    "            apply_brain_mask: Whether to apply brain masking\n",
    "            \n",
    "        Returns:\n",
    "            Preprocessed image tensor\n",
    "        \"\"\"\n",
    "        # Move to device\n",
    "        image = image.to(self.device)\n",
    "        \n",
    "        # Resize if target size is specified\n",
    "        if target_size is not None:\n",
    "            image = self.resize_volume(image, target_size)\n",
    "            \n",
    "        # Create and apply brain mask if requested\n",
    "        if apply_brain_mask:\n",
    "            mask = self.create_brain_mask(image)\n",
    "            image = self.apply_mask(image, mask)\n",
    "            \n",
    "        # Normalize intensity\n",
    "        image = self.normalize_intensity(image, method=normalize_method)\n",
    "        \n",
    "        return image\n",
    "        \n",
    "    def preprocess_batch(self, \n",
    "                        images: List[torch.Tensor],\n",
    "                        target_size: Optional[Tuple[int, ...]] = None,\n",
    "                        normalize_method: str = 'z_score',\n",
    "                        apply_brain_mask: bool = True) -> List[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Apply preprocessing to a batch of images.\n",
    "        \n",
    "        Args:\n",
    "            images: List of input image tensors\n",
    "            target_size: Optional target size for resizing\n",
    "            normalize_method: Method for intensity normalization\n",
    "            apply_brain_mask: Whether to apply brain masking\n",
    "            \n",
    "        Returns:\n",
    "            List of preprocessed image tensors\n",
    "        \"\"\"\n",
    "        preprocessed_images = []\n",
    "        \n",
    "        for image in tqdm(images, desc=\"Preprocessing images\"):\n",
    "            preprocessed = self.preprocess_single_image(\n",
    "                image,\n",
    "                target_size=target_size,\n",
    "                normalize_method=normalize_method,\n",
    "                apply_brain_mask=apply_brain_mask\n",
    "            )\n",
    "            preprocessed_images.append(preprocessed)\n",
    "            \n",
    "        return preprocessed_images\n",
    "        \n",
    "    def visualize_preprocessing(self, \n",
    "                              original_image: torch.Tensor,\n",
    "                              preprocessed_image: torch.Tensor,\n",
    "                              slice_idx: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        Visualize original and preprocessed images side by side.\n",
    "        \n",
    "        Args:\n",
    "            original_image: Original image tensor\n",
    "            preprocessed_image: Preprocessed image tensor\n",
    "            slice_idx: Index of slice to visualize (if None, uses middle slice)\n",
    "        \"\"\"\n",
    "        if slice_idx is None:\n",
    "            slice_idx = original_image.shape[0] // 2\n",
    "            \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "        \n",
    "        # Plot original image\n",
    "        ax1.imshow(original_image[slice_idx].cpu().numpy(), cmap='gray')\n",
    "        ax1.set_title('Original Image')\n",
    "        ax1.axis('off')\n",
    "        \n",
    "        # Plot preprocessed image\n",
    "        ax2.imshow(preprocessed_image[slice_idx].cpu().numpy(), cmap='gray')\n",
    "        ax2.set_title('Preprocessed Image')\n",
    "        ax2.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Example usage\n",
    "def preprocess_example():\n",
    "    # Assuming we have loaded some images using DATSCANDataLoader\n",
    "    loader = DATSCANDataLoader(base_path)\n",
    "    images, metadata = loader.load_all_data()\n",
    "    \n",
    "    # Convert to torch tensors\n",
    "    image_tensors = [torch.from_numpy(img).float() for img in images]\n",
    "    \n",
    "    # Initialize preprocessor\n",
    "    preprocessor = DATSCANPreprocessor()\n",
    "    \n",
    "    # Define target size (example dimensions)\n",
    "    target_size = (128, 128, 128)\n",
    "    \n",
    "    # Preprocess images\n",
    "    preprocessed_images = preprocessor.preprocess_batch(\n",
    "        image_tensors,\n",
    "        target_size=target_size,\n",
    "        normalize_method='z_score',\n",
    "        apply_brain_mask=True\n",
    "    )\n",
    "    \n",
    "    # Visualize first image before and after preprocessing\n",
    "    preprocessor.visualize_preprocessing(image_tensors[0], preprocessed_images[0])\n",
    "    \n",
    "    return preprocessed_images, metadata\n",
    "\n",
    "# Run example if needed\n",
    "# preprocessed_images, metadata = preprocess_example()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
