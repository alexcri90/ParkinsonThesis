# -*- coding: utf-8 -*-
"""notebook_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15OQ56Cnrz-XQlk7i8ACCQsHnGoFf3htw
"""

# %pip install pandas matplotlib seaborn torch transformers scikit-learn tqdm
# %pip install ipywidgets --upgrade

"""# Install dependencies"""

# Cell 1: Project Setup and Imports
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# PyTorch and CUDA setup
import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader

# Hugging Face Transformers for the pre-trained model and tokenizer
from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup

# Scikit-learn for train/test split and label encoding
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# For progress bar during training
from tqdm.notebook import tqdm

# Check if CUDA is available and select device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

"""# Dataset Loading"""

# Cell 2: Load Dataset
# Load the dataset (make sure data.csv is in the working directory)
# If you encounter delimiter issues, adjust the 'sep' parameter accordingly.
df = pd.read_csv("data.csv", sep=";")

# In Cell 2 (after reading the CSV)
print("Columns in DataFrame:", df.columns.tolist())


# Display basic info about the dataset
print("Dataset shape:", df.shape)
print("\nFirst 5 rows:")
display(df.head())

# Check for missing values
print("\nMissing values by column:")
print(df.isnull().sum())

"""# Tagging"""

# Cell 3: Remap Tags Using Tag Hierarchy and Report Unknown Tags
tag_hierarchy = {
    # NO_COMMENTO
    'NO COMMENTO': ('NO_COMMENTO', 'NO_COMMENTO'),
    'NESSUN COMMENTO': ('NO_COMMENTO', 'NO_COMMENTO'),
    'TUTTO OK': ('NO_COMMENTO', 'TUTTO_OK'),
    'tutto ok': ('NO_COMMENTO', 'TUTTO_OK'),

    # PROBLEMI_TECNICI
    'SITO E APP': ('PROBLEMI_TECNICI', 'SITO_E_APP'),
    'SITO E APP/ ALERT': ('PROBLEMI_TECNICI', 'SITO_E_APP'),
    'BLOCCO SISTEMI': ('PROBLEMI_TECNICI', 'SISTEMI_DOWN'),
    'DOWN SISTEMI': ('PROBLEMI_TECNICI', 'SISTEMI_DOWN'),
    'TOOL DI GESTIONE': ('PROBLEMI_TECNICI', 'SISTEMI_DOWN'),
    'SMS E MAIL': ('PROBLEMI_TECNICI', 'SMS'),
    'SMS ARRIVO IN SEDE': ('PROBLEMI_TECNICI', 'SMS'),
    'DOSSIER': ('PROBLEMI_TECNICI', 'DOSSIER'),
    'ACCOGLIENZA E TOTEM': ('PROBLEMI_TECNICI', 'TOTEM'),

    # TEMPI_E_ATTESE
    'TEMPI DI ATTESA VISITA': ('TEMPI_E_ATTESE', 'ATTESA_VISITA'),
    'TEMPI DI ATTESA VISITE': ('TEMPI_E_ATTESE', 'ATTESA_VISITA'),
    'TEMPI DI ATTESA': ('TEMPI_E_ATTESE', 'ATTESA_VISITA'),
    'TEMPI DI ATTESA VISITA\n': ('TEMPI_E_ATTESE', 'ATTESA_VISITA'),
    'DISPONIBILITÀ SLOT APP.TI': ('TEMPI_E_ATTESE', 'DISPONIBILITA_SLOT'),
    'TEMPI PRESA IN CARICO APP.TI': ('TEMPI_E_ATTESE', 'DISPONIBILITA_SLOT'),
    'TEMPI DI ATTESA VISITA\r\n': ('TEMPI_E_ATTESE', 'ATTESA_VISITA'),
    'DISPONIBILIT� SLOT APP.TI': ('TEMPI_E_ATTESE', 'DISPONIBILITA_SLOT'),

    # PAGAMENTI_E_TARIFFE
    'AUMENTO TARIFFE': ('PAGAMENTI_E_TARIFFE', 'VARIAZIONI_PREZZO'),
    'PREZZO VARIABILE/AUMENTI': ('PAGAMENTI_E_TARIFFE', 'VARIAZIONI_PREZZO'),
    'MODALITA PAGAMENTO': ('PAGAMENTI_E_TARIFFE', 'MODALITA_PAGAMENTO'),
    "MODALITA' DI PAGAMENTO": ('PAGAMENTI_E_TARIFFE', 'MODALITA_PAGAMENTO'),
    'PAGAMENTO ANTICIPATO': ('PAGAMENTI_E_TARIFFE', 'MODALITA_PAGAMENTO'),
    'PAGAMENTO DOPO VISITA': ('PAGAMENTI_E_TARIFFE', 'MODALITA_PAGAMENTO'),
    'CHIAREZZA PREZZI': ('PAGAMENTI_E_TARIFFE', 'VARIAZIONI_PREZZO'),
    'CHIAREZZA PREZZI/PRESTAZIONI AGGIUNTIVE': ('PAGAMENTI_E_TARIFFE', 'VARIAZIONI_PREZZO'),
    '5 EURO': ('PAGAMENTI_E_TARIFFE', 'SMART'),
    '5 EURO/EMPATIA DESK': ('PAGAMENTI_E_TARIFFE', 'SMART'),
    'PAZIENTE SMART': ('PAGAMENTI_E_TARIFFE', 'SMART'),

    # STRUTTURA_E_LOGISTICA
    'PARCHEGGIO': ('STRUTTURA_E_LOGISTICA', 'ACCESSIBILITA'),
    'PARCHEGGI': ('STRUTTURA_E_LOGISTICA', 'ACCESSIBILITA'),
    'SPAZI INTERNI': ('STRUTTURA_E_LOGISTICA', 'SPAZI_INTERNI'),
    'SPAZI INTERNI/ARREDI': ('STRUTTURA_E_LOGISTICA', 'SPAZI_INTERNI'),
    'PRIVACY': ('STRUTTURA_E_LOGISTICA', 'SPAZI_INTERNI'),
    'PRIVACY ACCETTAZIONE': ('STRUTTURA_E_LOGISTICA', 'SPAZI_INTERNI'),
    'MANUTENZIONE': ('STRUTTURA_E_LOGISTICA', 'SPAZI_INTERNI'),
    'MANUTENZIONE/PULIZIA': ('STRUTTURA_E_LOGISTICA', 'SPAZI_INTERNI'),
    'PULIZIA LOCALI': ('STRUTTURA_E_LOGISTICA', 'SPAZI_INTERNI'),
    'TEMPERATURA': ('STRUTTURA_E_LOGISTICA', 'SPAZI_INTERNI'),
    'TEMPERATURA AMBULATORI': ('STRUTTURA_E_LOGISTICA', 'SPAZI_INTERNI'),
    'LOCATION': ('STRUTTURA_E_LOGISTICA', 'ACCESSIBILITA'),
    'ACCESSIBILITA': ('STRUTTURA_E_LOGISTICA', 'ACCESSIBILITA'),
    'INDICAZIONI': ('STRUTTURA_E_LOGISTICA', 'ACCESSIBILITA'),
    'INDICAZIONI AMBULATORIO/ACCESSO SEDE': ('STRUTTURA_E_LOGISTICA', 'ACCESSIBILITA'),

    # SERVIZIO_CLIENTI
    'SERVICE CENTER': ('SERVIZIO_CLIENTI', 'SERVICE_CENTER'),
    'IVR': ('SERVIZIO_CLIENTI', 'SERVICE_CENTER'),
    'EMPATIA DESK': ('SERVIZIO_CLIENTI', 'DESK'),
    'DESK': ('SERVIZIO_CLIENTI', 'DESK'),
    'INFO ERRATE DESK': ('SERVIZIO_CLIENTI', 'DESK'),
    'QUALITA INFO DESK': ('SERVIZIO_CLIENTI', 'DESK'),
    'CONTATTO DIRETTO SEDE': ('SERVIZIO_CLIENTI', 'DESK'),
    'CONTATTO DIRETTO SEDE\n': ('SERVIZIO_CLIENTI', 'DESK'),
    'CONTATTO DIRETTO SEDE\r\n': ('SERVIZIO_CLIENTI', 'DESK'),
    'GESTIONE RECLAMI': ('SERVIZIO_CLIENTI', 'RECLAMI'),
    'RECLAMO': ('SERVIZIO_CLIENTI', 'RECLAMI'),
    'ACCETTAZIONE TEMPI/PROCEDURE': ('SERVIZIO_CLIENTI', 'DESK'),
    'CANCELLAZIONE APPUNTAMENTO': ('SERVIZIO_CLIENTI', 'SERVICE_CENTER'),
    'CANCELLAZIONE /VARIAZIONE APPUNTAMENTO': ('SERVIZIO_CLIENTI', 'SERVICE_CENTER'),
    'POLICY DI CANCELLAZIONE': ('SERVIZIO_CLIENTI', 'SERVICE_CENTER'),
    'POLICY CANCELLAZIONE': ('SERVIZIO_CLIENTI', 'SERVICE_CENTER'),
    'PRENOTAZIONE': ('SERVIZIO_CLIENTI', 'SERVICE_CENTER'),
    'PRENOTAZIONI MULTIPLE': ('SERVIZIO_CLIENTI', 'SERVICE_CENTER'),
    'PRENOTAZIONI FAMILIARI': ('SERVIZIO_CLIENTI', 'SERVICE_CENTER'),
    'ACCETTAZIONE CICLICHE': ('SERVIZIO_CLIENTI', 'SERVICE_CENTER'),
    'PRENOTAZIONE/ACCETTAZIONE CICLICHE': ('SERVIZIO_CLIENTI', 'SERVICE_CENTER'),

    # DOCUMENTAZIONE
    'CD': ('DOCUMENTAZIONE', 'CD'),
    'STAMPA CD': ('DOCUMENTAZIONE', 'CD'),
    'cd': ('DOCUMENTAZIONE', 'CD'),
    'FATTURAZIONE': ('DOCUMENTAZIONE', 'DOCUMENTI_FATTURE_REFERTO'),
    'INVIO MAIL FATTURE/REFERTI': ('DOCUMENTAZIONE', 'DOCUMENTI_FATTURE_REFERTO'),
    'STRUTTURA REFERTO': ('DOCUMENTAZIONE', 'DOCUMENTI_FATTURE_REFERTO'),
    'RICHIESTA GIUSTIFICATIVO': ('DOCUMENTAZIONE', 'DOCUMENTI_FATTURE_REFERTO'),
    'TEMPI REFERTAZIONE': ('DOCUMENTAZIONE', 'DOCUMENTI_FATTURE_REFERTO'),

    # QUALITA_SERVIZIO_MEDICO
    'QUALITA CLINICA': ('QUALITA_SERVIZIO_MEDICO', 'QUALITA_CLINICA'),
    'QUALITÀ CLINICA': ('QUALITA_SERVIZIO_MEDICO', 'QUALITA_CLINICA'),
    'QUALITA CLINICA\n': ('QUALITA_SERVIZIO_MEDICO', 'QUALITA_CLINICA'),
    'QUALIT� CLINICA': ('QUALITA_SERVIZIO_MEDICO', 'QUALITA_CLINICA'),
    'QUALITA CLINICA\r\n': ('QUALITA_SERVIZIO_MEDICO', 'QUALITA_CLINICA'),
    'EMPATIA MEDICO': ('QUALITA_SERVIZIO_MEDICO', 'RELAZIONE_MEDICO_PAZIENTE'),
    'MEDICO': ('QUALITA_SERVIZIO_MEDICO', 'RELAZIONE_MEDICO_PAZIENTE'),
    'MEDICO DONNA': ('QUALITA_SERVIZIO_MEDICO', 'RELAZIONE_MEDICO_PAZIENTE'),
    'OFFERTA CLINICA': ('QUALITA_SERVIZIO_MEDICO', 'QUALITA_CLINICA'),
    'STRUMENTAZIONE/ELETTROMEDICALI': ('QUALITA_SERVIZIO_MEDICO', 'QUALITA_CLINICA'),
    'STRUMENTAZIONE ELETTROMEDICALI': ('QUALITA_SERVIZIO_MEDICO', 'QUALITA_CLINICA'),
    'LAB': ('QUALITA_SERVIZIO_MEDICO', 'QUALITA_CLINICA'),
    'FISIO': ('QUALITA_SERVIZIO_MEDICO', 'QUALITA_CLINICA'),
    'VIDEOCONSULTO': ('QUALITA_SERVIZIO_MEDICO', 'QUALITA_CLINICA'),
    'VISITE DI CONTROLLO': ('QUALITA_SERVIZIO_MEDICO', 'QUALITA_CLINICA'),
    'CONTATTO CON MEDICO': ('QUALITA_SERVIZIO_MEDICO', 'RELAZIONE_MEDICO_PAZIENTE'),
    'PRESCRIZIONE FARMACO': ('QUALITA_SERVIZIO_MEDICO', 'QUALITA_CLINICA'),
    'SERVIZIO OTTICO': ('QUALITA_SERVIZIO_MEDICO', 'QUALITA_CLINICA'),
    'DURATA VISITA': ('QUALITA_SERVIZIO_MEDICO', 'TEMPI_VISITA'),
    'VISITA BREVE': ('QUALITA_SERVIZIO_MEDICO', 'TEMPI_VISITA'),

    # ASSICURAZIONI_E_CONVENZIONI
    'ASSICURAZIONI': ('ASSICURAZIONI_E_CONVENZIONI', 'ASSICURAZIONI_E_CONVENZIONI'),
    'PAGAMENTI IN CONVENZIONE': ('ASSICURAZIONI_E_CONVENZIONI', 'ASSICURAZIONI_E_CONVENZIONI'),

    # ALTRO
    'ALTRO': ('ALTRO', 'ALTRO'),
    'VARIE': ('ALTRO', 'ALTRO'),
    'SUGGERIMENTI': ('ALTRO', 'ALTRO'),
    'TEST': ('ALTRO', 'ALTRO'),
    'FUSIONE': ('ALTRO', 'ALTRO'),
    'NUOVE APERTURE': ('ALTRO', 'ALTRO'),
    'FORMAZIONE TOOL MEDICO': ('ALTRO', 'ALTRO'),
    'INFO INCOMPLETE': ('ALTRO', 'ALTRO'),
    'WIFI': ('ALTRO', 'ALTRO'),
    'CONVENZIONE SSN': ('ALTRO', 'SSN'),
    'RICETTE SSN': ('ALTRO', 'SSN'),
    'SSN': ('ALTRO', 'SSN'),
    'WI FI SEDE': ('ALTRO', 'ALTRO')
}

def map_tag(tag):
    # If the tag is not found in the dictionary, mark it as unknown.
    if tag in tag_hierarchy:
        return tag_hierarchy[tag]
    else:
        return ("UNKNOWN", "UNKNOWN")

# Apply mapping to create new columns "Macrocategory" and "Category"
df[['Macrocategory', 'Category']] = df['Tag'].apply(lambda t: pd.Series(map_tag(t)))
print("Unique Macrocategories:", df['Macrocategory'].unique())
print("Unique Categories:", df['Category'].unique())

# **New Edit 1:** Report any unknown tags that were not mapped.
unknown_tags = df.loc[df['Macrocategory'] == "UNKNOWN", "Tag"].unique()
if len(unknown_tags) > 0:
    print("\nUnknown tags detected. Please remap these tags as needed:")
    print(unknown_tags)
else:
    print("\nNo unknown tags detected.")

"""# EDA"""

# Cell 4: Dataset Exploration and Visualization
plt.figure(figsize=(12, 5))

# Count plot for Macrocategory
plt.subplot(1, 2, 1)
sns.countplot(data=df, x="Macrocategory", order=sorted(df["Macrocategory"].unique()))
plt.xticks(rotation=45)
plt.title("Distribution of Macrocategories")

# Count plot for Category
plt.subplot(1, 2, 2)
sns.countplot(data=df, x="Category", order=sorted(df["Category"].unique()))
plt.xticks(rotation=45)
plt.title("Distribution of Categories")
plt.tight_layout()
plt.show()

# Create and plot a heatmap for the cross-tabulation (includes all rows)
cross_tab = pd.crosstab(df['Macrocategory'], df['Category'])
plt.figure(figsize=(12, 6))
sns.heatmap(cross_tab, annot=True, fmt="d", cmap="YlGnBu")
plt.title("Heatmap of Macrocategory vs. Category (All Data)")
plt.show()

# Cell 5: Filtered Heatmap Excluding "NO_COMMENTO" and "TUTTO_OK"
# Filter out rows where Macrocategory is "NO_COMMENTO" or Category is "TUTTO_OK"
filtered_df = df[(df["Macrocategory"] != "NO_COMMENTO") & (df["Category"] != "TUTTO_OK")]
plt.figure(figsize=(12, 6))
cross_tab_filtered = pd.crosstab(filtered_df['Macrocategory'], filtered_df['Category'])
sns.heatmap(cross_tab_filtered, annot=True, fmt="d", cmap="YlGnBu")
plt.title("Filtered Heatmap: Macrocategory vs. Category (Excluding 'NO_COMMENTO' and 'TUTTO_OK')")
plt.show()

"""# Preprocessing"""

# Cell 6: Text Preprocessing and Train-Test Split

# (For Transformer models, minimal cleaning is needed. We simply ensure that the text is a string.)
df["Consigli per migliorarci"] = df["Consigli per migliorarci"].astype(str)

# Split the dataset (80% training, 20% validation)
train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df["Macrocategory"])
print("Training set shape:", train_df.shape)
print("Validation set shape:", val_df.shape)

# Create label encoders for Macrocategory and Category
macro_le = LabelEncoder()
cat_le = LabelEncoder()

# Fit on the training set
train_macro_labels = macro_le.fit_transform(train_df["Macrocategory"])
train_cat_labels = cat_le.fit_transform(train_df["Category"])

# Also transform the validation set labels
val_macro_labels = macro_le.transform(val_df["Macrocategory"])
val_cat_labels = cat_le.transform(val_df["Category"])

print("Encoded Macrocategories:", list(macro_le.classes_))
print("Encoded Categories:", list(cat_le.classes_))

# --- Prepare allowed categories mapping for coherence enforcement at inference ---
# We create a mapping from macro label (as string) to a set of allowed category labels (as strings).
macro_to_allowed = {}
for _, row in df.iterrows():
    macro = row["Macrocategory"]
    cat = row["Category"]
    if macro not in macro_to_allowed:
        macro_to_allowed[macro] = set()
    macro_to_allowed[macro].add(cat)

# Convert allowed category names to indices using the cat_le encoder.
allowed_indices = {}
for macro, cats in macro_to_allowed.items():
    if macro in macro_le.classes_:
        macro_idx = np.where(macro_le.classes_ == macro)[0][0]
        allowed_indices[macro_idx] = [cat_le.transform([c])[0] for c in cats]
print("\nAllowed category indices per Macrocategory index:")
print(allowed_indices)

# Cell 7: Prepare Dataset and DataLoader for Transformer

# Choose a pre-trained Italian model; you can change the model name if desired.
MODEL_NAME = "dbmdz/bert-base-italian-uncased"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

class ReviewsDataset(Dataset):
    def __init__(self, texts, macro_labels, cat_labels, tokenizer, max_length=128):
        self.texts = texts
        self.macro_labels = macro_labels
        self.cat_labels = cat_labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts.iloc[idx])
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_length,
            truncation=True,
            padding="max_length",
            return_attention_mask=True,
            return_tensors="pt"
        )
        item = {key: val.squeeze(0) for key, val in encoding.items()}
        item["macro_label"] = torch.tensor(self.macro_labels[idx], dtype=torch.long)
        item["cat_label"] = torch.tensor(self.cat_labels[idx], dtype=torch.long)
        return item

# Create Dataset objects for train and validation sets
train_dataset = ReviewsDataset(train_df["Consigli per migliorarci"].reset_index(drop=True),
                               train_macro_labels,
                               train_cat_labels,
                               tokenizer,
                               max_length=128)
val_dataset = ReviewsDataset(val_df["Consigli per migliorarci"].reset_index(drop=True),
                             val_macro_labels,
                             val_cat_labels,
                             tokenizer,
                             max_length=128)

# Create DataLoaders
BATCH_SIZE = 16  # You can tweak this parameter
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)

"""# Model parameters"""

# Cell 8: Define the Multi-Head Transformer Model

class MultiHeadClassifier(nn.Module):
    def __init__(self, model_name, num_macro_labels, num_cat_labels, dropout_prob=0.3):
        super(MultiHeadClassifier, self).__init__()
        self.transformer = AutoModel.from_pretrained(model_name)
        hidden_size = self.transformer.config.hidden_size

        # Shared dropout layer
        self.dropout = nn.Dropout(dropout_prob)

        # Classification head for Macrocategory
        self.macro_classifier = nn.Linear(hidden_size, num_macro_labels)

        # Classification head for Category
        self.cat_classifier = nn.Linear(hidden_size, num_cat_labels)

    def forward(self, input_ids, attention_mask):
        # Get outputs from transformer; use the [CLS] token representation
        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)
        cls_output = outputs.last_hidden_state[:, 0, :]  # (batch_size, hidden_size)
        cls_output = self.dropout(cls_output)

        macro_logits = self.macro_classifier(cls_output)
        cat_logits = self.cat_classifier(cls_output)

        return macro_logits, cat_logits

# Determine number of labels from our encoders
num_macro_labels = len(macro_le.classes_)
num_cat_labels = len(cat_le.classes_)
print(f"Number of Macro Labels: {num_macro_labels}")
print(f"Number of Category Labels: {num_cat_labels}")

# Instantiate the model and move it to the device (GPU if available)
model = MultiHeadClassifier(MODEL_NAME, num_macro_labels, num_cat_labels, dropout_prob=0.3)
model.to(device)

# Cell 9: Training Setup and Hyper-Parameters

# Hyperparameters (feel free to tweak these)
EPOCHS = 4
LEARNING_RATE = 2e-5
TOTAL_STEPS = len(train_loader) * EPOCHS

# Define loss function (we sum the losses from both heads)
criterion = nn.CrossEntropyLoss()

# Optimizer: We use AdamW (commonly used with Transformers)
optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)

# Learning rate scheduler
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1 * TOTAL_STEPS),
                                            num_training_steps=TOTAL_STEPS)

"""# Training Loop"""

# Cell 10: Training Loop and Evaluation

# To track metrics
train_loss_values = []
val_loss_values = []
macro_acc_values = []
cat_acc_values = []

def calculate_accuracy(logits, labels):
    preds = torch.argmax(logits, dim=1)
    correct = (preds == labels).sum().item()
    return correct / len(labels)

# Training loop
for epoch in range(EPOCHS):
    print(f"\nEpoch {epoch+1}/{EPOCHS}")
    model.train()
    total_loss = 0

    progress_bar = tqdm(train_loader, desc="Training", leave=False)
    for batch in progress_bar:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        macro_labels = batch["macro_label"].to(device)
        cat_labels = batch["cat_label"].to(device)

        optimizer.zero_grad()
        macro_logits, cat_logits = model(input_ids, attention_mask)

        # Compute loss for both heads
        loss_macro = criterion(macro_logits, macro_labels)
        loss_cat = criterion(cat_logits, cat_labels)
        loss = loss_macro + loss_cat

        loss.backward()
        optimizer.step()
        scheduler.step()

        total_loss += loss.item()
        progress_bar.set_postfix(loss=loss.item())

    avg_train_loss = total_loss / len(train_loader)
    train_loss_values.append(avg_train_loss)
    print(f"Average training loss: {avg_train_loss:.4f}")

    # Evaluation on validation set
    model.eval()
    total_val_loss = 0
    macro_correct = 0
    cat_correct = 0
    total_samples = 0

    with torch.no_grad():
        for batch in val_loader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            macro_labels = batch["macro_label"].to(device)
            cat_labels = batch["cat_label"].to(device)

            macro_logits, cat_logits = model(input_ids, attention_mask)
            loss_macro = criterion(macro_logits, macro_labels)
            loss_cat = criterion(cat_logits, cat_labels)
            loss = loss_macro + loss_cat
            total_val_loss += loss.item()

            # Compute accuracies (we enforce coherence during inference)
            batch_size = input_ids.size(0)
            for i in range(batch_size):
                # Macro prediction
                macro_logit = macro_logits[i].unsqueeze(0)
                pred_macro = torch.argmax(macro_logit, dim=1).item()

                # Category logits for sample i and allowed indices for the predicted macro
                cat_logit = cat_logits[i]
                allowed = allowed_indices.get(pred_macro, list(range(num_cat_labels)))
                # Select only allowed logits
                allowed_logits = cat_logit[allowed]
                # Get the index within allowed list with highest score
                idx_in_allowed = torch.argmax(allowed_logits).item()
                # Map back to the actual category index
                pred_cat = allowed[idx_in_allowed]

                # Compare to ground truth
                if pred_macro == macro_labels[i].item():
                    macro_correct += 1
                if pred_cat == cat_labels[i].item():
                    cat_correct += 1
                total_samples += 1

    avg_val_loss = total_val_loss / len(val_loader)
    val_loss_values.append(avg_val_loss)
    macro_acc = macro_correct / total_samples
    cat_acc = cat_correct / total_samples
    macro_acc_values.append(macro_acc)
    cat_acc_values.append(cat_acc)
    print(f"Validation Loss: {avg_val_loss:.4f} | Macro Accuracy: {macro_acc:.4f} | Category Accuracy: {cat_acc:.4f}")

# Cell 10: Save the Model and Metrics

# Define a dictionary to save model state, tokenizer, label encoders (as classes_) and training metrics.
save_dict = {
    "model_state_dict": model.state_dict(),
    "optimizer_state_dict": optimizer.state_dict(),
    "train_loss": train_loss_values,
    "val_loss": val_loss_values,
    "macro_acc": macro_acc_values,
    "cat_acc": cat_acc_values,
    "macro_classes": macro_le.classes_,
    "cat_classes": cat_le.classes_,
    "allowed_indices": allowed_indices,
    "model_name": MODEL_NAME
}

MODEL_SAVE_PATH = "multitask_model.pth"
torch.save(save_dict, MODEL_SAVE_PATH)
print(f"Model and metrics saved to {MODEL_SAVE_PATH}")

# Cell 11: Inference Example and Prediction Function

def predict_review(review_text, model, tokenizer, macro_le, cat_le, allowed_indices, max_length=128):
    """
    Given a review text, predict the Macrocategory and Category.
    The function first predicts the Macrocategory and then restricts the Category prediction to the allowed set.
    """
    model.eval()
    encoding = tokenizer.encode_plus(
        review_text,
        add_special_tokens=True,
        max_length=max_length,
        truncation=True,
        padding="max_length",
        return_attention_mask=True,
        return_tensors="pt"
    )
    input_ids = encoding["input_ids"].to(device)
    attention_mask = encoding["attention_mask"].to(device)

    with torch.no_grad():
        macro_logits, cat_logits = model(input_ids, attention_mask)

    # Predict Macrocategory
    pred_macro_idx = torch.argmax(macro_logits, dim=1).item()
    pred_macro = macro_le.classes_[pred_macro_idx]

    # Restrict Category logits to allowed indices for the predicted macro
    allowed = allowed_indices.get(pred_macro_idx, list(range(len(cat_le.classes_))))
    allowed_logits = cat_logits[0][allowed]
    idx_in_allowed = torch.argmax(allowed_logits).item()
    pred_cat_idx = allowed[idx_in_allowed]
    pred_cat = cat_le.classes_[pred_cat_idx]

    return pred_macro, pred_cat

# Example usage:
example_review = "Ho provato a prendere il bigliettino ma il macchinario non funzionava."
pred_macro, pred_cat = predict_review(example_review, model, tokenizer, macro_le, cat_le, allowed_indices)
print(f"Review: {example_review}")
print(f"Predicted Macrocategory: {pred_macro}")
print(f"Predicted Category: {pred_cat}")

