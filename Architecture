<architecture>
	<introduction>
        The objective of this project is to validate and test various unsupervised models for latent space representation of Dopamine Transporter Imaging (DATSCAN) in Parkinson's Disease. By leveraging unsupervised learning techniques, the goal is to uncover intrinsic patterns and features within the imaging data that could enhance the understanding of Parkinson's Disease progression.
	</introduction>
	<data_understanding>
        <data_sources>
            <source_1>
                <name> Parkinson's Progression Markers Initiative (PPMI) </name>
                <description> The PPMI dataset is a large, multicenter study that aims to identify biomarkers of Parkinson's Disease progression. </description>
                <link> https://www.ppmi-info.org/ </link>
            </source_1>
        <dataset_description>
            <dicom_structure>
                The dataset is organized in a nested structure, as follows:
                - **Patient ID Folder**: Unique numerical code for each patient.
                - **Reconstructed_DaTSCAN Folder**: Just a folder named "Reconstructed_DaTSCAN Folder".
                - **Exam Date Folders**: Multiple exams per patient, formatted as `YYYY_MM_DD_hh_mm_ss.v` (Year, Month, Day, Hours, Minutes, Seconds, Version).
                - **Exam Identifier Folder**: Named with `I<exam_code>`, e.g., `I123456`.
                - **DICOM File**: The image file, named like `S<some_numbers>_I<some_numbers>.dcm`.
            </dicom_structure>
            <example_path>
                Images_Test/dicom/3000/Reconstructed_DaTSCAN/2011-01-20_16_28_47.0/I323662/PPMI_3000_NM_Reconstructed_DaTSCAN_Br_20120814154829508_1_S117534_I323662.dcm
            </example_path>
        </dataset_description>
    </data_understanding>
	<ingestion_and_preprocessing>
        <data_loading>
            Develop a script to recursively traverse the nested folder structure to locate all DICOM files. Use libraries like pydicom to read DICOM files into NumPy arrays.
        </data_loading>
        <data_preprocessing>
            - Intensity Normalization: Normalize pixel intensities to a common scale to account for scanner differences.
            - Voxel Alignment: Ensure all images have consistent voxel dimensions.
            - Masking: Apply brain masks if necessary to focus on regions of interest.
            - Data Augmentation: Although images are centered and well-aligned, consider augmentations like slight rotations or noise addition if beneficial.
        </data_preprocessing>
    </ingestion_and_preprocessing>
    <eda>
        <statistical_analysis>
            - Descriptive Statistics: Compute mean, median, standard deviation, and other summary statistics for the dataset.
            - Distribution Analysis: Examine the distribution of pixel intensities and voxel values.
            - Correlation Analysis: Investigate relationships between different regions of the brain.
        </statistical_analysis>
        <visualization>
            - Image Visualization: Display sample images to understand the data quality and variability.
            - Histograms: Plot histograms of pixel intensities to identify common patterns.
            - Heatmaps: Generate heatmaps to visualize voxel correlations and patterns.
        </visualization>
    </eda>
    <model>
        <selection>
            <unsupervised_models>
                - Autoencoders: Train deep autoencoders to learn latent representations of the imaging data.
                - Variational Autoencoders (VAEs): Utilize VAEs to model the underlying distribution of the data.
                - Generative Adversarial Networks (GANs): Explore Deep Convolutional GANs for generating synthetic images and learning latent features.
                - Principal Component Analysis (PCA): Apply PCA for dimensionality reduction and feature extraction.
                - t-Distributed Stochastic Neighbor Embedding (t-SNE): Use t-SNE for visualizing high-dimensional data in 2D or 3D space.
                - UMAP: Employ Uniform Manifold Approximation and Projection (UMAP) for nonlinear dimensionality reduction.
            </unsupervised_models>
            <model_architecture>
                - Input Layer: Match the input dimensions to the image size and number of channels.
                - Encoder: Comprise convolutional layers to extract features and reduce dimensionality.
                - Latent Space: A bottleneck layer where the latent representation is learned.
                - Decoder: Mirror the encoder architecture to reconstruct the input from the latent space.
        </selection>
        <training>
            - Training Setup: Use frameworks like TensorFlow or PyTorch for model implementation. Utilize GPUs for computational efficiency.
            - Hyperparameters: Experiment with different latent space dimensions. Adjust learning rates, batch sizes, and activation functions.
            - Optimization: Use optimizers like Adam or RMSprop. Implement regularization techniques (dropout, weight decay) to prevent overfitting.
            - Training Loop: Incorporate checkpoints to save model states. Use early stopping based on validation loss to avoid overfitting.
        </training>
        <evaluation>
            - Reconstruction Error: Measure reconstruction loss (e.g., Mean Squared Error) between input and output images.
            - Latent Space Analysis: Visualize latent spaces using dimensionality reduction techniques like t-SNE or PCA. Check for clustering patterns that may correlate with patient metadata (e.g., disease stage).
            - Comparison Metrics: Evaluate models based on reconstruction quality and latent space interpretability. Use metrics like Structural Similarity Index (SSIM) for image quality assessment.
        </evaluation>
    </model>
    <workflow_pipeline>
        <data_pipeline>
            Step 1: Data loading and preprocessing scripts to prepare the dataset.
            Step 2: EDA notebooks to understand data characteristics.
        </data_pipeline>
        <model_pipeline>
            Step 3: Define model architectures in modular code.
            Step 4: Training scripts with configurable parameters.
            Step 5: Evaluation scripts to compute metrics and generate visualizations.
        </model_pipeline>
        <results_pipeline>
            Step 6: Aggregation of results for comparison across models.
            Step 7: Reporting tools to document findings.
        </results_pipeline>
    </workflow_pipeline>
</architecture>