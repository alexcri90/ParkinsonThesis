{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install dependencies\n",
    "# %pip install scikit-learn scikit-image SimpleITK nibabel nilearn albumentations seaborn pandas numpy matplotlib tqdm pydicom scipy umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 4070 Ti\n",
      "Total GPU Memory: 12.88 GB\n",
      "GPU Memory Allocated: 0.00 GB\n",
      "GPU Memory Reserved: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import necessary libraries and setup environment\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from umap import UMAP\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# GPU Setup\n",
    "def configure_gpu():\n",
    "    \"\"\"Configure GPU settings optimized for NVIDIA 4070Ti\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        # Enable CUDNN benchmark for optimized convolution performance\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        # Print GPU info\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(device)}\")\n",
    "        print(f\"Total GPU Memory: {torch.cuda.get_device_properties(device).total_memory / 1e9:.2f} GB\")\n",
    "    else:\n",
    "        raise EnvironmentError(\"CUDA-compatible GPU not found. Please check your GPU configuration.\")\n",
    "    return device\n",
    "\n",
    "# Memory monitoring\n",
    "def print_gpu_stats():\n",
    "    \"\"\"Print current GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1e9  # Convert to GB\n",
    "        reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        print(f\"GPU Memory Allocated: {allocated:.2f} GB\")\n",
    "        print(f\"GPU Memory Reserved: {reserved:.2f} GB\")\n",
    "\n",
    "# Configure device\n",
    "device = configure_gpu()\n",
    "print_gpu_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2a: Model Definitions\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Base Convolutional Block\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Memory-efficient convolutional block.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(OrderedDict([\n",
    "            ('conv', nn.Conv3d(in_channels, out_channels, kernel_size, stride, padding)),\n",
    "            ('bn', nn.BatchNorm3d(out_channels)),\n",
    "            ('relu', nn.ReLU(inplace=True))\n",
    "        ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "# Unsupervised Autoencoder\n",
    "class BaseAutoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim=256):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, skip_connections = self.encoder(x)\n",
    "        reconstruction = self.decoder(z, skip_connections)\n",
    "        return reconstruction\n",
    "\n",
    "    def encode(self, x):\n",
    "        z, _ = self.encoder(x)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        batch_size = z.size(0)\n",
    "        device = z.device\n",
    "        dummy_skips = (\n",
    "            torch.zeros(batch_size, 32, 64, 64, 64, device=device),\n",
    "            torch.zeros(batch_size, 64, 32, 32, 32, device=device),\n",
    "            torch.zeros(batch_size, 128, 16, 16, 16, device=device),\n",
    "            torch.zeros(batch_size, 256, 8, 8, 8, device=device)\n",
    "        )\n",
    "        return self.decoder(z, dummy_skips)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim=256):\n",
    "        super().__init__()\n",
    "        self.init_conv = ConvBlock(1, 16)\n",
    "        self.down1 = nn.Sequential(\n",
    "            ConvBlock(16, 32, stride=2),\n",
    "            ConvBlock(32, 32)\n",
    "        )\n",
    "        self.down2 = nn.Sequential(\n",
    "            ConvBlock(32, 64, stride=2),\n",
    "            ConvBlock(64, 64)\n",
    "        )\n",
    "        self.down3 = nn.Sequential(\n",
    "            ConvBlock(64, 128, stride=2),\n",
    "            ConvBlock(128, 128)\n",
    "        )\n",
    "        self.down4 = nn.Sequential(\n",
    "            ConvBlock(128, 256, stride=2),\n",
    "            ConvBlock(256, 256)\n",
    "        )\n",
    "        self.flatten_size = 256 * 8 * 8 * 8\n",
    "        self.fc = nn.Linear(self.flatten_size, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.init_conv(x)\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        flat = torch.flatten(d4, start_dim=1)\n",
    "        z = self.fc(flat)\n",
    "        return z, (d1, d2, d3, d4)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim=256):\n",
    "        super().__init__()\n",
    "        self.flatten_size = 256 * 8 * 8 * 8\n",
    "        self.fc = nn.Linear(latent_dim, self.flatten_size)\n",
    "        self.up1 = nn.Sequential(\n",
    "            nn.ConvTranspose3d(256, 128, kernel_size=2, stride=2),\n",
    "            ConvBlock(128, 128)\n",
    "        )\n",
    "        self.up2 = nn.Sequential(\n",
    "            nn.ConvTranspose3d(128, 64, kernel_size=2, stride=2),\n",
    "            ConvBlock(64, 64)\n",
    "        )\n",
    "        self.up3 = nn.Sequential(\n",
    "            nn.ConvTranspose3d(64, 32, kernel_size=2, stride=2),\n",
    "            ConvBlock(32, 32)\n",
    "        )\n",
    "        self.up4 = nn.Sequential(\n",
    "            nn.ConvTranspose3d(32, 16, kernel_size=2, stride=2),\n",
    "            ConvBlock(16, 16)\n",
    "        )\n",
    "        self.final_conv = nn.Conv3d(16, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, z, skip_connections):\n",
    "        x = self.fc(z)\n",
    "        x = x.view(-1, 256, 8, 8, 8)\n",
    "        d1, d2, d3, d4 = skip_connections\n",
    "        x = self.up1(x + d4)\n",
    "        x = self.up2(x + d3)\n",
    "        x = self.up3(x + d2)\n",
    "        x = self.up4(x + d1)\n",
    "        x = torch.sigmoid(self.final_conv(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2b: VAE and Semi-Supervised Models\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=256):\n",
    "        super().__init__()\n",
    "        self.encoder = VAEEncoder(latent_dim)\n",
    "        self.decoder = VAEDecoder(latent_dim)\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * log_var)\n",
    "            eps = torch.randn_like(std)\n",
    "            return mu + eps * std\n",
    "        return mu\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, log_var, skip_connections = self.encoder(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        reconstruction = self.decoder(z, skip_connections)\n",
    "        return reconstruction, mu, log_var\n",
    "\n",
    "class VAEEncoder(nn.Module):\n",
    "    def __init__(self, latent_dim=256):\n",
    "        super().__init__()\n",
    "        self.init_conv = ConvBlock(1, 16)\n",
    "        self.down1 = nn.Sequential(\n",
    "            ConvBlock(16, 32, stride=2),\n",
    "            ConvBlock(32, 32)\n",
    "        )\n",
    "        self.down2 = nn.Sequential(\n",
    "            ConvBlock(32, 64, stride=2),\n",
    "            ConvBlock(64, 64)\n",
    "        )\n",
    "        self.down3 = nn.Sequential(\n",
    "            ConvBlock(64, 128, stride=2),\n",
    "            ConvBlock(128, 128)\n",
    "        )\n",
    "        self.down4 = nn.Sequential(\n",
    "            ConvBlock(128, 256, stride=2),\n",
    "            ConvBlock(256, 256)\n",
    "        )\n",
    "        self.flatten_size = 256 * 8 * 8 * 8\n",
    "        self.fc_mu = nn.Linear(self.flatten_size, latent_dim)\n",
    "        self.fc_var = nn.Linear(self.flatten_size, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.init_conv(x)\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        flat = torch.flatten(d4, start_dim=1)\n",
    "        mu = self.fc_mu(flat)\n",
    "        log_var = self.fc_var(flat)\n",
    "        return mu, log_var, (d1, d2, d3, d4)\n",
    "\n",
    "class VAEDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim=256):\n",
    "        super().__init__()\n",
    "        self.flatten_size = 256 * 8 * 8 * 8\n",
    "        self.fc = nn.Linear(latent_dim, self.flatten_size)\n",
    "        self.up1 = nn.Sequential(\n",
    "            nn.ConvTranspose3d(256, 128, kernel_size=2, stride=2),\n",
    "            ConvBlock(128, 128)\n",
    "        )\n",
    "        self.up2 = nn.Sequential(\n",
    "            nn.ConvTranspose3d(128, 64, kernel_size=2, stride=2),\n",
    "            ConvBlock(64, 64)\n",
    "        )\n",
    "        self.up3 = nn.Sequential(\n",
    "            nn.ConvTranspose3d(64, 32, kernel_size=2, stride=2),\n",
    "            ConvBlock(32, 32)\n",
    "        )\n",
    "        self.up4 = nn.Sequential(\n",
    "            nn.ConvTranspose3d(32, 16, kernel_size=2, stride=2),\n",
    "            ConvBlock(16, 16)\n",
    "        )\n",
    "        self.final_conv = nn.Conv3d(16, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, z, skip_connections):\n",
    "        x = self.fc(z)\n",
    "        x = x.view(-1, 256, 8, 8, 8)\n",
    "        d1, d2, d3, d4 = skip_connections\n",
    "        x = self.up1(x + d4)\n",
    "        x = self.up2(x + d3)\n",
    "        x = self.up3(x + d2)\n",
    "        x = self.up4(x + d1)\n",
    "        x = torch.sigmoid(self.final_conv(x))\n",
    "        return x\n",
    "\n",
    "# Semi-Supervised Autoencoder\n",
    "class SemiSupervisedAE(nn.Module):\n",
    "    def __init__(self, latent_dim=256, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, skip_connections = self.encoder(x)\n",
    "        reconstruction = self.decoder(z, skip_connections)\n",
    "        classification = self.classifier(z)\n",
    "        return reconstruction, classification, z\n",
    "\n",
    "# Semi-Supervised VAE\n",
    "class SSVAE(nn.Module):\n",
    "    def __init__(self, latent_dim=256, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.encoder = VAEEncoder(latent_dim)\n",
    "        self.decoder = VAEDecoder(latent_dim)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * log_var)\n",
    "            eps = torch.randn_like(std)\n",
    "            return mu + eps * std\n",
    "        return mu\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, log_var, skip_connections = self.encoder(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        reconstruction = self.decoder(z, skip_connections)\n",
    "        classification = self.classifier(z)\n",
    "        return reconstruction, classification, mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load models and dataset\n",
    "\n",
    "class ModelLoader:\n",
    "    \"\"\"Handles loading of pretrained models\"\"\"\n",
    "    def __init__(self, checkpoint_dir='checkpoints'):\n",
    "        self.checkpoint_dir = Path(checkpoint_dir)\n",
    "        self.model_configs = {\n",
    "            'autoencoder': (BaseAutoencoder, 'autoencoder_checkpoint.pth'),\n",
    "            'vae': (VAE, 'vae_checkpoint.pth'),\n",
    "            'ssae': (SemiSupervisedAE, 'ssae_checkpoint.pth'),\n",
    "            'ssvae': (SSVAE, 'ssvae_checkpoint.pth')\n",
    "        }\n",
    "\n",
    "    def load_model(self, model_name):\n",
    "        \"\"\"Load a specific model from checkpoint\"\"\"\n",
    "        if model_name not in self.model_configs:\n",
    "            raise ValueError(f\"Unknown model: {model_name}\")\n",
    "\n",
    "        ModelClass, checkpoint_file = self.model_configs[model_name]\n",
    "        checkpoint_path = self.checkpoint_dir / checkpoint_file\n",
    "\n",
    "        if not checkpoint_path.exists():\n",
    "            raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "\n",
    "        # Initialize model\n",
    "        model = ModelClass()\n",
    "        \n",
    "        # Load checkpoint\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def load_all_models(self):\n",
    "        \"\"\"Load all models\"\"\"\n",
    "        models = {}\n",
    "        for model_name in self.model_configs:\n",
    "            print(f\"Loading {model_name}...\")\n",
    "            models[model_name] = self.load_model(model_name)\n",
    "            print_gpu_stats()\n",
    "        return models\n",
    "\n",
    "# Load validation dataset\n",
    "def load_validation_data(batch_size=8):\n",
    "    \"\"\"Load validation dataset from validated_file_paths.csv\"\"\"\n",
    "    df = pd.read_csv('validated_file_paths.csv')\n",
    "    val_dataset = DaTScanDataset(df)  # Your existing dataset class\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    return val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2c: Dataset Class and Loading Functions\n",
    "import pydicom\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def load_dicom(file_path):\n",
    "    \"\"\"Load and process a DICOM file\"\"\"\n",
    "    try:\n",
    "        ds = pydicom.dcmread(file_path)\n",
    "        pixel_array = ds.pixel_array.astype(np.float32)\n",
    "        \n",
    "        # Apply rescaling if attributes are present\n",
    "        if hasattr(ds, 'RescaleSlope') and hasattr(ds, 'RescaleIntercept'):\n",
    "            slope = ds.RescaleSlope\n",
    "            intercept = ds.RescaleIntercept\n",
    "            pixel_array = pixel_array * slope + intercept\n",
    "            \n",
    "        return pixel_array, ds\n",
    "    except Exception as e:\n",
    "        raise IOError(f\"Error reading DICOM file {file_path}: {e}\")\n",
    "\n",
    "def process_volume(volume, target_shape=(128, 128, 128)):\n",
    "    \"\"\"Process a 3D volume (normalize, resize, mask)\"\"\"\n",
    "    # Normalize intensity\n",
    "    volume = np.clip(volume, a_min=0, a_max=None)\n",
    "    vmin, vmax = volume.min(), volume.max()\n",
    "    if vmax > vmin:\n",
    "        volume = (volume - vmin) / (vmax - vmin)\n",
    "    else:\n",
    "        volume = volume - vmin\n",
    "\n",
    "    # Resize to target shape if needed\n",
    "    if volume.shape != target_shape:\n",
    "        # Your resizing logic here (maintaining aspect ratio)\n",
    "        pass\n",
    "\n",
    "    return volume\n",
    "\n",
    "class DaTScanDataset(Dataset):\n",
    "    \"\"\"Dataset class for DaTSCAN images\"\"\"\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe\n",
    "        self._calculate_dataset_statistics()\n",
    "\n",
    "    def _calculate_dataset_statistics(self):\n",
    "        \"\"\"Calculate dataset statistics\"\"\"\n",
    "        stats_list = []\n",
    "        for _, row in self.df.iterrows():\n",
    "            try:\n",
    "                volume, _ = load_dicom(row[\"file_path\"])\n",
    "                stats_list.append({\n",
    "                    'min': volume.min(),\n",
    "                    'max': volume.max()\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {row['file_path']}: {e}\")\n",
    "\n",
    "        if stats_list:\n",
    "            self.stats = {\n",
    "                'min': min(stat['min'] for stat in stats_list),\n",
    "                'max': max(stat['max'] for stat in stats_list)\n",
    "            }\n",
    "        else:\n",
    "            self.stats = {'min': 0, 'max': 1}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            file_path = self.df.iloc[idx][\"file_path\"]\n",
    "            \n",
    "            # Load DICOM\n",
    "            volume, _ = load_dicom(file_path)\n",
    "            \n",
    "            # Process volume\n",
    "            processed_vol = process_volume(volume)\n",
    "            \n",
    "            # Convert to tensor\n",
    "            volume_tensor = torch.from_numpy(np.expand_dims(processed_vol, axis=0)).float()\n",
    "\n",
    "            return {\n",
    "                \"volume\": volume_tensor,\n",
    "                \"label\": self.df.iloc[idx][\"label\"],\n",
    "                \"path\": file_path\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading file {file_path}: {str(e)}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading autoencoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexc\\AppData\\Local\\Temp\\ipykernel_31148\\397766712.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory Allocated: 1.70 GB\n",
      "GPU Memory Reserved: 2.80 GB\n",
      "Loading vae...\n",
      "GPU Memory Allocated: 2.12 GB\n",
      "GPU Memory Reserved: 3.48 GB\n",
      "Loading ssae...\n",
      "GPU Memory Allocated: 2.41 GB\n",
      "GPU Memory Reserved: 3.48 GB\n",
      "Loading ssvae...\n",
      "GPU Memory Allocated: 2.83 GB\n",
      "GPU Memory Reserved: 4.16 GB\n",
      "\n",
      "Extracting latent vectors for autoencoder...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff5f82a60ac147ddbe028f725cadc917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/374 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 3: Latent Space Extraction\n",
    "class LatentSpaceExtractor:\n",
    "    \"\"\"Extracts and manages latent representations from models\"\"\"\n",
    "    def __init__(self, models, val_loader):\n",
    "        self.models = models\n",
    "        self.val_loader = val_loader\n",
    "        self.latent_vectors = {}\n",
    "        self.labels = None\n",
    "\n",
    "    def extract_latent_vectors(self, batch_size=8):\n",
    "        \"\"\"Extract latent vectors from all models\"\"\"\n",
    "        for model_name, model in self.models.items():\n",
    "            print(f\"\\nExtracting latent vectors for {model_name}...\")\n",
    "            vectors = []\n",
    "            labels = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(self.val_loader):\n",
    "                    volumes = batch['volume'].to(device)\n",
    "                    batch_labels = batch['label']\n",
    "\n",
    "                    # Extract latent vectors based on model type\n",
    "                    if isinstance(model, (VAE, SSVAE)):\n",
    "                        _, _, mu, _ = model(volumes)\n",
    "                        vectors.append(mu.cpu().numpy())\n",
    "                    else:  # Autoencoder or Semi-supervised AE\n",
    "                        if isinstance(model, SemiSupervisedAE):\n",
    "                            _, _, z = model(volumes)\n",
    "                        else:\n",
    "                            z = model.encode(volumes)\n",
    "                        vectors.append(z.cpu().numpy())\n",
    "\n",
    "                    labels.extend(batch_labels)\n",
    "\n",
    "                    # Clean up GPU memory\n",
    "                    del volumes\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "            self.latent_vectors[model_name] = np.concatenate(vectors)\n",
    "            if self.labels is None:\n",
    "                self.labels = np.array(labels)\n",
    "\n",
    "            print_gpu_stats()\n",
    "\n",
    "        return self.latent_vectors, self.labels\n",
    "\n",
    "# Example usage:\n",
    "model_loader = ModelLoader()\n",
    "models = model_loader.load_all_models()\n",
    "val_loader = load_validation_data()\n",
    "extractor = LatentSpaceExtractor(models, val_loader)\n",
    "latent_vectors, labels = extractor.extract_latent_vectors()\n",
    "\n",
    "print(\"\\nExtraction complete! Latent vectors shape for each model:\")\n",
    "for model_name, vectors in latent_vectors.items():\n",
    "    print(f\"{model_name}: {vectors.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
