# -*- coding: utf-8 -*-
"""Project_Notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zkbb1EDlzyb3Th4K0IOr--4MKPeUN-z5

# Setup
"""

# Cell 1: Install dependencies
# Uncomment and run the following command if dependencies are not already installed.
# !pip install scikit-learn scikit-image SimpleITK nibabel nilearn albumentations seaborn pandas numpy matplotlib tqdm pydicom scipy
# !pip install umap-learn
#!pip install torch torchvision torchaudio
#!pip install pandas
# !pip install pydicom

# Cell 2: Import statements and environment setup
import torch

def configure_gpu():
    """
    Configures GPU settings:
    - Detects CUDA device
    - Enables CUDNN benchmarking for improved performance on NVIDIA 4070Ti
    """
    if torch.cuda.is_available():
        device = torch.device("cuda")
        # Enable CUDNN benchmark for optimized convolution algorithm selection
        torch.backends.cudnn.benchmark = True
        print(f"Using GPU: {torch.cuda.get_device_name(device)}")
    else:
        raise EnvironmentError("CUDA-compatible GPU not found. Please check your GPU configuration.")

def print_gpu_memory_stats():
    """
    Prints current GPU memory usage for monitoring.
    """
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / (1024 ** 2)
        reserved = torch.cuda.memory_reserved() / (1024 ** 2)
        print(f"GPU Memory Allocated: {allocated:.2f} MB")
        print(f"GPU Memory Reserved: {reserved:.2f} MB")
    else:
        print("CUDA not available.")

# Configure GPU on startup
configure_gpu()
print_gpu_memory_stats()

import warnings
warnings.filterwarnings('ignore')

# Cell 3: GPU Setup and Memory Management
import os
import logging
import warnings
import pandas as pd

# Configure logging for quality assurance (logs will be written to data_ingestion.log)
logging.basicConfig(level=logging.INFO, filename="data_ingestion.log", filemode="w",
                    format="%(asctime)s - %(levelname)s - %(message)s")

def collect_files(base_dir):
    """
    Recursively collects DICOM files only from the expected folders:
    - PPMI_Images_PD: Label "PD"
    - PPMI_Images_SWEDD: Label "SWEDD"
    - PPMI_Images_Cont: Label "Control"

    Excludes any file containing "br_raw" in its path and logs all skipped folders.

    :param base_dir: Base directory containing the Images folder.
    :return: (included_files, excluded_files)
             included_files: list of tuples (full_path, label)
             excluded_files: list of file paths that were excluded.
    """
    included_files = []
    excluded_files = []

    # Define the expected folders and corresponding labels
    expected_folders = {
        "PPMI_Images_PD": "PD",
        "PPMI_Images_SWEDD": "SWEDD",
        "PPMI_Images_Cont": "Control"
    }

    # Iterate over immediate subdirectories in base_dir
    for folder in os.listdir(base_dir):
        folder_path = os.path.join(base_dir, folder)
        if os.path.isdir(folder_path) and folder in expected_folders:
            logging.info(f"Processing folder: {folder_path}")
            # Recursively traverse the expected folder
            for root, dirs, files in os.walk(folder_path):
                for file in files:
                    if file.endswith(".dcm"):
                        full_path = os.path.join(root, file)
                        # Exclude any file with "br_raw" in its full path
                        if "br_raw" in full_path:
                            excluded_files.append(full_path)
                            logging.info(f"Excluding raw file: {full_path}")
                        else:
                            included_files.append((full_path, expected_folders[folder]))
        else:
            logging.info(f"Skipping folder: {folder_path}")

    return included_files, excluded_files

def generate_dataframe(included_files):
    """
    Creates a DataFrame from the list of validated file paths.

    :param included_files: List of tuples (file_path, label)
    :return: DataFrame with columns 'file_path' and 'label'
    """
    df = pd.DataFrame(included_files, columns=["file_path", "label"])
    return df

def save_qa_report(total_files, included_count, excluded_count, output_path="data_ingestion_QA_report.csv"):
    """
    Generates and saves a QA report of the file collection process.

    :param total_files: Total number of DICOM files encountered.
    :param included_count: Count of files included after filtering.
    :param excluded_count: Count of files excluded.
    :param output_path: File path for the QA report CSV.
    """
    exclusion_ratio = excluded_count / total_files if total_files > 0 else 0
    qa_report = {
        "total_files": total_files,
        "included_files": included_count,
        "excluded_files": excluded_count,
        "exclusion_ratio": exclusion_ratio,
    }
    qa_df = pd.DataFrame([qa_report])
    qa_df.to_csv(output_path, index=False)
    logging.info("QA report saved to %s", output_path)

    if exclusion_ratio > 0.5:
        warnings.warn(f"High proportion of raw files excluded: {exclusion_ratio:.2%}")

def load_dicom(file_path):
    """
    GPU-accelerated DICOM loading
    """
    try:
        ds = pydicom.dcmread(file_path)
        # Convert to float32 and move to GPU immediately
        pixel_array = torch.from_numpy(ds.pixel_array.astype(np.float32)).cuda()

        # Apply rescaling on GPU
        if hasattr(ds, 'RescaleSlope') and hasattr(ds, 'RescaleIntercept'):
            slope = float(ds.RescaleSlope)
            intercept = float(ds.RescaleIntercept)
            pixel_array = pixel_array * slope + intercept

        return pixel_array, ds
    except Exception as e:
        raise IOError(f"Error reading DICOM file {file_path}: {e}")

def verify_data_directory():
    """Verify the Images directory structure"""
    base_dir = "Images"
    if not os.path.exists(base_dir):
        print(f"ERROR: {base_dir} directory not found!")
        print(f"Current working directory: {os.getcwd()}")
        print("Contents of current directory:")
        print(os.listdir('.'))
        return False

    expected_subdirs = ['PPMI_Images_PD', 'PPMI_Images_SWEDD', 'PPMI_Images_Cont']
    for subdir in expected_subdirs:
        full_path = os.path.join(base_dir, subdir)
        if not os.path.exists(full_path):
            print(f"WARNING: Expected subdirectory {subdir} not found!")

    return True

# Add this check before running the main pipeline
if not verify_data_directory():
    print("Please ensure the Images directory is properly mounted and contains the expected structure")

"""## Data Ingestion"""

# Cell 4: Data Ingestion Pipeline
import pydicom
import numpy as np
import os

def normalize_path(path):
    """Convert Windows path to Linux path"""
    return path.replace('\\', '/')

def load_dicom(file_path, to_gpu=False):
    """
    Loads and processes a DICOM file with path normalization
    Args:
        file_path: Path to the DICOM file
        to_gpu: If True, moves the tensor to GPU. Default False for visualization purposes
    """
    try:
        # Normalize the path for Linux
        file_path = normalize_path(file_path)

        # Verify file exists
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"File not found: {file_path}")

        ds = pydicom.dcmread(file_path)

        # Extract pixel array and convert to float32
        pixel_array = ds.pixel_array.astype(np.float32)

        # Apply rescaling if attributes are present
        if hasattr(ds, 'RescaleSlope') and hasattr(ds, 'RescaleIntercept'):
            slope = float(ds.RescaleSlope)
            intercept = float(ds.RescaleIntercept)
            pixel_array = pixel_array * slope + intercept

        # Convert to tensor
        tensor = torch.from_numpy(pixel_array)

        # Move to GPU if requested
        if to_gpu:
            tensor = tensor.cuda()

        return tensor, ds

    except Exception as e:
        print(f"Full error details: {str(e)}")
        print(f"Current working directory: {os.getcwd()}")
        raise IOError(f"Error reading DICOM file {file_path}: {e}")

# Cell 5: Execute Data Ingestion Pipeline with Error Handling
import os
import csv

# Define the base directory containing the "Images" folder
base_dir = "Images"

try:
    # Collect files from only the expected subdirectories
    included_files, excluded_files = collect_files(base_dir)
    print(f"Found {len(included_files)} included files and {len(excluded_files)} excluded files")

    # Save using csv module instead of pandas
    with open('validated_file_paths.csv', 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['file_path', 'label'])  # header
        for file_path, label in included_files:
            # Convert Windows path to Linux path
            linux_path = file_path.replace('\\', '/')
            writer.writerow([linux_path, label])

    print("Validated file paths saved to validated_file_paths.csv")

    # Generate and save the QA report in a similar way
    total_files = len(included_files) + len(excluded_files)

    with open('data_ingestion_QA_report.csv', 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['total_files', 'included_files', 'excluded_files', 'exclusion_ratio'])
        exclusion_ratio = len(excluded_files) / total_files if total_files > 0 else 0
        writer.writerow([total_files, len(included_files), len(excluded_files), exclusion_ratio])

    print("QA report generated and saved as data_ingestion_QA_report.csv")

except Exception as e:
    print(f"Error during data ingestion: {str(e)}")

"""## Data Visualization"""

# Cell 6: Visualize One Axial, Coronal, and Sagittal Slice for a Random Patient per Group
import pandas as pd
import random
import matplotlib.pyplot as plt
import numpy as np

# Read the validated file paths CSV with proper path handling
df = pd.read_csv("validated_file_paths.csv")
df['file_path'] = df['file_path'].apply(normalize_path)

# Function to extract the three orthogonal slices from a 3D volume
def extract_slices(volume):
    """
    Given a 3D volume, returns one axial, one coronal, and one sagittal slice.
    Ensures output is on CPU for visualization.
    """
    if torch.is_tensor(volume):
        if volume.is_cuda:
            volume = volume.cpu()
        volume = volume.numpy()

    d, h, w = volume.shape
    axial = volume[d // 2, :, :]         # Axial: slice along depth
    coronal = volume[:, h // 2, :]        # Coronal: slice along height
    sagittal = volume[:, :, w // 2]       # Sagittal: slice along width
    return axial, coronal, sagittal

# List of groups and their expected labels
groups = {"PD": "PD", "SWEDD": "SWEDD", "Control": "Control"}

# Create a figure with one row per group and three columns for the views
fig, axes = plt.subplots(nrows=len(groups), ncols=3, figsize=(12, 4 * len(groups)))
fig.suptitle("Axial, Coronal, and Sagittal Slices for a Random Patient per Group", fontsize=16)

for i, (group_key, group_label) in enumerate(groups.items()):
    # Filter DataFrame for the current group
    group_df = df[df["label"] == group_label]
    if group_df.empty:
        print(f"No data found for group {group_label}")
        continue

    # Select a random file from the group
    random_file = group_df.sample(1)["file_path"].values[0]
    print(f"Loading file for group {group_label}: {random_file}")

    # Load the DICOM volume using the previously defined load_dicom() function
    # Note: don't move to GPU since we're visualizing
    volume, _ = load_dicom(random_file, to_gpu=False)

    # Verify the volume is 3D (if not, skip or raise an error)
    if not (isinstance(volume, torch.Tensor) and volume.ndim == 3):
        raise ValueError(f"Expected 3D volume, got shape {volume.shape} for file: {random_file}")

    axial, coronal, sagittal = extract_slices(volume)

    # Plot Axial slice
    ax = axes[i, 0]
    ax.imshow(axial, cmap="gray")
    ax.set_title(f"{group_label} - Axial")
    ax.axis("off")

    # Plot Coronal slice
    ax = axes[i, 1]
    ax.imshow(coronal, cmap="gray")
    ax.set_title(f"{group_label} - Coronal")
    ax.axis("off")

    # Plot Sagittal slice
    ax = axes[i, 2]
    ax.imshow(sagittal, cmap="gray")
    ax.set_title(f"{group_label} - Sagittal")
    ax.axis("off")

plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()

"""## Data Preprocessing

### Intensity Normalization and Volume Preprocessing
"""

# Cell 7: Data Preprocessing – Intensity Normalization & Volume Processing

import numpy as np
import torch

def intensity_normalization(volume):
    """
    Normalizes intensity values:
    - Truncates negative values (sets them to 0)
    - Applies min-max scaling to bring values between 0 and 1 per volume.

    :param volume: Input 3D volume as numpy array or PyTorch tensor
    :return: Normalized volume (same type as input)
    """
    if torch.is_tensor(volume):
        volume = torch.clamp(volume, min=0)
        vol_min, vol_max = volume.min(), volume.max()
        if vol_max > vol_min:
            volume = (volume - vol_min) / (vol_max - vol_min)
        else:
            volume = volume - vol_min  # volume is constant
    else:
        volume = np.clip(volume, a_min=0, a_max=None)
        vol_min, vol_max = volume.min(), volume.max()
        if vol_max > vol_min:
            volume = (volume - vol_min) / (vol_max - vol_min)
        else:
            volume = volume - vol_min  # volume is constant

    return volume

def resize_volume(volume, target_shape=(128, 128, 128)):
    """
    Resizes the volume to the target shape using zero-padding or center cropping
    while preserving aspect ratio.

    :param volume: Input 3D volume (numpy array or PyTorch tensor)
    :param target_shape: Desired output shape (d_out, h_out, w_out)
    :return: Resized volume with shape target_shape
    """
    is_tensor = torch.is_tensor(volume)
    if is_tensor:
        device = volume.device
        resized = volume.clone()
    else:
        resized = volume.copy()

    current_shape = volume.shape

    # For each dimension, either pad or crop to the target size
    for i in range(3):
        current = resized.shape[i]
        target = target_shape[i]

        if current < target:
            # Calculate padding sizes
            pad_total = target - current
            pad_before = pad_total // 2
            pad_after = pad_total - pad_before

            if is_tensor:
                # Create padding tuple for torch.nn.functional.pad
                pad_tuple = [0] * 6  # (left, right, top, bottom, front, back)
                pad_tuple[-(i*2+2)] = pad_before
                pad_tuple[-(i*2+1)] = pad_after
                resized = torch.nn.functional.pad(resized, pad_tuple, mode='constant', value=0)
            else:
                pad_width = [(0, 0), (0, 0), (0, 0)]
                pad_width[i] = (pad_before, pad_after)
                resized = np.pad(resized, pad_width=pad_width, mode="constant", constant_values=0)

        elif current > target:
            # Center crop
            start = (current - target) // 2
            end = start + target

            if is_tensor:
                if i == 0:
                    resized = resized[start:end, :, :]
                elif i == 1:
                    resized = resized[:, start:end, :]
                else:
                    resized = resized[:, :, start:end]
            else:
                if i == 0:
                    resized = resized[start:end, :, :]
                elif i == 1:
                    resized = resized[:, start:end, :]
                else:
                    resized = resized[:, :, start:end]

    if is_tensor:
        return resized.to(device)
    return resized

# Example usage (for testing on one volume):
print("Testing preprocessing functions...")
volume, _ = load_dicom(random_file, to_gpu=False)  # random_file selected previously
print("Original shape:", volume.shape)

# Test normalization
norm_vol = intensity_normalization(volume)
print("Normalized volume range:", norm_vol.min().item(), "to", norm_vol.max().item())

# Test resizing
resized_vol = resize_volume(norm_vol)
print("Resized shape:", resized_vol.shape)

# Additional checks
print("\nVolume statistics:")
print("Original volume type:", type(volume))
print("Normalized volume type:", type(norm_vol))
print("Resized volume type:", type(resized_vol))

if torch.is_tensor(resized_vol):
    print("Is on GPU:", resized_vol.is_cuda)

"""### Brain Masking"""

# Cell 8: Data Preprocessing – Brain Masking

import numpy as np
import matplotlib.pyplot as plt
from skimage.filters import threshold_otsu
from skimage.morphology import binary_closing, ball
import torch

def get_device():
    """Get the default device (GPU if available, else CPU)"""
    if torch.cuda.is_available():
        device = torch.device("cuda")
        print(f"Using GPU: {torch.cuda.get_device_name(device)}")
    else:
        device = torch.device("cpu")
        print("GPU not available, using CPU")
    return device

def resize_volume(volume, target_shape=(128, 128, 128)):
    """
    Resizes the volume to the target shape using zero-padding or center cropping.

    Args:
        volume: Input 3D volume as numpy array or PyTorch tensor
        target_shape: Desired output shape as tuple (d_new, h_new, w_new)

    Returns:
        Resized volume with shape target_shape
    """
    is_tensor = torch.is_tensor(volume)
    if is_tensor:
        device = volume.device
        resized = volume.clone()
    else:
        resized = volume.copy()

    current_shape = volume.shape

    # For each dimension, either pad or crop to the target size
    for i in range(3):
        current = resized.shape[i]
        target = target_shape[i]

        if current < target:
            # Calculate padding sizes
            pad_total = target - current
            pad_before = pad_total // 2
            pad_after = pad_total - pad_before

            if is_tensor:
                # Create padding tuple for torch.nn.functional.pad
                pad_tuple = [0] * 6  # (left, right, top, bottom, front, back)
                pad_tuple[-(i*2+2)] = pad_before
                pad_tuple[-(i*2+1)] = pad_after
                resized = torch.nn.functional.pad(resized, pad_tuple, mode='constant', value=0)
            else:
                pad_width = [(0, 0), (0, 0), (0, 0)]
                pad_width[i] = (pad_before, pad_after)
                resized = np.pad(resized, pad_width=pad_width, mode="constant", constant_values=0)

        elif current > target:
            # Center crop
            start = (current - target) // 2
            end = start + target

            if is_tensor:
                if i == 0:
                    resized = resized[start:end, :, :]
                elif i == 1:
                    resized = resized[:, start:end, :]
                else:
                    resized = resized[:, :, start:end]
            else:
                if i == 0:
                    resized = resized[start:end, :, :]
                elif i == 1:
                    resized = resized[:, start:end, :]
                else:
                    resized = resized[:, :, start:end]

    if is_tensor:
        return resized.to(device)
    return resized

def process_volume(volume, target_shape=(128, 128, 128)):
    """
    Process a 3D volume by:
    1. Normalizing intensity (truncating negatives and min-max scaling)
    2. Resizing to target_shape
    3. Generating a brain mask via Otsu thresholding and morphological closing

    Args:
        volume: Input 3D volume (numpy array or PyTorch tensor)
        target_shape: Desired output shape (depth, height, width)

    Returns:
        norm_vol: Normalized and resized volume
        mask: Brain mask
        masked_vol: Masked volume
    """
    is_tensor = torch.is_tensor(volume)
    if is_tensor:
        device = volume.device
        volume = volume.cpu().numpy()

    # 1. Intensity normalization
    volume = np.clip(volume, a_min=0, a_max=None)
    vmin, vmax = volume.min(), volume.max()
    if vmax > vmin:
        norm_vol = (volume - vmin) / (vmax - vmin)
    else:
        norm_vol = volume - vmin

    # 2. Resize the normalized volume
    norm_vol = resize_volume(norm_vol, target_shape=target_shape)

    # 3. Compute brain mask
    thresh = threshold_otsu(norm_vol)
    mask = norm_vol > thresh
    mask = binary_closing(mask, footprint=ball(2))
    masked_vol = norm_vol * mask

    # Convert back to tensor if input was tensor
    if is_tensor:
        norm_vol = torch.from_numpy(norm_vol).to(device)
        mask = torch.from_numpy(mask).to(device)
        masked_vol = torch.from_numpy(masked_vol).to(device)

    return norm_vol, mask, masked_vol

# Demonstration: Load one sample DICOM file (using the first file in your validated DataFrame)
sample_file = df.iloc[0]["file_path"]
original_volume, _ = load_dicom(sample_file, to_gpu=False)

# Process the volume with our new function
norm_vol, mask, masked_vol = process_volume(original_volume, target_shape=(128,128,128))

# Extract an axial (middle) slice from both the normalized volume and the masked volume
if torch.is_tensor(norm_vol):
    axial_norm = norm_vol[norm_vol.shape[0]//2].cpu().numpy()
    axial_masked = masked_vol[masked_vol.shape[0]//2].cpu().numpy()
else:
    axial_norm = norm_vol[norm_vol.shape[0]//2]
    axial_masked = masked_vol[masked_vol.shape[0]//2]

# Plot side-by-side for comparison
fig, axes = plt.subplots(1, 2, figsize=(12, 6))
axes[0].imshow(axial_norm, cmap="gray")
axes[0].set_title("Normalized Axial Slice")
axes[0].axis("off")

axes[1].imshow(axial_masked, cmap="gray")
axes[1].set_title("Masked Axial Slice")
axes[1].axis("off")

plt.tight_layout()
plt.show()

# Print some statistics
print("\nVolume Statistics:")
if torch.is_tensor(norm_vol):
    print(f"Normalized volume range: {norm_vol.min().item():.3f} to {norm_vol.max().item():.3f}")
    print(f"Masked volume range: {masked_vol.min().item():.3f} to {masked_vol.max().item():.3f}")
    print(f"Device: {norm_vol.device}")
else:
    print(f"Normalized volume range: {norm_vol.min():.3f} to {norm_vol.max():.3f}")
    print(f"Masked volume range: {masked_vol.min():.3f} to {masked_vol.max():.3f}")

"""## Dataloader Creation (with Shape Validation)"""

# if __name__ == '__main__':
#     mp.set_start_method('spawn', force=True)

# Cell 9: Dataset Implementation with Shape Validation
import torch
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import gc
import numpy as np
import os
import psutil
from sklearn.model_selection import train_test_split

# Move DaTScanDataset definition to module level
class DaTScanDataset(Dataset):
    def __init__(self, dataframe, transform=None, device=None):
        self.df = dataframe
        self.transform = transform
        self.device = device or get_device()
        self._calculate_dataset_statistics()

    def _calculate_dataset_statistics(self):
        """Calculate dataset statistics"""
        print("Calculating dataset statistics...")
        stats_list = []

        # Process in smaller chunks
        for _, row in self.df.iterrows():
            try:
                volume, _ = load_dicom(row["file_path"], to_gpu=False)
                stats_list.append({
                    'min': volume.min().item(),
                    'max': volume.max().item()
                })
            except Exception as e:
                print(f"Error processing file {row['file_path']}: {e}")

        # Compute final statistics
        if stats_list:
            self.stats = {
                'min': min(s['min'] for s in stats_list),
                'max': max(s['max'] for s in stats_list)
            }
        else:
            self.stats = {'min': 0, 'max': 1}

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        try:
            file_path = self.df.iloc[idx]["file_path"]

            # Load and process on CPU
            volume, _ = load_dicom(file_path, to_gpu=False)

            # Process volume
            norm_vol, mask, masked_vol = process_volume(volume, target_shape=(128, 128, 128))

            # Convert to tensor if needed
            if not torch.is_tensor(masked_vol):
                masked_vol = torch.from_numpy(masked_vol)

            # Add channel dimension
            volume_tensor = masked_vol.unsqueeze(0).float()

            # Clean up
            del volume, norm_vol, mask, masked_vol
            gc.collect()

            return {
                "volume": volume_tensor,
                "label": self.df.iloc[idx]["label"],
                "path": file_path
            }
        except Exception as e:
            print(f"Error loading file {file_path}: {str(e)}")
            return None

def create_dataloaders(df, batch_size=4, train_split=0.8):
    """Create train and validation dataloaders"""
    # Split data
    train_df, val_df = train_test_split(
        df,
        test_size=1-train_split,
        stratify=df['label'],
        random_state=42
    )

    print("\nTraining set distribution:")
    print(train_df['label'].value_counts())
    print("\nValidation set distribution:")
    print(val_df['label'].value_counts())

    # Create datasets
    train_dataset = DaTScanDataset(train_df)
    val_dataset = DaTScanDataset(val_df)

    # Create dataloaders with reduced number of workers initially
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=0,  # Start with 0 workers for debugging
        pin_memory=True
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=0,  # Start with 0 workers for debugging
        pin_memory=True
    )

    return train_loader, val_loader

def print_memory_stats():
    """Print memory usage statistics"""
    if torch.cuda.is_available():
        print("\nGPU Memory Usage:")
        print(f"Allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB")
        print(f"Cached: {torch.cuda.memory_reserved() / 1024**2:.2f} MB")
    print(f"CPU Memory Usage: {psutil.Process().memory_info().rss / 1024**2:.2f} MB")

# Test the dataloaders
print("Creating dataloaders...")
train_loader, val_loader = create_dataloaders(df, batch_size=2)

print(f"Number of training batches: {len(train_loader)}")
print(f"Number of validation batches: {len(val_loader)}")

# Test the first few batches
print("\nTesting first few batches from training loader...")
print_memory_stats()

try:
    for i, batch in enumerate(tqdm(train_loader)):
        if batch is not None:
            print(f"\nBatch {i+1}:")
            print(f"Volume shape: {batch['volume'].shape}")
            print(f"Label: {batch['label']}")
            print_memory_stats()
        else:
            print(f"Batch {i+1} is None!")

        if i >= 2:  # Test first 3 batches
            break

        gc.collect()
        torch.cuda.empty_cache()

except Exception as e:
    print(f"Error during batch processing: {str(e)}")
    import traceback
    traceback.print_exc()

"""# Exploratory Data Analysis (EDA)"""

# Cell 10: Comprehensive EDA Implementation with Stratified Split
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from tqdm import tqdm
from collections import defaultdict
from sklearn.model_selection import train_test_split

# First, let's implement proper stratified splitting
def create_stratified_dataloaders(df, batch_size=2, train_split=0.8):
    """
    Create train and validation dataloaders with stratified splitting to maintain class distributions
    """
    # Perform stratified split
    train_df, val_df = train_test_split(
        df,
        test_size=1-train_split,
        stratify=df['label'],
        random_state=42
    )

    print("\nDataset split statistics:")
    print("Training set distribution:")
    print(train_df['label'].value_counts())
    print("\nValidation set distribution:")
    print(val_df['label'].value_counts())

    # Create datasets
    train_dataset = DaTScanDataset(train_df)
    val_dataset = DaTScanDataset(val_df)

    # Create dataloaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=0,
        pin_memory=True
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=0,
        pin_memory=True
    )

    return train_loader, val_loader

def analyze_dataset_statistics(dataloader, num_batches=None):
    """
    Analyzes dataset statistics with memory-efficient batch processing
    Returns: Dictionary of statistical measures
    """
    print("Analyzing dataset statistics...")
    stats = defaultdict(list)

    try:
        for i, batch in enumerate(tqdm(dataloader, desc="Computing statistics")):
            if num_batches and i >= num_batches:
                break

            volumes = batch['volume']
            labels = batch['label']

            # Per-volume statistics
            for vol_idx, (volume, label) in enumerate(zip(volumes, labels)):
                vol_data = volume.numpy().flatten()

                stats['mean'].append(np.mean(vol_data))
                stats['std'].append(np.std(vol_data))
                stats['median'].append(np.median(vol_data))
                stats['min'].append(np.min(vol_data))
                stats['max'].append(np.max(vol_data))
                stats['label'].append(label)

            # Memory cleanup
            del volumes, labels
            gc.collect()
            torch.cuda.empty_cache()

    except Exception as e:
        print(f"Error during statistical analysis: {str(e)}")
        import traceback
        traceback.print_exc()

    return pd.DataFrame(stats)

def plot_intensity_distributions(stats_df):
    """
    Creates violin plots of intensity distributions by group
    """
    plt.figure(figsize=(15, 6))

    # Plot intensity distributions
    plt.subplot(1, 2, 1)
    sns.violinplot(data=stats_df, x='label', y='mean', palette='viridis')
    plt.title('Distribution of Mean Intensities by Group')
    plt.xlabel('Group')
    plt.ylabel('Mean Intensity')
    plt.xticks(rotation=45)

    plt.subplot(1, 2, 2)
    sns.violinplot(data=stats_df, x='label', y='std', palette='viridis')
    plt.title('Distribution of Intensity Standard Deviations by Group')
    plt.xlabel('Group')
    plt.ylabel('Standard Deviation')
    plt.xticks(rotation=45)

    plt.tight_layout()
    plt.show()

def plot_group_statistics(stats_df):
    """
    Plots statistical summaries by group
    """
    plt.figure(figsize=(15, 5))

    # Group counts
    plt.subplot(1, 3, 1)
    sns.countplot(data=stats_df, x='label', palette='viridis')
    plt.title('Sample Count by Group')
    plt.xlabel('Group')
    plt.ylabel('Count')
    plt.xticks(rotation=45)

    # Box plots
    plt.subplot(1, 3, 2)
    sns.boxplot(data=stats_df, x='label', y='mean', palette='viridis')
    plt.title('Mean Intensity Distribution')
    plt.xlabel('Group')
    plt.ylabel('Mean Intensity')
    plt.xticks(rotation=45)

    # Violin plots for variance
    plt.subplot(1, 3, 3)
    sns.violinplot(data=stats_df, x='label', y='std', palette='viridis')
    plt.title('Intensity Variance Distribution')
    plt.xlabel('Group')
    plt.ylabel('Standard Deviation')
    plt.xticks(rotation=45)

    plt.tight_layout()
    plt.show()

def analyze_spatial_patterns(dataloader, num_samples_per_group=2):
    """
    Analyzes spatial patterns in the volumes, ensuring samples from each group
    """
    # Collect samples per group
    samples = defaultdict(list)

    print("Collecting samples for spatial analysis...")
    for batch in dataloader:
        volumes = batch['volume']
        labels = batch['label']

        for volume, label in zip(volumes, labels):
            if len(samples[label]) < num_samples_per_group:
                samples[label].append(volume)

        # Check if we have enough samples from each group
        if all(len(v) >= num_samples_per_group for v in samples.values()):
            break

    # Plot samples
    total_samples = len(samples) * num_samples_per_group
    plt.figure(figsize=(15, 5 * total_samples))

    plot_idx = 1
    for label in samples:
        for volume in samples[label]:
            # Get middle slices
            vol_data = volume.squeeze().numpy()
            axial = vol_data[vol_data.shape[0]//2, :, :]
            sagittal = vol_data[:, :, vol_data.shape[2]//2]
            coronal = vol_data[:, vol_data.shape[1]//2, :]

            # Plot
            plt.subplot(total_samples, 3, plot_idx)
            plt.imshow(axial, cmap='gray')
            plt.title(f'{label} - Axial')
            plt.axis('off')

            plt.subplot(total_samples, 3, plot_idx + 1)
            plt.imshow(sagittal, cmap='gray')
            plt.title(f'{label} - Sagittal')
            plt.axis('off')

            plt.subplot(total_samples, 3, plot_idx + 2)
            plt.imshow(coronal, cmap='gray')
            plt.title(f'{label} - Coronal')
            plt.axis('off')

            plot_idx += 3

    plt.tight_layout()
    plt.show()

# Create new stratified dataloaders
print("Creating stratified dataloaders...")
train_loader, val_loader = create_stratified_dataloaders(df, batch_size=2)

# Run the EDA
print("Starting Exploratory Data Analysis...")

# Analyze training dataset statistics
print("\nAnalyzing training dataset...")
train_stats = analyze_dataset_statistics(train_loader, num_batches=50)

# Plot distributions
print("\nPlotting intensity distributions...")
plot_intensity_distributions(train_stats)

# Plot group statistics
print("\nPlotting group statistics...")
plot_group_statistics(train_stats)

# Analyze spatial patterns
print("\nAnalyzing spatial patterns...")
analyze_spatial_patterns(train_loader, num_samples_per_group=2)

# Print summary statistics
print("\nSummary Statistics by Group:")
summary_stats = train_stats.groupby('label').agg({
    'mean': ['mean', 'std'],
    'std': ['mean', 'std'],
    'median': ['mean', 'std'],
    'min': ['mean', 'std'],
    'max': ['mean', 'std']
}).round(3)
print(summary_stats)

# Memory cleanup
gc.collect()
torch.cuda.empty_cache()
print("\nEDA completed!")

"""### Slice Intensity Variance Analysis"""

def analyze_slice_variance(dataloader, num_samples_per_group=5):
    """
    Analyzes slice-wise variance across different views for each patient group
    """
    print("Analyzing slice-wise variance patterns...")

    # Initialize storage for variances
    group_variances = {
        'PD': {'axial': [], 'coronal': [], 'sagittal': []},
        'Control': {'axial': [], 'coronal': [], 'sagittal': []},
        'SWEDD': {'axial': [], 'coronal': [], 'sagittal': []}
    }
    sample_counts = {'PD': 0, 'Control': 0, 'SWEDD': 0}

    try:
        for batch in tqdm(dataloader, desc="Computing slice variances"):
            volumes = batch['volume']
            labels = batch['label']

            for volume, label in zip(volumes, labels):
                label = label if isinstance(label, str) else label.item()

                if sample_counts[label] >= num_samples_per_group:
                    continue

                # Get volume data
                vol_data = volume.squeeze().numpy()
                d, h, w = vol_data.shape

                # Compute variance for each slice in each view
                axial_var = [np.var(vol_data[i, :, :]) for i in range(d)]
                coronal_var = [np.var(vol_data[:, i, :]) for i in range(h)]
                sagittal_var = [np.var(vol_data[:, :, i]) for i in range(w)]

                # Store variances
                group_variances[label]['axial'].append(axial_var)
                group_variances[label]['coronal'].append(coronal_var)
                group_variances[label]['sagittal'].append(sagittal_var)

                sample_counts[label] += 1

            # Check if we have enough samples from each group
            if all(count >= num_samples_per_group for count in sample_counts.values()):
                break

            # Memory cleanup
            del volumes, labels
            gc.collect()
            torch.cuda.empty_cache()

    except Exception as e:
        print(f"Error during variance analysis: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

    # Compute average variances across samples for each group
    avg_variances = {}
    for group in group_variances:
        avg_variances[group] = {
            view: np.mean(variances, axis=0)
            for view, variances in group_variances[group].items()
        }

    return avg_variances

# Plot the slice variance results
def plot_slice_variances(avg_variances):
    """
    Creates line plots for slice-wise variance analysis
    """
    views = ['axial', 'coronal', 'sagittal']
    fig, axes = plt.subplots(1, 3, figsize=(20, 6))

    for idx, view in enumerate(views):
        ax = axes[idx]

        for group in avg_variances:
            variances = avg_variances[group][view]
            ax.plot(range(len(variances)), variances, label=group)

        ax.set_title(f'{view.capitalize()} View - Slice-wise Variance')
        ax.set_xlabel('Slice Index')
        ax.set_ylabel('Average Variance')
        ax.legend()
        ax.grid(True)

    plt.tight_layout()
    plt.show()

# Analyze slice-wise variance
print("\nAnalyzing slice-wise variance patterns...")
avg_variances = analyze_slice_variance(train_loader, num_samples_per_group=5)

if avg_variances is not None:
    print("\nPlotting slice-wise variance analysis...")
    plot_slice_variances(avg_variances)

"""# Model Phase"""

# # Memory and Batch Size Optimization test control

# def test_batch_memory_limits(data_loader, start_size=8, max_size=48, step=8):
#     """
#     Tests different batch sizes to find the optimal one that maximizes GPU utilization
#     while avoiding out-of-memory errors.
#     """
#     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#     memory_stats = {}
#     optimal_batch_size = start_size

#     print("Starting batch size optimization test...")
#     print(f"Testing batch sizes from {start_size} to {max_size} in steps of {step}")
#     print("\nYour GPU: NVIDIA RTX 4070 Ti (12GB)")
#     print("Current test will estimate memory headroom for both training and inference\n")

#     # Get a single batch from the data loader to understand the data shape
#     sample_batch = next(iter(data_loader))
#     sample_volume = sample_batch['volume']
#     volume_shape = sample_volume.shape[1:]
#     print(f"Volume shape (excluding batch dimension): {volume_shape}")

#     for batch_size in range(start_size, max_size + step, step):
#         try:
#             print(f"\nTesting batch size: {batch_size}")

#             # Create a dummy batch with gradient tracking
#             dummy_data = torch.randn(batch_size, *volume_shape, device=device, requires_grad=True)

#             # Simulate model operations (more intensive to better represent real usage)
#             conv_weight1 = torch.randn(32, 1, 3, 3, 3, device=device, requires_grad=True)
#             conv_weight2 = torch.randn(64, 32, 3, 3, 3, device=device, requires_grad=True)

#             # Forward operations (more complex to better simulate real model)
#             conv1 = torch.nn.functional.conv3d(dummy_data, conv_weight1, padding=1)
#             act1 = torch.nn.functional.relu(conv1)
#             pool1 = torch.nn.functional.max_pool3d(act1, kernel_size=2)

#             conv2 = torch.nn.functional.conv3d(pool1, conv_weight2, padding=1)
#             act2 = torch.nn.functional.relu(conv2)
#             pool2 = torch.nn.functional.max_pool3d(act2, kernel_size=2)

#             loss = pool2.mean()
#             loss.backward()

#             # Get memory statistics
#             allocated = torch.cuda.memory_allocated() / (1024 ** 2)  # MB
#             reserved = torch.cuda.memory_reserved() / (1024 ** 2)    # MB
#             max_memory = 12 * 1024  # 12GB for RTX 4070 Ti
#             memory_usage_percent = (reserved / max_memory) * 100

#             memory_stats[batch_size] = {
#                 'allocated': allocated,
#                 'reserved': reserved,
#                 'usage_percent': memory_usage_percent
#             }

#             print(f"GPU Memory Allocated: {allocated:.2f} MB")
#             print(f"GPU Memory Reserved: {reserved:.2f} MB")
#             print(f"Total GPU Memory Usage: {memory_usage_percent:.1f}%")

#             # Estimate training headroom
#             headroom = max_memory - reserved
#             print(f"Estimated memory headroom: {headroom:.2f} MB")

#             # Update optimal batch size
#             optimal_batch_size = batch_size

#             # Clean up
#             del dummy_data, conv_weight1, conv_weight2, conv1, conv2, act1, act2, pool1, pool2, loss
#             torch.cuda.empty_cache()

#             # If we're using more than 70% of GPU memory, stop testing
#             if memory_usage_percent > 70:
#                 print("\nReaching high memory usage, stopping tests for safety")
#                 break

#         except RuntimeError as e:
#             if "out of memory" in str(e):
#                 print(f"\nOut of memory at batch size {batch_size}")
#                 print(f"Last successful batch size was {optimal_batch_size}")
#                 break
#             else:
#                 print(f"Unexpected error: {str(e)}")
#                 break

#     # Calculate recommended values based on memory usage pattern
#     recommended_batch_size = int(optimal_batch_size * 0.8)  # 80% of max for safety
#     recommended_accum_steps = 2 if recommended_batch_size >= 16 else 4

#     print("\nBatch Size Optimization Results:")
#     print(f"Maximum tested batch size: {optimal_batch_size}")
#     print(f"Recommended batch size for training: {recommended_batch_size}")
#     print(f"Recommended accumulation steps: {recommended_accum_steps}")
#     print("\nRecommended training configuration:")
#     print(f"config = TrainingConfig(")
#     print(f"    batch_size={recommended_batch_size},")
#     print(f"    accumulation_steps={recommended_accum_steps},")
#     print(f"    use_mixed_precision=True,")
#     print(f"    num_workers=4,")
#     print(f"    pin_memory=True")
#     print(f")")

#     return recommended_batch_size, memory_stats

# # Example usage
# if __name__ == "__main__":
#     try:
#         print("\nTesting optimal batch size configuration...")
#         optimal_batch_size, memory_stats = test_batch_memory_limits(train_loader)
#     except Exception as e:
#         print(f"Error during batch size testing: {str(e)}")
#         print("Using default conservative values:")
#         print("batch_size = 8")
#         print("accumulation_steps = 4")

"""## 1. Autoencoder

### Model Setup
"""

# Cell 11: Base Autoencoder Implementation with Memory Optimization
import torch
import torch.nn as nn
import torch.nn.functional as F
from collections import OrderedDict

import torch.cuda.amp as amp

class BaseAutoencoder(nn.Module):
    """Memory-optimized 3D Autoencoder with mixed precision support."""
    def __init__(self, latent_dim=256):
        super().__init__()
        self.encoder = Encoder(latent_dim)
        self.decoder = Decoder(latent_dim)
        # Enable CUDA benchmarking for optimal convolution algorithms
        torch.backends.cudnn.benchmark = True

    def forward(self, x):
        # Forward pass remains the same for compatibility
        z, skip_connections = self.encoder(x)
        reconstruction = self.decoder(z, skip_connections)
        return reconstruction

class ConvBlock(nn.Module):
    """Memory-efficient convolutional block with batch normalization and ReLU activation."""
    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):
        super().__init__()
        self.block = nn.Sequential(OrderedDict([
            ('conv', nn.Conv3d(in_channels, out_channels, kernel_size, stride, padding)),
            ('bn', nn.BatchNorm3d(out_channels)),
            ('relu', nn.ReLU(inplace=True))  # inplace ReLU for memory efficiency
        ]))

    def forward(self, x):
        return self.block(x)

class Encoder(nn.Module):
    """3D Encoder network optimized for 128³ input volumes."""
    def __init__(self, latent_dim=256):
        super().__init__()

        # Initial feature extraction
        self.init_conv = ConvBlock(1, 16)  # 128 -> 128

        # Downsampling path with progressive channel increase
        self.down1 = nn.Sequential(
            ConvBlock(16, 32, stride=2),    # 128 -> 64
            ConvBlock(32, 32)
        )

        self.down2 = nn.Sequential(
            ConvBlock(32, 64, stride=2),    # 64 -> 32
            ConvBlock(64, 64)
        )

        self.down3 = nn.Sequential(
            ConvBlock(64, 128, stride=2),   # 32 -> 16
            ConvBlock(128, 128)
        )

        self.down4 = nn.Sequential(
            ConvBlock(128, 256, stride=2),  # 16 -> 8
            ConvBlock(256, 256)
        )

        # Project to latent space
        self.flatten_size = 256 * 8 * 8 * 8
        self.fc = nn.Linear(self.flatten_size, latent_dim)

    def forward(self, x):
        # Track shapes for debugging
        if self.training:
            print(f"Input shape: {x.shape}")

        x = self.init_conv(x)
        d1 = self.down1(x)
        d2 = self.down2(d1)
        d3 = self.down3(d2)
        d4 = self.down4(d3)

        # Flatten and project to latent space
        flat = torch.flatten(d4, start_dim=1)
        z = self.fc(flat)

        if self.training:
            print(f"Latent shape: {z.shape}")

        return z, (d1, d2, d3, d4)

class Decoder(nn.Module):
    """3D Decoder network optimized for 128³ output volumes."""
    def __init__(self, latent_dim=256):
        super().__init__()

        self.flatten_size = 256 * 8 * 8 * 8
        self.fc = nn.Linear(latent_dim, self.flatten_size)

        # Upsampling path with skip connections
        self.up1 = nn.Sequential(
            nn.ConvTranspose3d(256, 128, kernel_size=2, stride=2),  # 8 -> 16
            ConvBlock(128, 128)
        )

        self.up2 = nn.Sequential(
            nn.ConvTranspose3d(128, 64, kernel_size=2, stride=2),   # 16 -> 32
            ConvBlock(64, 64)
        )

        self.up3 = nn.Sequential(
            nn.ConvTranspose3d(64, 32, kernel_size=2, stride=2),    # 32 -> 64
            ConvBlock(32, 32)
        )

        self.up4 = nn.Sequential(
            nn.ConvTranspose3d(32, 16, kernel_size=2, stride=2),    # 64 -> 128
            ConvBlock(16, 16)
        )

        # Final convolution
        self.final_conv = nn.Conv3d(16, 1, kernel_size=1)

    def forward(self, z, skip_connections):
        if self.training:
            print(f"Decoder input shape: {z.shape}")

        # Reshape from latent space
        x = self.fc(z)
        x = x.view(-1, 256, 8, 8, 8)

        # Unpack skip connections
        d1, d2, d3, d4 = skip_connections

        # Upsampling with skip connections
        x = self.up1(x + d4)
        x = self.up2(x + d3)
        x = self.up3(x + d2)
        x = self.up4(x + d1)

        # Final convolution with sigmoid activation
        x = torch.sigmoid(self.final_conv(x))

        if self.training:
            print(f"Output shape: {x.shape}")

        return x

class BaseAutoencoder(nn.Module):
    """Memory-optimized 3D Autoencoder for 128³ medical volumes."""
    def __init__(self, latent_dim=256):
        super().__init__()
        self.encoder = Encoder(latent_dim)
        self.decoder = Decoder(latent_dim)

    def forward(self, x):
        z, skip_connections = self.encoder(x)
        reconstruction = self.decoder(z, skip_connections)
        return reconstruction

    def encode(self, x):
        """Encode input to latent space"""
        z, _ = self.encoder(x)
        return z

    def decode(self, z):
        """Decode from latent space (for generation)"""
        batch_size = z.size(0)
        device = z.device
        dummy_skips = (
            torch.zeros(batch_size, 32, 64, 64, 64, device=device),
            torch.zeros(batch_size, 64, 32, 32, 32, device=device),
            torch.zeros(batch_size, 128, 16, 16, 16, device=device),
            torch.zeros(batch_size, 256, 8, 8, 8, device=device)
        )
        return self.decoder(z, dummy_skips)

def test_autoencoder(batch_size=2):
    """Test the autoencoder with dummy data and verify memory usage."""
    print("\nTesting Autoencoder Architecture...")

    try:
        # Create model and move to GPU
        model = BaseAutoencoder()
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = model.to(device)

        # Print model summary
        print("\nModel Architecture:")
        print(model)

        # Create dummy input (128³ volume)
        dummy_input = torch.randn(batch_size, 1, 128, 128, 128, device=device)

        # Print initial memory usage
        print("\nInitial GPU Memory Usage:")
        print_gpu_memory_stats()

        # Test forward pass
        print("\nTesting forward pass...")
        with torch.no_grad():
            output = model(dummy_input)

        # Print output shape and final memory usage
        print(f"\nOutput shape: {output.shape}")
        print("\nFinal GPU Memory Usage:")
        print_gpu_memory_stats()

        # Verify shapes
        assert output.shape == dummy_input.shape, f"Shape mismatch: {output.shape} vs {dummy_input.shape}"

        # Clean up
        del model, dummy_input, output
        torch.cuda.empty_cache()

        print("\nAutoencoder test completed successfully!")

    except Exception as e:
        print(f"Error testing autoencoder: {str(e)}")
        import traceback
        traceback.print_exc()

# Run test if this cell is executed
if __name__ == "__main__":
    test_autoencoder()

# Cell 12: Training Utilities and Configuration
import os
import json
import time
from pathlib import Path
import torch.optim as optim
from torch.optim.lr_scheduler import ReduceLROnPlateau

class TrainingConfig:
    """Training configuration optimized for NVIDIA 4070Ti"""
    def __init__(self, **kwargs):
        self.learning_rate = kwargs.get('learning_rate', 1e-4)
        self.batch_size = kwargs.get('batch_size', 8)  # Increased from 2 to 8
        self.accumulation_steps = kwargs.get('accumulation_steps', 4)  # New: gradient accumulation
        self.epochs = kwargs.get('epochs', 100)
        self.early_stopping_patience = kwargs.get('early_stopping_patience', 10)
        self.checkpoint_dir = kwargs.get('checkpoint_dir', 'checkpoints')
        self.model_name = kwargs.get('model_name', 'autoencoder')
        self.use_mixed_precision = kwargs.get('use_mixed_precision', True)  # Enable by default
        self.num_workers = kwargs.get('num_workers', 4)  # Optimize data loading
        self.pin_memory = kwargs.get('pin_memory', True)  # Faster data transfer to GPU

        # Create checkpoint directory
        Path(self.checkpoint_dir).mkdir(parents=True, exist_ok=True)

class EarlyStopping:
    """Early stopping handler with patience"""
    def __init__(self, patience=10, min_delta=0):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = None
        self.should_stop = False

    def __call__(self, val_loss):
        if self.best_loss is None:
            self.best_loss = val_loss
            return False

        if val_loss > self.best_loss - self.min_delta:
            self.counter += 1
            if self.counter >= self.patience:
                self.should_stop = True
        else:
            self.best_loss = val_loss
            self.counter = 0

        return self.should_stop

class CheckpointHandler:
    """Handles saving and loading of model checkpoints"""
    def __init__(self, checkpoint_dir, model_name):
        self.checkpoint_dir = Path(checkpoint_dir)
        self.model_name = model_name
        self.checkpoint_path = self.checkpoint_dir / f"{model_name}_checkpoint.pth"
        self.metadata_path = self.checkpoint_dir / f"{model_name}_metadata.json"

    def save(self, model, optimizer, scheduler, epoch, train_losses, val_losses):
        """Save model checkpoint and training metadata"""
        # Save model checkpoint
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
            'train_losses': train_losses,
            'val_losses': val_losses
        }
        torch.save(checkpoint, self.checkpoint_path)

        # Save metadata
        metadata = {
            'last_epoch': epoch,
            'train_losses': train_losses,
            'val_losses': val_losses,
            'timestamp': time.strftime("%Y-%m-%d %H:%M:%S")
        }
        with open(self.metadata_path, 'w') as f:
            json.dump(metadata, f, indent=4)

    def load(self, model, optimizer, scheduler):
        """Load model checkpoint and return training metadata"""
        if not self.checkpoint_path.exists():
            return None

        checkpoint = torch.load(self.checkpoint_path)
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        if scheduler and checkpoint['scheduler_state_dict']:
            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])

        return {
            'epoch': checkpoint['epoch'],
            'train_losses': checkpoint['train_losses'],
            'val_losses': checkpoint['val_losses']
        }

def print_gpu_memory_stats():
    """Print current GPU memory usage"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / (1024 ** 2)
        reserved = torch.cuda.memory_reserved() / (1024 ** 2)
        print(f"GPU Memory Allocated: {allocated:.2f} MB")
        print(f"GPU Memory Reserved: {reserved:.2f} MB")

# Cell 13: Training Loop Implementation
import torch.nn as nn
from tqdm import tqdm
import matplotlib.pyplot as plt

def train_autoencoder(model, train_loader, val_loader, config=None):
    """GPU-optimized training loop with mixed precision and gradient accumulation"""
    if config is None:
        config = TrainingConfig()

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    # Initialize training components
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)
    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)
    early_stopping = EarlyStopping(patience=config.early_stopping_patience)
    checkpoint_handler = CheckpointHandler(config.checkpoint_dir, config.model_name)

    # Mixed precision setup
    scaler = amp.GradScaler(enabled=config.use_mixed_precision)

    # Load checkpoint if available
    start_epoch = 0
    train_losses = []
    val_losses = []
    checkpoint_data = checkpoint_handler.load(model, optimizer, scheduler)
    if checkpoint_data:
        start_epoch = checkpoint_data['epoch'] + 1
        train_losses = checkpoint_data['train_losses']
        val_losses = checkpoint_data['val_losses']
        print(f"Resuming training from epoch {start_epoch}")

    try:
        for epoch in range(start_epoch, config.epochs):
            model.train()
            epoch_loss = 0
            optimizer.zero_grad()  # Zero gradients at epoch start

            train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{config.epochs} [Train]')

            for batch_idx, batch in enumerate(train_pbar):
                try:
                    volumes = batch['volume'].to(device, non_blocking=True)

                    # Mixed precision forward pass
                    with amp.autocast(enabled=config.use_mixed_precision):
                        reconstructed = model(volumes)
                        loss = criterion(reconstructed, volumes)
                        # Scale loss by accumulation steps
                        loss = loss / config.accumulation_steps

                    # Mixed precision backward pass
                    scaler.scale(loss).backward()

                    # Gradient accumulation
                    if (batch_idx + 1) % config.accumulation_steps == 0:
                        scaler.step(optimizer)
                        scaler.update()
                        optimizer.zero_grad()

                    epoch_loss += loss.item() * config.accumulation_steps
                    train_pbar.set_postfix({'loss': loss.item() * config.accumulation_steps})

                except RuntimeError as e:
                    if "out of memory" in str(e):
                        print(f"\nOOM in batch {batch_idx}. Adjusting batch size...")
                        torch.cuda.empty_cache()
                        continue
                    raise e

                # Clean up
                del volumes, reconstructed, loss
                torch.cuda.empty_cache()

            avg_train_loss = epoch_loss / len(train_loader)
            train_losses.append(avg_train_loss)

            # Validation phase
            model.eval()
            val_loss = 0
            val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{config.epochs} [Val]')

            with torch.no_grad():
                for batch in val_pbar:
                    try:
                        volumes = batch['volume'].to(device)
                        reconstructed = model(volumes)
                        loss = criterion(reconstructed, volumes)
                        val_loss += loss.item()

                    except RuntimeError as e:
                        if "out of memory" in str(e):
                            print("\nOOM during validation. Cleaning up...")
                            if 'volumes' in locals():
                                del volumes
                            if 'reconstructed' in locals():
                                del reconstructed
                            if 'loss' in locals():
                                del loss
                            torch.cuda.empty_cache()
                            continue
                        else:
                            raise e

                    # Clean up
                    del volumes, reconstructed, loss
                    torch.cuda.empty_cache()

            avg_val_loss = val_loss / len(val_loader)
            val_losses.append(avg_val_loss)

            # Update learning rate
            scheduler.step(avg_val_loss)

            # Save checkpoint
            checkpoint_handler.save(
                model, optimizer, scheduler,
                epoch, train_losses, val_losses
            )

            # Print epoch summary
            print(f"\nEpoch {epoch+1}/{config.epochs}")
            print(f"Train Loss: {avg_train_loss:.6f}")
            print(f"Val Loss: {avg_val_loss:.6f}")
            print_gpu_memory_stats()

            # Early stopping check
            if early_stopping(avg_val_loss):
                print("\nEarly stopping triggered!")
                break

    except KeyboardInterrupt:
        print("\nTraining interrupted by user!")

    finally:
        # Plot training history
        plt.figure(figsize=(10, 5))
        plt.plot(train_losses, label='Train Loss')
        plt.plot(val_losses, label='Validation Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True)
        plt.savefig(os.path.join(config.checkpoint_dir, f"{config.model_name}_training_history.png"))
        plt.show()

        return train_losses, val_losses

"""### Training"""

# TRAINING
if __name__ == "__main__":
    try:
        model = BaseAutoencoder()
        config = TrainingConfig(
            learning_rate=1e-4,
            batch_size=16,  # Increased batch size
            accumulation_steps=2,  # Gradient accumulation
            epochs=100,
            use_mixed_precision=True,
            num_workers=8,
            pin_memory=True
        )

        train_losses, val_losses = train_autoencoder(model, train_loader, val_loader, config)

    except Exception as e:
        print(f"Error during training: {str(e)}")
        import traceback
        traceback.print_exc()

"""### Evaluation and Result Visualization"""

# Cell 14: Model Evaluation and Visualization
import matplotlib.pyplot as plt
import numpy as np
import torch
import json
from pathlib import Path

def load_checkpoint_for_evaluation(checkpoint_dir, model_name):
    """Load model and training history for evaluation"""
    checkpoint_path = Path(checkpoint_dir) / f"{model_name}_checkpoint.pth"
    metadata_path = Path(checkpoint_dir) / f"{model_name}_metadata.json"

    # Load model
    model = BaseAutoencoder()
    checkpoint = torch.load(checkpoint_path)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()

    # Load training history
    with open(metadata_path, 'r') as f:
        metadata = json.load(f)

    return model, metadata

def plot_training_history(metadata):
    """Plot training and validation losses"""
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(metadata['train_losses'], label='Train Loss')
    plt.plot(metadata['val_losses'], label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training History')
    plt.legend()
    plt.grid(True)

    # Plot recent epochs in detail
    recent_epochs = 20
    if len(metadata['train_losses']) > recent_epochs:
        plt.subplot(1, 2, 2)
        plt.plot(metadata['train_losses'][-recent_epochs:], label='Train Loss')
        plt.plot(metadata['val_losses'][-recent_epochs:], label='Validation Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title(f'Last {recent_epochs} Epochs')
        plt.legend()
        plt.grid(True)

    plt.tight_layout()
    plt.show()

def visualize_reconstructions(model, val_loader, num_samples=3):
    """Visualize original vs reconstructed volumes"""
    device = next(model.parameters()).device

    with torch.no_grad():
        for batch in val_loader:
            volumes = batch['volume'].to(device)
            reconstructed = model(volumes)

            # Process num_samples samples
            for idx in range(min(num_samples, volumes.shape[0])):
                fig = plt.figure(figsize=(15, 5))

                # Get middle slices
                orig_vol = volumes[idx, 0].cpu().numpy()
                recon_vol = reconstructed[idx, 0].cpu().numpy()

                # Plot axial, sagittal, and coronal views
                views = ['Axial', 'Sagittal', 'Coronal']
                slices_orig = [
                    orig_vol[orig_vol.shape[0]//2, :, :],
                    orig_vol[:, orig_vol.shape[1]//2, :],
                    orig_vol[:, :, orig_vol.shape[2]//2]
                ]
                slices_recon = [
                    recon_vol[recon_vol.shape[0]//2, :, :],
                    recon_vol[:, recon_vol.shape[1]//2, :],
                    recon_vol[:, :, recon_vol.shape[2]//2]
                ]

                for i, (view, orig_slice, recon_slice) in enumerate(zip(views, slices_orig, slices_recon)):
                    # Original
                    plt.subplot(2, 3, i+1)
                    plt.imshow(orig_slice, cmap='gray')
                    plt.title(f'Original {view}')
                    plt.axis('off')

                    # Reconstructed
                    plt.subplot(2, 3, i+4)
                    plt.imshow(recon_slice, cmap='gray')
                    plt.title(f'Reconstructed {view}')
                    plt.axis('off')

                plt.suptitle(f'Sample {idx+1} - Original vs Reconstructed', y=1.02)
                plt.tight_layout()
                plt.show()

            break  # Only process first batch

def compute_metrics(model, val_loader):
    """Compute quantitative metrics on validation set"""
    device = next(model.parameters()).device
    mse_criterion = nn.MSELoss()

    total_mse = 0
    total_samples = 0

    with torch.no_grad():
        for batch in tqdm(val_loader, desc="Computing metrics"):
            volumes = batch['volume'].to(device)
            reconstructed = model(volumes)

            mse = mse_criterion(reconstructed, volumes).item()
            total_mse += mse * volumes.shape[0]
            total_samples += volumes.shape[0]

    avg_mse = total_mse / total_samples
    print(f"\nValidation Metrics:")
    print(f"Average MSE: {avg_mse:.6f}")
    print(f"RMSE: {np.sqrt(avg_mse):.6f}")

# Example usage
if __name__ == "__main__":
    # Load latest checkpoint
    model, metadata = load_checkpoint_for_evaluation('checkpoints', 'autoencoder')
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    # Plot training history
    plot_training_history(metadata)

    # Visualize reconstructions
    visualize_reconstructions(model, val_loader)

    # Compute metrics
    compute_metrics(model, val_loader)

"""### Latent Space Analysis"""

# Cell 15: Latent Space Analysis
import torch
import numpy as np
import seaborn as sns
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from collections import defaultdict

def extract_latent_representations(model, dataloader, device):
    """
    Extract latent representations and corresponding labels for all samples
    """
    model.eval()
    latent_vectors = []
    labels = []

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Extracting latent representations"):
            volumes = batch['volume'].to(device)
            # Get latent vectors using the encode method
            z = model.encode(volumes)
            latent_vectors.append(z.cpu().numpy())
            labels.extend(batch['label'])

            # Clean up memory
            del volumes, z
            torch.cuda.empty_cache()

    return np.vstack(latent_vectors), np.array(labels)

def visualize_latent_space(latent_vectors, labels, method='tsne'):
    """
    Visualize latent space using t-SNE or PCA
    """
    plt.figure(figsize=(12, 8))

    if method == 'tsne':
        reducer = TSNE(n_components=2, random_state=42)
        title = 't-SNE Visualization of Latent Space'
    else:
        reducer = PCA(n_components=2, random_state=42)
        title = 'PCA Visualization of Latent Space'

    # Reduce dimensionality
    reduced_vecs = reducer.fit_transform(latent_vectors)

    # Create scatter plot
    unique_labels = np.unique(labels)
    colors = plt.cm.rainbow(np.linspace(0, 1, len(unique_labels)))

    for label, color in zip(unique_labels, colors):
        mask = labels == label
        plt.scatter(reduced_vecs[mask, 0], reduced_vecs[mask, 1],
                   label=label, color=color, alpha=0.6)

    plt.title(title)
    plt.legend()
    plt.grid(True)
    plt.show()

    return reduced_vecs

def analyze_latent_dimensions(latent_vectors, labels):
    """
    Analyze the distribution of values in each latent dimension
    """
    plt.figure(figsize=(15, 6))

    # Plot distribution of latent values per group
    unique_labels = np.unique(labels)

    # Compute mean activation per dimension for each group
    mean_activations = defaultdict(list)
    for label in unique_labels:
        mask = labels == label
        mean_activations[label] = np.mean(latent_vectors[mask], axis=0)

    # Plot heatmap of mean activations
    activation_matrix = np.vstack([mean_activations[label] for label in unique_labels])
    plt.subplot(1, 2, 1)
    sns.heatmap(activation_matrix, xticklabels=False, yticklabels=unique_labels)
    plt.title('Mean Latent Dimension Activation by Group')
    plt.xlabel('Latent Dimensions')
    plt.ylabel('Group')

    # Plot top discriminative dimensions
    variance_ratio = np.var(activation_matrix, axis=0)
    top_dims = np.argsort(variance_ratio)[-5:]  # Top 5 dimensions

    plt.subplot(1, 2, 2)
    for label in unique_labels:
        mask = labels == label
        plt.boxplot([latent_vectors[mask, dim] for dim in top_dims],
                   positions=np.arange(len(top_dims)) + (unique_labels == label).nonzero()[0][0] * 0.3,
                   widths=0.2, label=label)

    plt.title('Distribution of Top 5 Discriminative Dimensions')
    plt.xlabel('Dimension Index')
    plt.ylabel('Activation')
    plt.legend()
    plt.tight_layout()
    plt.show()

# Run the analysis
if __name__ == "__main__":
    # Load model and move to device
    model, _ = load_checkpoint_for_evaluation('checkpoints', 'autoencoder')
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    # Extract latent representations
    latent_vecs, labels = extract_latent_representations(model, val_loader, device)

    # Visualize using t-SNE and PCA
    print("Generating t-SNE visualization...")
    tsne_coords = visualize_latent_space(latent_vecs, labels, method='tsne')

    print("\nGenerating PCA visualization...")
    pca_coords = visualize_latent_space(latent_vecs, labels, method='pca')

    # Analyze latent dimensions
    print("\nAnalyzing latent dimensions...")
    analyze_latent_dimensions(latent_vecs, labels)

# Cell 16: Region-based Reconstruction Analysis
import numpy as np
import matplotlib.pyplot as plt
from scipy.ndimage import center_of_mass

def analyze_reconstruction_quality_by_region(model, dataloader, device):
    """
    Analyze reconstruction quality in different brain regions
    """
    model.eval()
    region_errors = defaultdict(list)

    # Define regions of interest (approximate coordinates for DaTSCAN)
    regions = {
        'Left Striatum': (slice(54, 74), slice(54, 74), slice(44, 64)),
        'Right Striatum': (slice(54, 74), slice(54, 74), slice(64, 84)),
        'Background': (slice(0, 20), slice(0, 20), slice(0, 20))
    }

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Analyzing reconstruction quality"):
            volumes = batch['volume'].to(device)
            reconstructed = model(volumes)

            # Calculate error for each region
            for vol_idx in range(volumes.shape[0]):
                orig = volumes[vol_idx, 0].cpu().numpy()
                recon = reconstructed[vol_idx, 0].cpu().numpy()

                for region_name, coords in regions.items():
                    orig_region = orig[coords]
                    recon_region = recon[coords]
                    mse = np.mean((orig_region - recon_region) ** 2)
                    region_errors[region_name].append(mse)

            # Clean up
            del volumes, reconstructed
            torch.cuda.empty_cache()

    # Plot results
    plt.figure(figsize=(10, 6))
    plt.boxplot([region_errors[region] for region in regions.keys()],
                labels=regions.keys())
    plt.title('Reconstruction Error by Brain Region')
    plt.ylabel('Mean Squared Error')
    plt.xticks(rotation=45)
    plt.grid(True)
    plt.show()

    # Print summary statistics
    print("\nReconstruction Error Statistics by Region:")
    for region in regions.keys():
        errors = region_errors[region]
        print(f"\n{region}:")
        print(f"Mean MSE: {np.mean(errors):.6f}")
        print(f"Std MSE: {np.std(errors):.6f}")
        print(f"Min MSE: {np.min(errors):.6f}")
        print(f"Max MSE: {np.max(errors):.6f}")

# Run the analysis
if __name__ == "__main__":
    print("Analyzing reconstruction quality by region...")
    analyze_reconstruction_quality_by_region(model, val_loader, device)

# Cell 17: Latent Space Clinical Correlation
import numpy as np
from scipy.stats import spearmanr
import matplotlib.pyplot as plt
import seaborn as sns

def analyze_latent_clinical_correlation(latent_vectors, labels):
    """
    Analyze correlation between latent dimensions and clinical groups
    """
    # Convert labels to numeric for correlation analysis
    label_mapping = {label: idx for idx, label in enumerate(np.unique(labels))}
    numeric_labels = np.array([label_mapping[label] for label in labels])

    # Calculate correlation between each latent dimension and the clinical groups
    correlations = []
    p_values = []

    for dim_idx in range(latent_vectors.shape[1]):
        corr, p_val = spearmanr(latent_vectors[:, dim_idx], numeric_labels)
        correlations.append(corr)
        p_values.append(p_val)

    # Plot correlation heatmap
    plt.figure(figsize=(15, 5))

    # Plot correlations
    plt.subplot(1, 2, 1)
    significant_dims = np.array(p_values) < 0.05
    plt.bar(range(len(correlations)), correlations,
            color=['red' if sig else 'gray' for sig in significant_dims])
    plt.title('Correlation of Latent Dimensions with Clinical Groups')
    plt.xlabel('Latent Dimension')
    plt.ylabel('Correlation Coefficient')
    plt.grid(True)

    # Plot most discriminative dimensions
    top_dims = np.argsort(np.abs(correlations))[-5:]
    plt.subplot(1, 2, 2)
    for label in np.unique(labels):
        mask = labels == label
        values = latent_vectors[mask][:, top_dims]
        plt.violinplot(values, positions=range(len(top_dims)),
                      showmeans=True, showextrema=True)

    plt.title('Distribution of Top Discriminative Dimensions')
    plt.xlabel('Dimension Index')
    plt.ylabel('Activation')
    plt.tight_layout()
    plt.show()

    # Print summary of most significant dimensions
    print("\nTop 5 Most Discriminative Dimensions:")
    for dim_idx in top_dims:
        print(f"\nDimension {dim_idx}:")
        print(f"Correlation: {correlations[dim_idx]:.3f}")
        print(f"P-value: {p_values[dim_idx]:.3e}")

# Run the analysis
if __name__ == "__main__":
    print("Analyzing latent space clinical correlations...")
    analyze_latent_clinical_correlation(latent_vecs, labels)

"""### Synthetic Brain Generation"""

# Cell 18: Synthetic Brain Generation
import torch
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

def generate_synthetic_brain(model, dataloader, condition='PD', num_samples=5):
    """
    Generate synthetic brain scans by manipulating the latent space

    Args:
        model: Trained autoencoder model
        dataloader: DataLoader containing the validation set
        condition: Target condition ('PD', 'Control', or 'SWEDD')
        num_samples: Number of synthetic samples to generate
    """
    device = next(model.parameters()).device
    model.eval()

    # First, get latent representations of real brains
    print("Extracting latent representations...")
    latent_vectors = []
    conditions = []

    with torch.no_grad():
        for batch in dataloader:
            volumes = batch['volume'].to(device)
            labels = batch['label']

            # Get latent vectors
            z = model.encode(volumes)
            latent_vectors.append(z.cpu().numpy())
            conditions.extend(labels)

            del volumes, z
            torch.cuda.empty_cache()

    latent_vectors = np.vstack(latent_vectors)
    conditions = np.array(conditions)

    # Calculate mean and covariance of latent vectors for target condition
    target_mask = conditions == condition
    target_vectors = latent_vectors[target_mask]
    latent_mean = np.mean(target_vectors, axis=0)
    latent_cov = np.cov(target_vectors.T)

    # Generate new latent vectors by sampling from learned distribution
    print(f"\nGenerating synthetic {condition} brains...")
    synthetic_vectors = np.random.multivariate_normal(
        latent_mean,
        latent_cov + 1e-6 * np.eye(latent_cov.shape[0]),  # Add small value to ensure positive definiteness
        size=num_samples
    )

    # Convert to torch tensor and move to device
    synthetic_vectors = torch.tensor(synthetic_vectors, dtype=torch.float32).to(device)

    # Generate synthetic brains using decoder
    with torch.no_grad():
        synthetic_brains = model.decode(synthetic_vectors)

    # Visualize results
    fig = plt.figure(figsize=(15, 3*num_samples))

    for i in range(num_samples):
        brain = synthetic_brains[i, 0].cpu().numpy()

        # Get middle slices
        axial = brain[brain.shape[0]//2, :, :]
        sagittal = brain[:, brain.shape[1]//2, :]
        coronal = brain[:, :, brain.shape[2]//2]

        # Plot
        plt.subplot(num_samples, 3, i*3 + 1)
        plt.imshow(axial, cmap='gray')
        plt.title(f'Synthetic {condition} - Axial' if i == 0 else '')
        plt.axis('off')

        plt.subplot(num_samples, 3, i*3 + 2)
        plt.imshow(sagittal, cmap='gray')
        plt.title(f'Synthetic {condition} - Sagittal' if i == 0 else '')
        plt.axis('off')

        plt.subplot(num_samples, 3, i*3 + 3)
        plt.imshow(coronal, cmap='gray')
        plt.title(f'Synthetic {condition} - Coronal' if i == 0 else '')
        plt.axis('off')

    plt.tight_layout()
    plt.show()

    return synthetic_brains

def interpolate_between_conditions(model, dataloader, start_condition='Control', end_condition='PD', steps=5):
    """
    Generate interpolated brains between two conditions
    """
    device = next(model.parameters()).device
    model.eval()

    # Get latent representations
    print("Extracting latent representations...")
    latent_vectors = []
    conditions = []

    with torch.no_grad():
        for batch in dataloader:
            volumes = batch['volume'].to(device)
            labels = batch['label']

            z = model.encode(volumes)
            latent_vectors.append(z.cpu().numpy())
            conditions.extend(labels)

            del volumes, z
            torch.cuda.empty_cache()

    latent_vectors = np.vstack(latent_vectors)
    conditions = np.array(conditions)

    # Get mean latent vectors for both conditions
    start_mean = np.mean(latent_vectors[conditions == start_condition], axis=0)
    end_mean = np.mean(latent_vectors[conditions == end_condition], axis=0)

    # Create interpolation steps
    alphas = np.linspace(0, 1, steps)
    interpolated_vectors = np.array([
        (1 - alpha) * start_mean + alpha * end_mean
        for alpha in alphas
    ])

    # Generate interpolated brains
    interpolated_vectors = torch.tensor(interpolated_vectors, dtype=torch.float32).to(device)

    with torch.no_grad():
        interpolated_brains = model.decode(interpolated_vectors)

    # Visualize interpolation
    fig = plt.figure(figsize=(15, 3*steps))

    for i in range(steps):
        brain = interpolated_brains[i, 0].cpu().numpy()

        # Get middle slices
        axial = brain[brain.shape[0]//2, :, :]
        sagittal = brain[:, brain.shape[1]//2, :]
        coronal = brain[:, :, brain.shape[2]//2]

        # Plot
        plt.subplot(steps, 3, i*3 + 1)
        plt.imshow(axial, cmap='gray')
        plt.title(f'Interpolation {i/(steps-1):.1f} - Axial' if i == 0 else '')
        plt.axis('off')

        plt.subplot(steps, 3, i*3 + 2)
        plt.imshow(sagittal, cmap='gray')
        plt.title(f'Interpolation {i/(steps-1):.1f} - Sagittal' if i == 0 else '')
        plt.axis('off')

        plt.subplot(steps, 3, i*3 + 3)
        plt.imshow(coronal, cmap='gray')
        plt.title(f'Interpolation {i/(steps-1):.1f} - Coronal' if i == 0 else '')
        plt.axis('off')

    plt.suptitle(f'Interpolation from {start_condition} to {end_condition}')
    plt.tight_layout()
    plt.show()

    return interpolated_brains

# Run the generation
if __name__ == "__main__":
    # Load model
    model, _ = load_checkpoint_for_evaluation('checkpoints', 'autoencoder')
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    # Generate synthetic brains for each condition
    print("\nGenerating synthetic Control brains...")
    synthetic_control = generate_synthetic_brain(model, val_loader, condition='Control')

    print("\nGenerating synthetic PD brains...")
    synthetic_pd = generate_synthetic_brain(model, val_loader, condition='PD')

    print("\nGenerating synthetic SWEDD brains...")
    synthetic_swedd = generate_synthetic_brain(model, val_loader, condition='SWEDD')

    # Generate interpolation between conditions
    print("\nGenerating interpolation from Control to PD...")
    interpolated = interpolate_between_conditions(model, val_loader)

"""## 2. Variational Autoencoder (VAE)

### Model Setup
"""

# Cell 19: VAE Model Architecture
import torch
import torch.nn as nn
import torch.nn.functional as F
from collections import OrderedDict

class VAE(nn.Module):
    """
    3D Variational Autoencoder optimized for 128³ medical volumes with
    memory-efficient implementation for NVIDIA 4070Ti.
    """
    def __init__(self, latent_dim=256):
        super().__init__()
        self.encoder = VAEEncoder(latent_dim)
        self.decoder = VAEDecoder(latent_dim)
        # Enable CUDNN benchmarking for optimal performance
        torch.backends.cudnn.benchmark = True

    def reparameterize(self, mu, log_var):
        """
        Reparameterization trick to enable backpropagation through sampling.
        """
        if self.training:
            std = torch.exp(0.5 * log_var)
            eps = torch.randn_like(std)
            return mu + eps * std
        return mu

    def forward(self, x):
        # Encode input to get mean and variance of latent distribution
        mu, log_var, skip_connections = self.encoder(x)

        # Sample from latent distribution
        z = self.reparameterize(mu, log_var)

        # Decode sampled latent vector
        reconstruction = self.decoder(z, skip_connections)

        return reconstruction, mu, log_var

class ConvBlock(nn.Module):
    """Memory-efficient convolutional block with batch normalization and ReLU."""
    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):
        super().__init__()
        self.block = nn.Sequential(OrderedDict([
            ('conv', nn.Conv3d(in_channels, out_channels, kernel_size, stride, padding)),
            ('bn', nn.BatchNorm3d(out_channels)),
            ('relu', nn.ReLU(inplace=True))  # inplace ReLU for memory efficiency
        ]))

    def forward(self, x):
        return self.block(x)

class VAEEncoder(nn.Module):
    """3D Encoder network with probabilistic latent space."""
    def __init__(self, latent_dim=256):
        super().__init__()

        # Initial feature extraction
        self.init_conv = ConvBlock(1, 16)  # 128 -> 128

        # Downsampling path with progressive channel increase
        self.down1 = nn.Sequential(
            ConvBlock(16, 32, stride=2),    # 128 -> 64
            ConvBlock(32, 32)
        )

        self.down2 = nn.Sequential(
            ConvBlock(32, 64, stride=2),    # 64 -> 32
            ConvBlock(64, 64)
        )

        self.down3 = nn.Sequential(
            ConvBlock(64, 128, stride=2),   # 32 -> 16
            ConvBlock(128, 128)
        )

        self.down4 = nn.Sequential(
            ConvBlock(128, 256, stride=2),  # 16 -> 8
            ConvBlock(256, 256)
        )

        # Project to latent parameters
        self.flatten_size = 256 * 8 * 8 * 8
        self.fc_mu = nn.Linear(self.flatten_size, latent_dim)
        self.fc_var = nn.Linear(self.flatten_size, latent_dim)

    def forward(self, x):
        # Initial convolution
        x = self.init_conv(x)

        # Downsampling with skip connections
        d1 = self.down1(x)
        d2 = self.down2(d1)
        d3 = self.down3(d2)
        d4 = self.down4(d3)

        # Flatten and project to latent parameters
        flat = torch.flatten(d4, start_dim=1)
        mu = self.fc_mu(flat)
        log_var = self.fc_var(flat)

        return mu, log_var, (d1, d2, d3, d4)

class VAEDecoder(nn.Module):
    """3D Decoder network with skip connections."""
    def __init__(self, latent_dim=256):
        super().__init__()

        self.flatten_size = 256 * 8 * 8 * 8
        self.fc = nn.Linear(latent_dim, self.flatten_size)

        # Upsampling path with skip connections
        self.up1 = nn.Sequential(
            nn.ConvTranspose3d(256, 128, kernel_size=2, stride=2),  # 8 -> 16
            ConvBlock(128, 128)
        )

        self.up2 = nn.Sequential(
            nn.ConvTranspose3d(128, 64, kernel_size=2, stride=2),   # 16 -> 32
            ConvBlock(64, 64)
        )

        self.up3 = nn.Sequential(
            nn.ConvTranspose3d(64, 32, kernel_size=2, stride=2),    # 32 -> 64
            ConvBlock(32, 32)
        )

        self.up4 = nn.Sequential(
            nn.ConvTranspose3d(32, 16, kernel_size=2, stride=2),    # 64 -> 128
            ConvBlock(16, 16)
        )

        # Final convolution
        self.final_conv = nn.Conv3d(16, 1, kernel_size=1)

    def forward(self, z, skip_connections):
        # Reshape from latent space
        x = self.fc(z)
        x = x.view(-1, 256, 8, 8, 8)

        # Unpack skip connections
        d1, d2, d3, d4 = skip_connections

        # Upsampling with skip connections
        x = self.up1(x + d4)
        x = self.up2(x + d3)
        x = self.up3(x + d2)
        x = self.up4(x + d1)

        # Final convolution with sigmoid activation
        x = torch.sigmoid(self.final_conv(x))

        return x

# Cell 20: VAE Loss Function Implementation
class VAELoss:
    """
    Custom loss function for VAE combining reconstruction loss and KL divergence.
    Implements β-VAE formulation with annealing for better training stability.
    """
    def __init__(self, beta_start=0.0, beta_end=1.0, beta_steps=10000):
        self.beta_start = beta_start
        self.beta_end = beta_end
        self.beta_steps = beta_steps
        self.current_step = 0

    def __call__(self, recon_x, x, mu, log_var):
        """
        Compute VAE loss with annealed β-VAE formulation.

        Args:
            recon_x: Reconstructed volume
            x: Original volume
            mu: Mean of latent distribution
            log_var: Log variance of latent distribution
        """
        # Reconstruction loss (MSE)
        recon_loss = F.mse_loss(recon_x, x, reduction='mean')

        # KL divergence
        kl_loss = -0.5 * torch.mean(1 + log_var - mu.pow(2) - log_var.exp())

        # Calculate beta for current step
        beta = min(self.beta_end,
                  self.beta_start + (self.beta_end - self.beta_start) *
                  (self.current_step / self.beta_steps))

        # Increment step
        self.current_step += 1

        # Total loss
        total_loss = recon_loss + beta * kl_loss

        return total_loss, recon_loss, kl_loss, beta

# Cell 21: Training Configuration and Utilities
from tqdm import tqdm
import os
from pathlib import Path
import json
import time

class VAEConfig:
    """Configuration for VAE training."""
    def __init__(self):
        # Model parameters
        self.latent_dim = 256
        self.learning_rate = 1e-4

        # Training parameters
        self.batch_size = 8  # Conservative batch size for 12GB VRAM
        self.epochs = 100
        self.accumulation_steps = 4  # Gradient accumulation for effective batch size of 32

        # Loss parameters
        self.beta_start = 0.0
        self.beta_end = 1.0
        self.beta_steps = 10000

        # Early stopping
        self.patience = 10
        self.min_delta = 1e-6

        # Optimization
        self.use_amp = True  # Automatic mixed precision
        self.num_workers = 4
        self.pin_memory = True

        # Checkpoint configuration
        self.checkpoint_dir = 'checkpoints'
        self.model_name = 'vae'
        Path(self.checkpoint_dir).mkdir(parents=True, exist_ok=True)

class CheckpointHandler:
    """Handles saving and loading of model checkpoints"""
    def __init__(self, checkpoint_dir, model_name):
        self.checkpoint_dir = Path(checkpoint_dir)
        self.model_name = model_name
        self.checkpoint_path = self.checkpoint_dir / f"{model_name}_checkpoint.pth"
        self.metadata_path = self.checkpoint_dir / f"{model_name}_metadata.json"

    def save(self, model, optimizer, scheduler, epoch, train_losses, val_losses):
        """Save model checkpoint and training metadata"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
            'train_losses': train_losses,
            'val_losses': val_losses
        }
        torch.save(checkpoint, self.checkpoint_path)

        # Save metadata
        metadata = {
            'last_epoch': epoch,
            'train_losses': train_losses,
            'val_losses': val_losses,
            'timestamp': time.strftime("%Y-%m-%d %H:%M:%S")
        }
        with open(self.metadata_path, 'w') as f:
            json.dump(metadata, f, indent=4)

    def load(self, model, optimizer, scheduler):
        """Load model checkpoint and return training metadata"""
        if not self.checkpoint_path.exists():
            return None

        checkpoint = torch.load(self.checkpoint_path)
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        if scheduler and checkpoint['scheduler_state_dict']:
            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])

        return {
            'epoch': checkpoint['epoch'],
            'train_losses': checkpoint['train_losses'],
            'val_losses': checkpoint['val_losses']
        }

# Cell 22: Training Loop
def train_vae(model, train_loader, val_loader, config=None):
    """Training loop with memory optimization for 4070Ti."""
    if config is None:
        config = VAEConfig()

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    # Initialize components
    criterion = VAELoss(config.beta_start, config.beta_end, config.beta_steps)
    optimizer = create_vae_optimizer(model, config)
    scheduler = create_vae_scheduler(optimizer)
    scaler = torch.cuda.amp.GradScaler(enabled=config.use_amp)
    checkpoint_handler = CheckpointHandler(config.checkpoint_dir, config.model_name)

    # Training tracking
    best_val_loss = float('inf')
    patience_counter = 0
    train_losses = []
    val_losses = []

    # Load checkpoint if available
    start_epoch = 0
    checkpoint_data = checkpoint_handler.load(model, optimizer, scheduler)
    if checkpoint_data:
        start_epoch = checkpoint_data['epoch'] + 1
        train_losses = checkpoint_data['train_losses']
        val_losses = checkpoint_data['val_losses']
        print(f"Resuming training from epoch {start_epoch}")

    try:
        for epoch in range(start_epoch, config.epochs):
            # Training phase
            model.train()
            train_loss = 0
            train_recon_loss = 0
            train_kl_loss = 0

            train_pbar = tqdm(train_loader,
                            desc=f'Epoch {epoch+1}/{config.epochs} [Train]',
                            leave=False)

            for i, batch in enumerate(train_pbar):
                try:
                    volumes = batch['volume'].to(device, non_blocking=True)

                    # Forward pass with mixed precision
                    with torch.cuda.amp.autocast(enabled=config.use_amp):
                        recon, mu, log_var = model(volumes)
                        loss, recon_l, kl_l, beta = criterion(recon, volumes, mu, log_var)
                        loss = loss / config.accumulation_steps

                    # Backward pass
                    scaler.scale(loss).backward()

                    # Gradient accumulation
                    if (i + 1) % config.accumulation_steps == 0:
                        scaler.step(optimizer)
                        scaler.update()
                        optimizer.zero_grad()

                    # Track losses
                    train_loss += loss.item() * config.accumulation_steps
                    train_recon_loss += recon_l.item()
                    train_kl_loss += kl_l.item()

                    # Update progress bar
                    train_pbar.set_postfix({
                        'loss': loss.item() * config.accumulation_steps,
                        'recon': recon_l.item(),
                        'kl': kl_l.item(),
                        'beta': beta
                    })

                    # Clean up
                    del volumes, recon, mu, log_var
                    torch.cuda.empty_cache()

                except RuntimeError as e:
                    if "out of memory" in str(e):
                        print(f"\nOOM in batch {i}. Cleaning up...")
                        if 'volumes' in locals():
                            del volumes
                        if 'recon' in locals():
                            del recon
                        torch.cuda.empty_cache()
                        continue
                    raise e

            # Validation phase
            model.eval()
            val_loss = 0

            val_pbar = tqdm(val_loader,
                          desc=f'Epoch {epoch+1}/{config.epochs} [Val]',
                          leave=False)

            with torch.no_grad():
                for batch in val_pbar:
                    try:
                        volumes = batch['volume'].to(device)
                        recon, mu, log_var = model(volumes)
                        loss, _, _, _ = criterion(recon, volumes, mu, log_var)
                        val_loss += loss.item()

                        val_pbar.set_postfix({'loss': loss.item()})

                        del volumes, recon, mu, log_var
                        torch.cuda.empty_cache()

                    except RuntimeError as e:
                        if "out of memory" in str(e):
                            print("\nOOM during validation. Cleaning up...")
                            if 'volumes' in locals():
                                del volumes
                            if 'recon' in locals():
                                del recon
                            torch.cuda.empty_cache()
                            continue
                        raise e

            # Calculate average losses
            avg_train_loss = train_loss / len(train_loader)
            avg_val_loss = val_loss / len(val_loader)

            train_losses.append(avg_train_loss)
            val_losses.append(avg_val_loss)

            # Update learning rate
            scheduler.step(avg_val_loss)

            # Save checkpoint
            checkpoint_handler.save(
                model, optimizer, scheduler,
                epoch, train_losses, val_losses
            )

            # Early stopping check
            if avg_val_loss < best_val_loss - config.min_delta:
                best_val_loss = avg_val_loss
                patience_counter = 0
                # Save best model
                torch.save(model.state_dict(),
                         os.path.join(config.checkpoint_dir, f'{config.model_name}_best.pth'))
            else:
                patience_counter += 1
                if patience_counter >= config.patience:
                    print(f"\nEarly stopping triggered at epoch {epoch+1}")
                    break

            # Print epoch summary
            print(f"\nEpoch [{epoch+1}/{config.epochs}]")
            print(f"Train Loss: {avg_train_loss:.6f}")
            print(f"Val Loss: {avg_val_loss:.6f}")
            print(f"Beta: {beta:.4f}")
            print_gpu_memory_stats()  # From previous implementation

    except KeyboardInterrupt:
        print("\nTraining interrupted by user!")

    finally:
        # Plot training history
        plt.figure(figsize=(10, 5))
        plt.plot(train_losses, label='Train Loss')
        plt.plot(val_losses, label='Validation Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True)
        plt.savefig(os.path.join(config.checkpoint_dir, f"{config.model_name}_training_history.png"))
        plt.show()

    return train_losses, val_losses

def create_vae_optimizer(model, config):
    """Create optimizer for VAE training."""
    return torch.optim.Adam(model.parameters(), lr=config.learning_rate)

def create_vae_scheduler(optimizer):
    """Create learning rate scheduler for VAE training."""
    return torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer,
        mode='min',
        factor=0.5,
        patience=5,
        verbose=True
    )

"""### Training"""

# TRAINING

if __name__ == "__main__":
    # Initialize model and config
    model = VAE()
    config = VAEConfig()

    # Train model
    train_losses, val_losses = train_vae(model, train_loader, val_loader, config)

# Cell 23: Model Evaluation and Reconstruction Visualization
import torch
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path

def load_trained_vae(checkpoint_dir, model_name='vae'):
    """Load trained VAE model and metadata"""
    checkpoint_path = Path(checkpoint_dir) / f"{model_name}_checkpoint.pth"
    metadata_path = Path(checkpoint_dir) / f"{model_name}_metadata.json"

    # Load model
    model = VAE()
    checkpoint = torch.load(checkpoint_path)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()

    # Load training history
    with open(metadata_path, 'r') as f:
        metadata = json.load(f)

    return model, metadata

def visualize_reconstructions(model, val_loader, num_samples=3):
    """Visualize original vs reconstructed volumes with added uncertainty visualization"""
    device = next(model.parameters()).device

    with torch.no_grad():
        for batch in val_loader:
            volumes = batch['volume'].to(device)
            # Generate multiple reconstructions to assess variance
            reconstructions = []
            for _ in range(5):  # Generate 5 reconstructions per sample
                recon, _, _ = model(volumes)
                reconstructions.append(recon.cpu().numpy())
            reconstructions = np.stack(reconstructions)

            # Process num_samples samples
            for idx in range(min(num_samples, volumes.shape[0])):
                fig = plt.figure(figsize=(15, 8))

                # Get middle slices
                orig_vol = volumes[idx, 0].cpu().numpy()
                recon_mean = reconstructions[:, idx, 0].mean(axis=0)
                recon_std = reconstructions[:, idx, 0].std(axis=0)

                # Plot axial, sagittal, and coronal views
                views = ['Axial', 'Sagittal', 'Coronal']
                slices_orig = [
                    orig_vol[orig_vol.shape[0]//2, :, :],
                    orig_vol[:, orig_vol.shape[1]//2, :],
                    orig_vol[:, :, orig_vol.shape[2]//2]
                ]
                slices_recon = [
                    recon_mean[recon_mean.shape[0]//2, :, :],
                    recon_mean[:, recon_mean.shape[1]//2, :],
                    recon_mean[:, :, recon_mean.shape[2]//2]
                ]
                slices_std = [
                    recon_std[recon_std.shape[0]//2, :, :],
                    recon_std[:, recon_std.shape[1]//2, :],
                    recon_std[:, :, recon_std.shape[2]//2]
                ]

                for i, (view, orig_slice, recon_slice, std_slice) in enumerate(zip(views, slices_orig, slices_recon, slices_std)):
                    # Original
                    plt.subplot(3, 3, i+1)
                    plt.imshow(orig_slice, cmap='gray')
                    plt.title(f'Original {view}')
                    plt.axis('off')

                    # Reconstructed
                    plt.subplot(3, 3, i+4)
                    plt.imshow(recon_slice, cmap='gray')
                    plt.title(f'Reconstructed {view}')
                    plt.axis('off')

                    # Uncertainty
                    plt.subplot(3, 3, i+7)
                    plt.imshow(std_slice, cmap='viridis')
                    plt.title(f'Uncertainty {view}')
                    plt.colorbar()
                    plt.axis('off')

                plt.suptitle(f'Sample {idx+1} - Original vs Reconstructed with Uncertainty', y=1.02)
                plt.tight_layout()
                plt.show()

            break  # Only process first batch

# Cell 24: Quantitative Metrics
def compute_metrics(model, val_loader):
    """Compute quantitative metrics including reconstruction error and KL divergence"""
    device = next(model.parameters()).device
    mse_criterion = nn.MSELoss(reduction='none')

    total_mse = 0
    total_kl = 0
    total_samples = 0

    region_errors = {
        'Left Striatum': [],
        'Right Striatum': [],
        'Background': []
    }

    # Define regions of interest (approximate coordinates for DaTSCAN)
    regions = {
        'Left Striatum': (slice(54, 74), slice(54, 74), slice(44, 64)),
        'Right Striatum': (slice(54, 74), slice(54, 74), slice(64, 84)),
        'Background': (slice(0, 20), slice(0, 20), slice(0, 20))
    }

    with torch.no_grad():
        for batch in tqdm(val_loader, desc="Computing metrics"):
            volumes = batch['volume'].to(device)
            recon, mu, log_var = model(volumes)

            # Overall MSE
            mse = mse_criterion(recon, volumes).mean(dim=(1,2,3,4))

            # KL divergence
            kl = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp(), dim=1)

            total_mse += mse.sum().item()
            total_kl += kl.sum().item()
            total_samples += volumes.shape[0]

            # Region-specific analysis
            for vol_idx in range(volumes.shape[0]):
                orig = volumes[vol_idx, 0].cpu().numpy()
                recon_vol = recon[vol_idx, 0].cpu().numpy()

                for region_name, coords in regions.items():
                    orig_region = orig[coords]
                    recon_region = recon_vol[coords]
                    mse = np.mean((orig_region - recon_region) ** 2)
                    region_errors[region_name].append(mse)

    # Calculate average metrics
    avg_mse = total_mse / total_samples
    avg_kl = total_kl / total_samples

    print(f"\nOverall Metrics:")
    print(f"Average MSE: {avg_mse:.6f}")
    print(f"Average KL Divergence: {avg_kl:.6f}")
    print(f"RMSE: {np.sqrt(avg_mse):.6f}")

    print("\nRegion-Specific MSE:")
    for region in regions.keys():
        errors = region_errors[region]
        print(f"\n{region}:")
        print(f"Mean MSE: {np.mean(errors):.6f}")
        print(f"Std MSE: {np.std(errors):.6f}")
        print(f"Min MSE: {np.min(errors):.6f}")
        print(f"Max MSE: {np.max(errors):.6f}")

    return avg_mse, avg_kl, region_errors

# Cell 25: Latent Space Analysis
def analyze_latent_space(model, val_loader, num_samples=1000):
    """Analyze the structure of the learned latent space"""
    device = next(model.parameters()).device
    latent_vectors = []
    labels = []
    reconstructions = []

    with torch.no_grad():
        for batch in tqdm(val_loader, desc="Collecting latent vectors"):
            volumes = batch['volume'].to(device)
            mu, log_var, _ = model.encoder(volumes)
            z = model.reparameterize(mu, log_var)

            latent_vectors.append(z.cpu().numpy())
            labels.extend(batch['label'])

            if len(np.concatenate(latent_vectors)) >= num_samples:
                break

    latent_vectors = np.concatenate(latent_vectors)[:num_samples]
    labels = np.array(labels[:num_samples])

    # Plot latent space distribution
    plt.figure(figsize=(15, 5))

    # Plot distribution of first two dimensions
    plt.subplot(131)
    plt.scatter(latent_vectors[:, 0], latent_vectors[:, 1], c=labels.astype('category').cat.codes, alpha=0.6)
    plt.title('First Two Latent Dimensions')
    plt.xlabel('Dimension 1')
    plt.ylabel('Dimension 2')

    # Plot average activation per dimension
    plt.subplot(132)
    avg_activation = np.mean(np.abs(latent_vectors), axis=0)
    plt.bar(range(len(avg_activation)), avg_activation)
    plt.title('Average Activation per Dimension')
    plt.xlabel('Dimension')
    plt.ylabel('Average Activation')

    # Plot variance explained
    plt.subplot(133)
    var_explained = np.var(latent_vectors, axis=0)
    plt.plot(np.cumsum(var_explained) / np.sum(var_explained))
    plt.title('Cumulative Variance Explained')
    plt.xlabel('Number of Dimensions')
    plt.ylabel('Cumulative Variance Ratio')

    plt.tight_layout()
    plt.show()

    return latent_vectors, labels

# Cell 26: Run Evaluation
if __name__ == "__main__":
    # Load trained model
    model, metadata = load_trained_vae('checkpoints')
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    print("Visualizing reconstructions...")
    visualize_reconstructions(model, val_loader)

    print("\nComputing metrics...")
    avg_mse, avg_kl, region_errors = compute_metrics(model, val_loader)

    print("\nAnalyzing latent space...")
    latent_vectors, labels = analyze_latent_space(model, val_loader)

"""## 3. AE Semi-Supervised

### Model Setup
"""

# Cell 27: Semi-Supervised Model Architecture
# Common imports
import os
import time
from pathlib import Path
import json
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
from collections import OrderedDict

# PyTorch imports
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.cuda.amp as amp

# Sklearn imports
from sklearn.metrics import confusion_matrix, classification_report

# Visualization
import seaborn as sns

class SemiSupervisedAE(nn.Module):
    """
    Semi-supervised autoencoder with classification branch.
    Optimized for 128³ medical volumes on NVIDIA 4070Ti.
    """
    def __init__(self, latent_dim=256, num_classes=3):
        super().__init__()
        self.encoder = SSEncoder(latent_dim)
        self.decoder = SSDecoder(latent_dim)
        self.classifier = nn.Sequential(
            nn.Linear(latent_dim, 128),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(128, num_classes)
        )
        torch.backends.cudnn.benchmark = True

    def forward(self, x):
        # Encode input
        z, skip_connections = self.encoder(x)

        # Generate reconstruction
        reconstruction = self.decoder(z, skip_connections)

        # Generate classification
        classification = self.classifier(z)

        return reconstruction, classification, z

class ConvBlock(nn.Module):
    """Memory-efficient convolutional block."""
    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):
        super().__init__()
        self.block = nn.Sequential(OrderedDict([
            ('conv', nn.Conv3d(in_channels, out_channels, kernel_size, stride, padding)),
            ('bn', nn.BatchNorm3d(out_channels)),
            ('relu', nn.ReLU(inplace=True))
        ]))

    def forward(self, x):
        return self.block(x)

class SSEncoder(nn.Module):
    """Encoder network with shared features for reconstruction and classification."""
    def __init__(self, latent_dim=256):
        super().__init__()

        # Initial feature extraction
        self.init_conv = ConvBlock(1, 16)  # 128 -> 128

        # Downsampling path
        self.down1 = nn.Sequential(
            ConvBlock(16, 32, stride=2),    # 128 -> 64
            ConvBlock(32, 32)
        )

        self.down2 = nn.Sequential(
            ConvBlock(32, 64, stride=2),    # 64 -> 32
            ConvBlock(64, 64)
        )

        self.down3 = nn.Sequential(
            ConvBlock(64, 128, stride=2),   # 32 -> 16
            ConvBlock(128, 128)
        )

        self.down4 = nn.Sequential(
            ConvBlock(128, 256, stride=2),  # 16 -> 8
            ConvBlock(256, 256)
        )

        # Project to latent space
        self.flatten_size = 256 * 8 * 8 * 8
        self.fc = nn.Linear(self.flatten_size, latent_dim)

    def forward(self, x):
        x = self.init_conv(x)
        d1 = self.down1(x)
        d2 = self.down2(d1)
        d3 = self.down3(d2)
        d4 = self.down4(d3)

        # Flatten and project to latent space
        flat = torch.flatten(d4, start_dim=1)
        z = self.fc(flat)

        return z, (d1, d2, d3, d4)

class SSDecoder(nn.Module):
    """Decoder network for reconstruction."""
    def __init__(self, latent_dim=256):
        super().__init__()

        self.flatten_size = 256 * 8 * 8 * 8
        self.fc = nn.Linear(latent_dim, self.flatten_size)

        # Upsampling path
        self.up1 = nn.Sequential(
            nn.ConvTranspose3d(256, 128, kernel_size=2, stride=2),  # 8 -> 16
            ConvBlock(128, 128)
        )

        self.up2 = nn.Sequential(
            nn.ConvTranspose3d(128, 64, kernel_size=2, stride=2),   # 16 -> 32
            ConvBlock(64, 64)
        )

        self.up3 = nn.Sequential(
            nn.ConvTranspose3d(64, 32, kernel_size=2, stride=2),    # 32 -> 64
            ConvBlock(32, 32)
        )

        self.up4 = nn.Sequential(
            nn.ConvTranspose3d(32, 16, kernel_size=2, stride=2),    # 64 -> 128
            ConvBlock(16, 16)
        )

        # Final convolution
        self.final_conv = nn.Conv3d(16, 1, kernel_size=1)

    def forward(self, z, skip_connections):
        # Reshape from latent space
        x = self.fc(z)
        x = x.view(-1, 256, 8, 8, 8)

        # Unpack skip connections
        d1, d2, d3, d4 = skip_connections

        # Upsampling with skip connections
        x = self.up1(x + d4)
        x = self.up2(x + d3)
        x = self.up3(x + d2)
        x = self.up4(x + d1)

        # Final convolution with sigmoid activation
        x = torch.sigmoid(self.final_conv(x))

        return x

# Cell 28: Training Configuration
class SSAEConfig:
    """Configuration for semi-supervised training."""
    def __init__(self):
        # Model parameters
        self.latent_dim = 256
        self.num_classes = 3
        self.learning_rate = 1e-4

        # Loss weights
        self.recon_weight = 1.0
        self.class_weight = 0.5

        # Training parameters
        self.batch_size = 8
        self.epochs = 100
        self.accumulation_steps = 4

        # Early stopping
        self.patience = 10
        self.min_delta = 1e-6

        # Optimization
        self.use_amp = True
        self.num_workers = 4
        self.pin_memory = True

        # Checkpoint configuration
        self.checkpoint_dir = 'checkpoints'
        self.model_name = 'ssae'
        Path(self.checkpoint_dir).mkdir(parents=True, exist_ok=True)

class SSAELoss:
    """Combined loss for semi-supervised training."""
    def __init__(self, recon_weight=1.0, class_weight=0.5):
        self.recon_weight = recon_weight
        self.class_weight = class_weight
        self.recon_criterion = nn.MSELoss()
        self.class_criterion = nn.CrossEntropyLoss()

    def __call__(self, recon, target, class_pred, class_target):
        recon_loss = self.recon_criterion(recon, target)
        class_loss = self.class_criterion(class_pred, class_target)

        total_loss = (self.recon_weight * recon_loss +
                     self.class_weight * class_loss)

        return total_loss, recon_loss, class_loss

# Cell 29: Label Processing
def process_labels(labels):
    """Convert string labels to tensor indices."""
    label_map = {'Control': 0, 'PD': 1, 'SWEDD': 2}
    return torch.tensor([label_map[label] for label in labels])

def create_class_weights(dataloader):
    """Calculate class weights for balanced training."""
    label_counts = torch.zeros(3)
    for batch in dataloader:
        labels = process_labels(batch['label'])
        for label in labels:
            label_counts[label] += 1

    total = label_counts.sum()
    class_weights = total / (3 * label_counts)
    return class_weights

# Cell 30: Checkpoint Handler
class SSAECheckpointHandler:
    """Handles saving and loading of model checkpoints."""
    def __init__(self, checkpoint_dir, model_name):
        self.checkpoint_dir = Path(checkpoint_dir)
        self.model_name = model_name
        self.checkpoint_path = self.checkpoint_dir / f"{model_name}_checkpoint.pth"
        self.metadata_path = self.checkpoint_dir / f"{model_name}_metadata.json"

    def save(self, model, optimizer, scheduler, epoch, metrics):
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
            'metrics': metrics
        }
        torch.save(checkpoint, self.checkpoint_path)

        metadata = {
            'last_epoch': epoch,
            'metrics': metrics,
            'timestamp': time.strftime("%Y-%m-%d %H:%M:%S")
        }
        with open(self.metadata_path, 'w') as f:
            json.dump(metadata, f, indent=4)

    def load(self, model, optimizer, scheduler):
        if not self.checkpoint_path.exists():
            return None

        checkpoint = torch.load(self.checkpoint_path)
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        if scheduler and checkpoint['scheduler_state_dict']:
            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])

        return checkpoint['epoch'], checkpoint['metrics']

# Cell 31: Training Loop
import torch.cuda.amp as amp
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns

def train_ssae(model, train_loader, val_loader, config=None):
    """Training loop with memory optimization for NVIDIA 4070Ti."""
    if config is None:
        config = SSAEConfig()

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    # Initialize components
    criterion = SSAELoss(config.recon_weight, config.class_weight)
    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)
    scaler = amp.GradScaler(enabled=config.use_amp)
    checkpoint_handler = SSAECheckpointHandler(config.checkpoint_dir, config.model_name)

    # Training tracking
    best_val_loss = float('inf')
    patience_counter = 0
    metrics = {
        'train_losses': [], 'val_losses': [],
        'train_recon': [], 'val_recon': [],
        'train_class': [], 'val_class': [],
        'train_acc': [], 'val_acc': []
    }

    # Load checkpoint if available
    start_epoch = 0
    checkpoint_data = checkpoint_handler.load(model, optimizer, scheduler)
    if checkpoint_data:
        start_epoch, metrics = checkpoint_data
        print(f"Resuming training from epoch {start_epoch + 1}")

    try:
        for epoch in range(start_epoch, config.epochs):
            # Training phase
            model.train()
            train_loss = train_recon = train_class = 0
            correct_preds = total_preds = 0

            train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{config.epochs} [Train]')

            for i, batch in enumerate(train_pbar):
                try:
                    volumes = batch['volume'].to(device, non_blocking=True)
                    labels = process_labels(batch['label']).to(device)

                    # Forward pass with mixed precision
                    with amp.autocast(enabled=config.use_amp):
                        recon, class_pred, _ = model(volumes)
                        loss, recon_l, class_l = criterion(recon, volumes, class_pred, labels)
                        loss = loss / config.accumulation_steps

                    # Backward pass
                    scaler.scale(loss).backward()

                    # Gradient accumulation
                    if (i + 1) % config.accumulation_steps == 0:
                        scaler.step(optimizer)
                        scaler.update()
                        optimizer.zero_grad()

                    # Track metrics
                    train_loss += loss.item() * config.accumulation_steps
                    train_recon += recon_l.item()
                    train_class += class_l.item()

                    # Track accuracy
                    pred = class_pred.argmax(dim=1)
                    correct_preds += (pred == labels).sum().item()
                    total_preds += labels.size(0)

                    # Update progress bar
                    train_pbar.set_postfix({
                        'loss': loss.item() * config.accumulation_steps,
                        'recon': recon_l.item(),
                        'class': class_l.item(),
                        'acc': correct_preds/total_preds
                    })

                    # Clean up
                    del volumes, labels, recon, class_pred
                    torch.cuda.empty_cache()

                except RuntimeError as e:
                    if "out of memory" in str(e):
                        print(f"\nOOM in batch {i}. Cleaning up...")
                        if 'volumes' in locals():
                            del volumes
                        if 'recon' in locals():
                            del recon
                        torch.cuda.empty_cache()
                        continue
                    raise e

            # Validation phase
            model.eval()
            val_loss = val_recon = val_class = 0
            val_correct = val_total = 0
            all_preds = []
            all_labels = []

            val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{config.epochs} [Val]')


            with torch.no_grad():
                for batch in val_pbar:
                    try:
                        volumes = batch['volume'].to(device)
                        labels = process_labels(batch['label']).to(device)

                        recon, class_pred, _ = model(volumes)
                        loss, recon_l, class_l = criterion(recon, volumes, class_pred, labels)

                        val_loss += loss.item()
                        val_recon += recon_l.item()
                        val_class += class_l.item()

                        # Track predictions for metrics
                        pred = class_pred.argmax(dim=1)
                        val_correct += (pred == labels).sum().item()
                        val_total += labels.size(0)

                        all_preds.extend(pred.cpu().numpy())
                        all_labels.extend(labels.cpu().numpy())

                        val_pbar.set_postfix({
                            'loss': loss.item(),
                            'acc': val_correct/val_total
                        })

                        del volumes, labels, recon, class_pred
                        torch.cuda.empty_cache()

                    except RuntimeError as e:
                        if "out of memory" in str(e):
                            print("\nOOM during validation. Cleaning up...")
                            if 'volumes' in locals():
                                del volumes
                            if 'recon' in locals():
                                del recon
                            torch.cuda.empty_cache()
                            continue
                        raise e

            # Calculate epoch metrics
            train_metrics = {
                'loss': train_loss / len(train_loader),
                'recon': train_recon / len(train_loader),
                'class': train_class / len(train_loader),
                'acc': correct_preds / total_preds
            }

            val_metrics = {
                'loss': val_loss / len(val_loader),
                'recon': val_recon / len(val_loader),
                'class': val_class / len(val_loader),
                'acc': val_correct / val_total
            }

            # Update tracking metrics
            metrics['train_losses'].append(train_metrics['loss'])
            metrics['val_losses'].append(val_metrics['loss'])
            metrics['train_recon'].append(train_metrics['recon'])
            metrics['val_recon'].append(val_metrics['recon'])
            metrics['train_class'].append(train_metrics['class'])
            metrics['val_class'].append(val_metrics['class'])
            metrics['train_acc'].append(train_metrics['acc'])
            metrics['val_acc'].append(val_metrics['acc'])

            # Update learning rate
            scheduler.step(val_metrics['loss'])

            # Save checkpoint
            checkpoint_handler.save(model, optimizer, scheduler, epoch, metrics)

            # Print epoch summary
            print(f"\nEpoch {epoch+1}/{config.epochs}")
            print("Training Metrics:")
            print(f"Loss: {train_metrics['loss']:.6f}")
            print(f"Recon Loss: {train_metrics['recon']:.6f}")
            print(f"Class Loss: {train_metrics['class']:.6f}")
            print(f"Accuracy: {train_metrics['acc']:.2%}")

            print("\nValidation Metrics:")
            print(f"Loss: {val_metrics['loss']:.6f}")
            print(f"Recon Loss: {val_metrics['recon']:.6f}")
            print(f"Class Loss: {val_metrics['class']:.6f}")
            print(f"Accuracy: {val_metrics['acc']:.2%}")

            # Early stopping check
            if val_metrics['loss'] < best_val_loss - config.min_delta:
                best_val_loss = val_metrics['loss']
                patience_counter = 0
                # Save best model
                torch.save(model.state_dict(),
                         os.path.join(config.checkpoint_dir, f'{config.model_name}_best.pth'))
            else:
                patience_counter += 1
                if patience_counter >= config.patience:
                    print(f"\nEarly stopping triggered at epoch {epoch+1}")
                    break

            # Print memory stats
            print("\nGPU Memory Stats:")
            print_gpu_memory_stats()

    except KeyboardInterrupt:
        print("\nTraining interrupted by user!")

    finally:
        # Plot training history
        plot_training_history(metrics)

    return metrics

def plot_training_history(metrics):
    """Plot training and validation metrics history."""
    plt.figure(figsize=(15, 10))

    # Plot losses
    plt.subplot(2, 2, 1)
    plt.plot(metrics['train_losses'], label='Train Loss')
    plt.plot(metrics['val_losses'], label='Val Loss')
    plt.title('Total Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)

    # Plot reconstruction loss
    plt.subplot(2, 2, 2)
    plt.plot(metrics['train_recon'], label='Train Recon')
    plt.plot(metrics['val_recon'], label='Val Recon')
    plt.title('Reconstruction Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)

    # Plot classification loss
    plt.subplot(2, 2, 3)
    plt.plot(metrics['train_class'], label='Train Class')
    plt.plot(metrics['val_class'], label='Val Class')
    plt.title('Classification Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)

    # Plot accuracy
    plt.subplot(2, 2, 4)
    plt.plot(metrics['train_acc'], label='Train Acc')
    plt.plot(metrics['val_acc'], label='Val Acc')
    plt.title('Classification Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.show()

# Cell 32: Model Evaluation
def evaluate_model(model, val_loader, config=None):
    """Comprehensive model evaluation including reconstruction and classification metrics."""
    if config is None:
        config = SSAEConfig()

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    model.eval()

    # Initialize metrics
    val_recon_loss = 0
    val_class_loss = 0
    all_preds = []
    all_labels = []
    reconstructions = []
    originals = []

    criterion = SSAELoss(config.recon_weight, config.class_weight)

    with torch.no_grad():
        for batch in tqdm(val_loader, desc="Evaluating"):
            volumes = batch['volume'].to(device)
            labels = process_labels(batch['label']).to(device)

            recon, class_pred, _ = model(volumes)
            _, recon_l, class_l = criterion(recon, volumes, class_pred, labels)

            val_recon_loss += recon_l.item()
            val_class_loss += class_l.item()

            pred = class_pred.argmax(dim=1)
            all_preds.extend(pred.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

            # Store some examples for visualization
            if len(reconstructions) < 5:
                reconstructions.append(recon.cpu().numpy())
                originals.append(volumes.cpu().numpy())

    # Calculate average losses
    avg_recon_loss = val_recon_loss / len(val_loader)
    avg_class_loss = val_class_loss / len(val_loader)

    # Print classification report
    label_names = ['Control', 'PD', 'SWEDD']
    print("\nClassification Report:")
    print(classification_report(all_labels, all_preds, target_names=label_names))

    # Plot confusion matrix
    cm = confusion_matrix(all_labels, all_preds)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=label_names,
                yticklabels=label_names)
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.show()

    # Visualize reconstructions
    visualize_reconstructions(originals, reconstructions, label_names,
                            [label_names[l] for l in all_labels[:5]])

    return {
        'recon_loss': avg_recon_loss,
        'class_loss': avg_class_loss,
        'predictions': all_preds,
        'true_labels': all_labels
    }

def visualize_reconstructions(originals, reconstructions, label_names, true_labels):
    """Visualize original vs reconstructed volumes."""
    num_samples = len(originals)
    plt.figure(figsize=(15, 3*num_samples))

    for i in range(num_samples):
        orig = originals[i][0]
        recon = reconstructions[i][0]

        # Get middle slices
        orig_axial = orig[orig.shape[0]//2, :, :]
        orig_sagittal = orig[:, orig.shape[1]//2, :]
        orig_coronal = orig[:, :, orig.shape[2]//2]

        recon_axial = recon[recon.shape[0]//2, :, :]
        recon_sagittal = recon[:, recon.shape[1]//2, :]
        recon_coronal = recon[:, :, recon.shape[2]//2]

        # Plot original
        plt.subplot(num_samples, 6, i*6 + 1)
        plt.imshow(orig_axial, cmap='gray')
        plt.title(f'Original Axial\n{true_labels[i]}' if i == 0 else '')
        plt.axis('off')

        plt.subplot(num_samples, 6, i*6 + 2)
        plt.imshow(orig_sagittal, cmap='gray')
        plt.title('Original Sagittal' if i == 0 else '')
        plt.axis('off')

        plt.subplot(num_samples, 6, i*6 + 3)
        plt.imshow(orig_coronal, cmap='gray')
        plt.title('Original Coronal' if i == 0 else '')
        plt.axis('off')

        # Plot reconstruction
        plt.subplot(num_samples, 6, i*6 + 4)
        plt.imshow(recon_axial, cmap='gray')
        plt.title('Reconstructed Axial' if i == 0 else '')
        plt.axis('off')

        plt.subplot(num_samples, 6, i*6 + 5)
        plt.imshow(recon_sagittal, cmap='gray')
        plt.title('Reconstructed Sagittal' if i == 0 else '')
        plt.axis('off')

        plt.subplot(num_samples, 6, i*6 + 6)
        plt.imshow(recon_coronal, cmap='gray')
        plt.title('Reconstructed Coronal' if i == 0 else '')
        plt.axis('off')

    plt.tight_layout()
    plt.show()

"""### Training"""

# Cell 33: Smart Training Cell
def initialize_training():
    """Initialize or resume training based on checkpoint existence."""
    config = SSAEConfig()
    model = SemiSupervisedAE(
        latent_dim=config.latent_dim,
        num_classes=config.num_classes
    )

    # Check for existing checkpoint
    checkpoint_path = Path(config.checkpoint_dir) / f"{config.model_name}_checkpoint.pth"
    if checkpoint_path.exists():
        print("\nFound existing checkpoint. Resuming training...")
    else:
        print("\nNo checkpoint found. Starting new training...")

    return model, config

if __name__ == "__main__":
    training_completed = False

    try:
        # Initialize or resume
        model, config = initialize_training()

        # Start or resume training
        metrics = train_ssae(model, train_loader, val_loader, config)

        # Set flag for successful completion
        training_completed = True

    except KeyboardInterrupt:
        print("\nTraining interrupted by user. Progress has been saved to checkpoint.")
        print("Run this cell again to resume training from the last checkpoint.")
    except Exception as e:
        print(f"\nError during execution: {str(e)}")
        import traceback
        traceback.print_exc()

    # Only run evaluation if training completed normally
    if training_completed:
        try:
            print("\nTraining completed successfully. Starting evaluation...")
            evaluation_results = evaluate_model(model, val_loader, config)
            print("\nEvaluation completed. Results saved in 'evaluation_results'")
        except Exception as e:
            print(f"\nError during evaluation: {str(e)}")
            traceback.print_exc()

# Cell 34: Semi-Supervised Model Reconstruction and Evaluation
import torch
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path
from tqdm import tqdm
import seaborn as sns
from skimage.metrics import structural_similarity as ssim

def load_trained_ssae(checkpoint_dir, model_name='ssae'):
    """
    Load trained semi-supervised model and metadata without modifying previous cells.

    Args:
        checkpoint_dir (str): Directory containing checkpoints
        model_name (str): Name of the model checkpoint

    Returns:
        model: Loaded model
        metadata: Training metadata
    """
    checkpoint_path = Path(checkpoint_dir) / f"{model_name}_checkpoint.pth"
    metadata_path = Path(checkpoint_dir) / f"{model_name}_metadata.json"

    # Initialize model with same configuration as training
    model = SemiSupervisedAE(latent_dim=256, num_classes=3)

    # Load checkpoint
    checkpoint = torch.load(checkpoint_path)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()

    # Load metadata if available
    metadata = None
    if metadata_path.exists():
        with open(metadata_path, 'r') as f:
            metadata = json.load(f)

    return model, metadata

def compute_slice_metrics(original_slice, reconstructed_slice):
    """
    Compute quality metrics for a single slice.

    Args:
        original_slice: Original image slice
        reconstructed_slice: Reconstructed image slice

    Returns:
        dict: Dictionary containing MSE and SSIM metrics
    """
    mse = np.mean((original_slice - reconstructed_slice) ** 2)
    ssim_score = ssim(original_slice, reconstructed_slice, data_range=1.0)

    return {
        'MSE': mse,
        'SSIM': ssim_score
    }

# Cell 35: Reconstruction Visualization Functions
def extract_orthogonal_slices(volume):
    """
    Extract axial, sagittal, and coronal slices from the middle of the volume.

    Args:
        volume: 3D volume array

    Returns:
        tuple: (axial_slice, sagittal_slice, coronal_slice)
    """
    d, h, w = volume.shape
    axial = volume[d//2, :, :]      # Top-down view
    sagittal = volume[:, h//2, :]    # Side view
    coronal = volume[:, :, w//2]     # Front view

    return axial, sagittal, coronal

def visualize_reconstruction_comparison(original, reconstructed, metrics, title="Sample Reconstruction Comparison"):
    """
    Create side-by-side visualization of original and reconstructed slices.

    Args:
        original: Original volume
        reconstructed: Reconstructed volume
        metrics: Dictionary containing evaluation metrics
        title: Plot title
    """
    # Extract slices
    orig_axial, orig_sagittal, orig_coronal = extract_orthogonal_slices(original)
    recon_axial, recon_sagittal, recon_coronal = extract_orthogonal_slices(reconstructed)

    # Create figure
    fig = plt.figure(figsize=(15, 8))
    plt.suptitle(title, fontsize=16, y=1.05)

    # Plot original slices
    views = ['Axial', 'Sagittal', 'Coronal']
    orig_slices = [orig_axial, orig_sagittal, orig_coronal]
    recon_slices = [recon_axial, recon_sagittal, recon_coronal]

    for idx, (view, orig_slice, recon_slice) in enumerate(zip(views, orig_slices, recon_slices)):
        # Original
        plt.subplot(2, 3, idx + 1)
        plt.imshow(orig_slice, cmap='gray')
        plt.title(f'Original {view}')
        plt.axis('off')

        # Reconstructed
        plt.subplot(2, 3, idx + 4)
        plt.imshow(recon_slice, cmap='gray')
        plt.title(f'Reconstructed {view}\nMSE: {metrics[view]["MSE"]:.4f}\nSSIM: {metrics[view]["SSIM"]:.4f}')
        plt.axis('off')

    plt.tight_layout()
    plt.show()

# Cell 36: Evaluation Pipeline
def evaluate_ssae_reconstruction(model, val_loader, samples_per_group=2):
    """
    Evaluate semi-supervised model reconstruction on validation set.

    Args:
        model: Trained semi-supervised model
        val_loader: Validation data loader
        samples_per_group: Number of samples to visualize per patient group
    """
    device = next(model.parameters()).device
    model.eval()

    # Initialize storage for group-wise metrics
    group_metrics = {
        'Control': {'mse': [], 'samples': 0},
        'PD': {'mse': [], 'samples': 0},
        'SWEDD': {'mse': [], 'samples': 0}
    }

    # Track visualized samples per group
    visualized_samples = {
        'Control': 0,
        'PD': 0,
        'SWEDD': 0
    }

    print("Evaluating reconstruction quality...")

    with torch.no_grad():
        for batch in tqdm(val_loader):
            # Check if we have enough samples from each group
            if all(count >= samples_per_group for count in visualized_samples.values()):
                break

            # Move data to GPU
            volumes = batch['volume'].to(device)
            labels = batch['label']  # Original string labels

            # Get reconstructions
            reconstructions, classifications, _ = model(volumes)

            # Process each sample in the batch
            for idx in range(volumes.shape[0]):
                current_group = labels[idx]

                # Skip if we already have enough samples from this group
                if visualized_samples[current_group] >= samples_per_group:
                    continue

                # Get original and reconstructed volumes
                orig_vol = volumes[idx, 0].cpu().numpy()
                recon_vol = reconstructions[idx, 0].cpu().numpy()

                # Calculate metrics for each view
                metrics = {}
                for view, (orig_slice, recon_slice) in zip(
                    ['Axial', 'Sagittal', 'Coronal'],
                    zip(*[extract_orthogonal_slices(vol) for vol in [orig_vol, recon_vol]])
                ):
                    metrics[view] = compute_slice_metrics(orig_slice, recon_slice)

                # Visualize comparison
                visualize_reconstruction_comparison(
                    orig_vol,
                    recon_vol,
                    metrics,
                    f"Patient Group: {current_group} - Sample {visualized_samples[current_group] + 1}"
                )

                # Update group metrics
                mse = np.mean((volumes[idx, 0].cpu().numpy() -
                             reconstructions[idx, 0].cpu().numpy()) ** 2)
                group_metrics[current_group]['mse'].append(mse)
                group_metrics[current_group]['samples'] += 1
                visualized_samples[current_group] += 1

            # Clean up GPU memory
            del volumes, reconstructions
            torch.cuda.empty_cache()

    # Print group-wise metrics
    print("\nReconstruction Metrics by Patient Group:")
    for group in group_metrics:
        if group_metrics[group]['samples'] > 0:
            avg_mse = np.mean(group_metrics[group]['mse'])
            print(f"\n{group}:")
            print(f"Number of samples: {group_metrics[group]['samples']}")
            print(f"Average MSE: {avg_mse:.6f}")
            print(f"RMSE: {np.sqrt(avg_mse):.6f}")

    return group_metrics

# Cell 37: Run Evaluation
if __name__ == "__main__":
    try:
        # Load trained model
        print("Loading trained semi-supervised model...")
        model, metadata = load_trained_ssae('checkpoints')
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = model.to(device)

        # Run evaluation with samples per group
        print("\nStarting reconstruction evaluation...")
        group_metrics = evaluate_ssae_reconstruction(model, val_loader, samples_per_group=2)

        # Visualize group-wise metrics
        groups = list(group_metrics.keys())
        mse_values = [np.mean(group_metrics[g]['mse']) for g in groups]

        plt.figure(figsize=(10, 6))
        plt.bar(groups, mse_values)
        plt.title('Average MSE by Patient Group')
        plt.ylabel('Mean Squared Error')
        plt.grid(True, alpha=0.3)
        plt.show()

        # Clean up
        torch.cuda.empty_cache()
        print("\nEvaluation completed successfully!")

    except Exception as e:
        print(f"Error during evaluation: {str(e)}")
        import traceback
        traceback.print_exc()

"""## 4. VAE Semi-Supervised

### Model Setup
"""

# Cell 38: Semi-Supervised VAE Model Architecture
import torch
import torch.nn as nn
import torch.nn.functional as F
from collections import OrderedDict

class SSVAE(nn.Module):
    """
    Semi-Supervised Variational Autoencoder combining reconstruction,
    latent sampling, and classification objectives.
    Optimized for 128³ medical volumes on NVIDIA 4070Ti.
    """
    def __init__(self, latent_dim=256, num_classes=3):
        super().__init__()
        self.encoder = SSVAEEncoder(latent_dim)
        self.decoder = SSVAEDecoder(latent_dim)
        self.classifier = nn.Sequential(
            nn.Linear(latent_dim, 128),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(128, num_classes)
        )
        torch.backends.cudnn.benchmark = True

    def reparameterize(self, mu, log_var):
        """Reparameterization trick for VAE sampling."""
        if self.training:
            std = torch.exp(0.5 * log_var)
            eps = torch.randn_like(std)
            return mu + eps * std
        return mu

    def forward(self, x):
        # Encode input
        mu, log_var, skip_connections = self.encoder(x)

        # Sample from latent distribution
        z = self.reparameterize(mu, log_var)

        # Generate reconstruction and classification
        reconstruction = self.decoder(z, skip_connections)
        classification = self.classifier(z)

        return reconstruction, classification, mu, log_var

class ConvBlock(nn.Module):
    """Memory-efficient convolutional block."""
    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):
        super().__init__()
        self.block = nn.Sequential(OrderedDict([
            ('conv', nn.Conv3d(in_channels, out_channels, kernel_size, stride, padding)),
            ('bn', nn.BatchNorm3d(out_channels)),
            ('relu', nn.ReLU(inplace=True))
        ]))

    def forward(self, x):
        return self.block(x)

class SSVAEEncoder(nn.Module):
    """Encoder network with probabilistic latent space."""
    def __init__(self, latent_dim=256):
        super().__init__()

        # Initial feature extraction
        self.init_conv = ConvBlock(1, 16)  # 128 -> 128

        # Downsampling path
        self.down1 = nn.Sequential(
            ConvBlock(16, 32, stride=2),    # 128 -> 64
            ConvBlock(32, 32)
        )

        self.down2 = nn.Sequential(
            ConvBlock(32, 64, stride=2),    # 64 -> 32
            ConvBlock(64, 64)
        )

        self.down3 = nn.Sequential(
            ConvBlock(64, 128, stride=2),   # 32 -> 16
            ConvBlock(128, 128)
        )

        self.down4 = nn.Sequential(
            ConvBlock(128, 256, stride=2),  # 16 -> 8
            ConvBlock(256, 256)
        )

        # Project to latent parameters
        self.flatten_size = 256 * 8 * 8 * 8
        self.fc_mu = nn.Linear(self.flatten_size, latent_dim)
        self.fc_var = nn.Linear(self.flatten_size, latent_dim)

    def forward(self, x):
        x = self.init_conv(x)
        d1 = self.down1(x)
        d2 = self.down2(d1)
        d3 = self.down3(d2)
        d4 = self.down4(d3)

        # Flatten and project to latent parameters
        flat = torch.flatten(d4, start_dim=1)
        mu = self.fc_mu(flat)
        log_var = self.fc_var(flat)

        return mu, log_var, (d1, d2, d3, d4)

class SSVAEDecoder(nn.Module):
    """Decoder network with skip connections."""
    def __init__(self, latent_dim=256):
        super().__init__()

        self.flatten_size = 256 * 8 * 8 * 8
        self.fc = nn.Linear(latent_dim, self.flatten_size)

        # Upsampling path
        self.up1 = nn.Sequential(
            nn.ConvTranspose3d(256, 128, kernel_size=2, stride=2),  # 8 -> 16
            ConvBlock(128, 128)
        )

        self.up2 = nn.Sequential(
            nn.ConvTranspose3d(128, 64, kernel_size=2, stride=2),   # 16 -> 32
            ConvBlock(64, 64)
        )

        self.up3 = nn.Sequential(
            nn.ConvTranspose3d(64, 32, kernel_size=2, stride=2),    # 32 -> 64
            ConvBlock(32, 32)
        )

        self.up4 = nn.Sequential(
            nn.ConvTranspose3d(32, 16, kernel_size=2, stride=2),    # 64 -> 128
            ConvBlock(16, 16)
        )

        # Final convolution
        self.final_conv = nn.Conv3d(16, 1, kernel_size=1)

    def forward(self, z, skip_connections):
        # Reshape from latent space
        x = self.fc(z)
        x = x.view(-1, 256, 8, 8, 8)

        # Unpack skip connections
        d1, d2, d3, d4 = skip_connections

        # Upsampling with skip connections
        x = self.up1(x + d4)
        x = self.up2(x + d3)
        x = self.up3(x + d2)
        x = self.up4(x + d1)

        # Final convolution with sigmoid activation
        x = torch.sigmoid(self.final_conv(x))

        return x

# Cell 39: Training Configuration and Loss Functions
import os
import json
import time
from pathlib import Path

class SSVAEConfig:
    """Configuration for SSVAE training."""
    def __init__(self):
        # Model parameters
        self.latent_dim = 256
        self.num_classes = 3
        self.learning_rate = 1e-4

        # Loss weights
        self.recon_weight = 1.0
        self.kl_weight = 0.1
        self.class_weight = 0.5

        # Training parameters
        self.batch_size = 8
        self.epochs = 100
        self.accumulation_steps = 4

        # Early stopping
        self.patience = 10
        self.min_delta = 1e-6

        # Optimization
        self.use_amp = True
        self.num_workers = 4
        self.pin_memory = True

        # Checkpoint configuration
        self.checkpoint_dir = 'checkpoints'
        self.model_name = 'ssvae'
        Path(self.checkpoint_dir).mkdir(parents=True, exist_ok=True)

class SSVAELoss:
    """Combined loss for semi-supervised VAE training."""
    def __init__(self, recon_weight=1.0, kl_weight=0.1, class_weight=0.5):
        self.recon_weight = recon_weight
        self.kl_weight = kl_weight
        self.class_weight = class_weight
        self.recon_criterion = nn.MSELoss()
        self.class_criterion = nn.CrossEntropyLoss()

    def __call__(self, recon, target, class_pred, class_target, mu, log_var):
        # Reconstruction loss
        recon_loss = self.recon_criterion(recon, target)

        # KL divergence
        kl_loss = -0.5 * torch.mean(1 + log_var - mu.pow(2) - log_var.exp())

        # Classification loss
        class_loss = self.class_criterion(class_pred, class_target)

        # Total loss
        total_loss = (self.recon_weight * recon_loss +
                     self.kl_weight * kl_loss +
                     self.class_weight * class_loss)

        return total_loss, recon_loss, kl_loss, class_loss

class SSVAECheckpointHandler:
    """Handles saving and loading of model checkpoints."""
    def __init__(self, checkpoint_dir, model_name):
        self.checkpoint_dir = Path(checkpoint_dir)
        self.model_name = model_name
        self.checkpoint_path = self.checkpoint_dir / f"{model_name}_checkpoint.pth"
        self.metadata_path = self.checkpoint_dir / f"{model_name}_metadata.json"

    def save(self, model, optimizer, scheduler, epoch, metrics):
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict() if optimizer else None,
            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
            'metrics': metrics
        }
        torch.save(checkpoint, self.checkpoint_path)

        metadata = {
            'last_epoch': epoch,
            'metrics': metrics,
            'timestamp': time.strftime("%Y-%m-%d %H:%M:%S")
        }
        with open(self.metadata_path, 'w') as f:
            json.dump(metadata, f, indent=4)

    def load(self, model, optimizer=None, scheduler=None):
        if not self.checkpoint_path.exists():
            return None

        checkpoint = torch.load(self.checkpoint_path)
        model.load_state_dict(checkpoint['model_state_dict'])

        if optimizer and 'optimizer_state_dict' in checkpoint:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

        if scheduler and 'scheduler_state_dict' in checkpoint:
            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])

        return checkpoint['epoch'], checkpoint['metrics']

# Cell 40: Training Loop
import torch
import torch.cuda.amp as amp
from tqdm import tqdm
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt

def train_ssvae(model, train_loader, val_loader, config=None):
    """Training loop optimized for NVIDIA RTX 4070 Ti."""
    if config is None:
        config = SSVAEConfig()

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    # Initialize components
    criterion = SSVAELoss(
        config.recon_weight,
        config.kl_weight,
        config.class_weight
    )
    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=5, verbose=True
    )
    scaler = amp.GradScaler(enabled=config.use_amp)
    checkpoint_handler = SSVAECheckpointHandler(config.checkpoint_dir, config.model_name)

    # Training tracking
    best_val_loss = float('inf')
    patience_counter = 0
    metrics = {
        'train_losses': [], 'val_losses': [],
        'train_recon': [], 'val_recon': [],
        'train_kl': [], 'val_kl': [],
        'train_class': [], 'val_class': [],
        'train_acc': [], 'val_acc': []
    }

    # Load checkpoint if available
    start_epoch = 0
    checkpoint_data = checkpoint_handler.load(model, optimizer, scheduler)
    if checkpoint_data:
        start_epoch, metrics = checkpoint_data
        print(f"Resuming training from epoch {start_epoch + 1}")

    try:
        for epoch in range(start_epoch, config.epochs):
            # Training phase
            model.train()
            train_loss = train_recon = train_kl = train_class = 0
            correct_preds = total_preds = 0

            train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{config.epochs} [Train]')

            for i, batch in enumerate(train_pbar):
                volumes = batch['volume'].to(device, non_blocking=True)
                labels = process_labels(batch['label']).to(device)

                # Mixed precision forward pass
                with amp.autocast(enabled=config.use_amp):
                    recon, class_pred, mu, log_var = model(volumes)
                    loss, recon_l, kl_l, class_l = criterion(
                        recon, volumes, class_pred, labels, mu, log_var
                    )
                    loss = loss / config.accumulation_steps

                # Backward pass
                scaler.scale(loss).backward()

                # Gradient accumulation
                if (i + 1) % config.accumulation_steps == 0:
                    scaler.step(optimizer)
                    scaler.update()
                    optimizer.zero_grad()

                # Track metrics
                train_loss += loss.item() * config.accumulation_steps
                train_recon += recon_l.item()
                train_kl += kl_l.item()
                train_class += class_l.item()

                pred = class_pred.argmax(dim=1)
                correct_preds += (pred == labels).sum().item()
                total_preds += labels.size(0)

                train_pbar.set_postfix({
                    'loss': loss.item() * config.accumulation_steps,
                    'acc': correct_preds/total_preds
                })

            # Validation phase
            model.eval()
            val_loss = val_recon = val_kl = val_class = 0
            val_correct = val_total = 0
            all_preds = []
            all_labels = []

            val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{config.epochs} [Val]')

            with torch.no_grad():
                for batch in val_pbar:
                    volumes = batch['volume'].to(device)
                    labels = process_labels(batch['label']).to(device)

                    recon, class_pred, mu, log_var = model(volumes)
                    loss, recon_l, kl_l, class_l = criterion(
                        recon, volumes, class_pred, labels, mu, log_var
                    )

                    val_loss += loss.item()
                    val_recon += recon_l.item()
                    val_kl += kl_l.item()
                    val_class += class_l.item()

                    pred = class_pred.argmax(dim=1)
                    val_correct += (pred == labels).sum().item()
                    val_total += labels.size(0)

                    all_preds.extend(pred.cpu().numpy())
                    all_labels.extend(labels.cpu().numpy())

                    val_pbar.set_postfix({
                        'loss': loss.item(),
                        'acc': val_correct/val_total
                    })

            # Calculate average metrics
            avg_train_loss = train_loss / len(train_loader)
            avg_train_recon = train_recon / len(train_loader)
            avg_train_kl = train_kl / len(train_loader)
            avg_train_class = train_class / len(train_loader)
            train_acc = correct_preds / total_preds

            avg_val_loss = val_loss / len(val_loader)
            avg_val_recon = val_recon / len(val_loader)
            avg_val_kl = val_kl / len(val_loader)
            avg_val_class = val_class / len(val_loader)
            val_acc = val_correct / val_total

            # Update tracking metrics
            metrics['train_losses'].append(avg_train_loss)
            metrics['val_losses'].append(avg_val_loss)
            metrics['train_recon'].append(avg_train_recon)
            metrics['val_recon'].append(avg_val_recon)
            metrics['train_kl'].append(avg_train_kl)
            metrics['val_kl'].append(avg_val_kl)
            metrics['train_class'].append(avg_train_class)
            metrics['val_class'].append(avg_val_class)
            metrics['train_acc'].append(train_acc)
            metrics['val_acc'].append(val_acc)

            # Update learning rate
            scheduler.step(avg_val_loss)

            # Save checkpoint
            checkpoint_handler.save(model, optimizer, scheduler, epoch, metrics)

            # Print epoch summary
            print(f"\nEpoch {epoch+1}/{config.epochs}")
            print("Training Metrics:")
            print(f"Loss: {avg_train_loss:.6f}")
            print(f"Recon Loss: {avg_train_recon:.6f}")
            print(f"KL Loss: {avg_train_kl:.6f}")
            print(f"Class Loss: {avg_train_class:.6f}")
            print(f"Accuracy: {train_acc:.2%}")

            print("\nValidation Metrics:")
            print(f"Loss: {avg_val_loss:.6f}")
            print(f"Recon Loss: {avg_val_recon:.6f}")
            print(f"KL Loss: {avg_val_kl:.6f}")
            print(f"Class Loss: {avg_val_class:.6f}")
            print(f"Accuracy: {val_acc:.2%}")

            # Print classification report for validation set
            label_names = ['Control', 'PD', 'SWEDD']
            print("\nClassification Report:")
            print(classification_report(all_labels, all_preds, target_names=label_names))

            # Early stopping check
            if avg_val_loss < best_val_loss - config.min_delta:
                best_val_loss = avg_val_loss
                patience_counter = 0
                # Save best model
                torch.save(model.state_dict(),
                         os.path.join(config.checkpoint_dir, f'{config.model_name}_best.pth'))
            else:
                patience_counter += 1
                if patience_counter >= config.patience:
                    print(f"\nEarly stopping triggered at epoch {epoch+1}")
                    break

            # Print memory stats
            print(f"\nGPU Memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB")

    except KeyboardInterrupt:
        print("\nTraining interrupted by user!")

    finally:
        # Plot training history
        plot_training_history(metrics)

    return metrics

def plot_training_history(metrics):
    """Plot training and validation metrics history."""
    plt.figure(figsize=(15, 10))

    # Plot losses
    plt.subplot(2, 2, 1)
    plt.plot(metrics['train_losses'], label='Train Loss')
    plt.plot(metrics['val_losses'], label='Val Loss')
    plt.title('Total Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    # Plot reconstruction loss
    plt.subplot(2, 2, 2)
    plt.plot(metrics['train_recon'], label='Train Recon')
    plt.plot(metrics['val_recon'], label='Val Recon')
    plt.title('Reconstruction Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    # Plot KL divergence
    plt.subplot(2, 2, 3)
    plt.plot(metrics['train_kl'], label='Train KL')
    plt.plot(metrics['val_kl'], label='Val KL')
    plt.title('KL Divergence')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    # Plot classification accuracy
    plt.subplot(2, 2, 4)
    plt.plot(metrics['train_acc'], label='Train Acc')
    plt.plot(metrics['val_acc'], label='Val Acc')
    plt.title('Classification Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.tight_layout()
    plt.show()

def process_labels(labels):
    """Convert string labels to tensor indices."""
    label_map = {'Control': 0, 'PD': 1, 'SWEDD': 2}
    return torch.tensor([label_map[label] for label in labels])

# Cell 41: Model Evaluation
def evaluate_ssvae(model, val_loader, config=None):
    """Comprehensive model evaluation including reconstruction and classification metrics."""
    if config is None:
        config = SSVAEConfig()

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    model.eval()

    # Initialize metrics
    val_recon_loss = 0
    val_kl_loss = 0
    val_class_loss = 0
    all_preds = []
    all_labels = []
    reconstructions = []
    originals = []
    latent_vectors = []

    criterion = SSVAELoss(
        config.recon_weight,
        config.kl_weight,
        config.class_weight
    )

    with torch.no_grad():
        for batch in tqdm(val_loader, desc="Evaluating"):
            volumes = batch['volume'].to(device)
            labels = process_labels(batch['label']).to(device)

            recon, class_pred, mu, log_var = model(volumes)
            loss, recon_l, kl_l, class_l = criterion(
                recon, volumes, class_pred, labels, mu, log_var
            )

            val_recon_loss += recon_l.item()
            val_kl_loss += kl_l.item()
            val_class_loss += class_l.item()

            pred = class_pred.argmax(dim=1)
            all_preds.extend(pred.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

            # Store latent vectors for visualization
            latent_vectors.append(mu.cpu().numpy())

            # Store some examples for visualization
            if len(reconstructions) < 5:
                reconstructions.append(recon.cpu().numpy())
                originals.append(volumes.cpu().numpy())

    # Calculate average losses
    avg_recon_loss = val_recon_loss / len(val_loader)
    avg_kl_loss = val_kl_loss / len(val_loader)
    avg_class_loss = val_class_loss / len(val_loader)

    # Print classification report
    label_names = ['Control', 'PD', 'SWEDD']
    print("\nClassification Report:")
    print(classification_report(all_labels, all_preds, target_names=label_names))

    # Plot confusion matrix
    cm = confusion_matrix(all_labels, all_preds)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=label_names,
                yticklabels=label_names)
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.show()

    # Visualize reconstructions
    visualize_reconstructions(originals, reconstructions, label_names,
                            [label_names[l] for l in all_labels[:5]])

    # Visualize latent space
    latent_vectors = np.concatenate(latent_vectors)
    visualize_latent_space(latent_vectors, all_labels, label_names)

    return {
        'recon_loss': avg_recon_loss,
        'kl_loss': avg_kl_loss,
        'class_loss': avg_class_loss,
        'predictions': all_preds,
        'true_labels': all_labels,
        'latent_vectors': latent_vectors
    }

def visualize_reconstructions(originals, reconstructions, label_names, true_labels):
    """Visualize original vs reconstructed volumes."""
    num_samples = len(originals)
    plt.figure(figsize=(15, 3*num_samples))

    for i in range(num_samples):
        orig = originals[i][0]
        recon = reconstructions[i][0]

        # Get middle slices
        orig_axial = orig[orig.shape[0]//2, :, :]
        orig_sagittal = orig[:, orig.shape[1]//2, :]
        orig_coronal = orig[:, :, orig.shape[2]//2]

        recon_axial = recon[recon.shape[0]//2, :, :]
        recon_sagittal = recon[:, recon.shape[1]//2, :]
        recon_coronal = recon[:, :, recon.shape[2]//2]

        # Plot original
        plt.subplot(num_samples, 6, i*6 + 1)
        plt.imshow(orig_axial, cmap='gray')
        plt.title(f'Original Axial\n{true_labels[i]}' if i == 0 else '')
        plt.axis('off')

        plt.subplot(num_samples, 6, i*6 + 2)
        plt.imshow(orig_sagittal, cmap='gray')
        plt.title('Original Sagittal' if i == 0 else '')
        plt.axis('off')

        plt.subplot(num_samples, 6, i*6 + 3)
        plt.imshow(orig_coronal, cmap='gray')
        plt.title('Original Coronal' if i == 0 else '')
        plt.axis('off')

        # Plot reconstruction
        plt.subplot(num_samples, 6, i*6 + 4)
        plt.imshow(recon_axial, cmap='gray')
        plt.title('Reconstructed Axial' if i == 0 else '')
        plt.axis('off')

        plt.subplot(num_samples, 6, i*6 + 5)
        plt.imshow(recon_sagittal, cmap='gray')
        plt.title('Reconstructed Sagittal' if i == 0 else '')
        plt.axis('off')

        plt.subplot(num_samples, 6, i*6 + 6)
        plt.imshow(recon_coronal, cmap='gray')
        plt.title('Reconstructed Coronal' if i == 0 else '')
        plt.axis('off')

    plt.tight_layout()
    plt.show()

def visualize_latent_space(latent_vectors, labels, label_names):
    """Visualize latent space using dimensionality reduction."""
    from sklearn.manifold import TSNE
    from sklearn.decomposition import PCA

    # Reduce dimensionality using t-SNE
    tsne = TSNE(n_components=2, random_state=42)
    reduced_vecs = tsne.fit_transform(latent_vectors)

    # Create scatter plot
    plt.figure(figsize=(10, 8))
    for i, label in enumerate(np.unique(labels)):
        mask = labels == label
        plt.scatter(reduced_vecs[mask, 0], reduced_vecs[mask, 1],
                   label=label_names[i], alpha=0.6)

    plt.title('t-SNE Visualization of Latent Space')
    plt.xlabel('t-SNE Dimension 1')
    plt.ylabel('t-SNE Dimension 2')
    plt.legend()
    plt.grid(True)
    plt.show()

    # Also visualize using PCA for comparison
    pca = PCA(n_components=2)
    pca_vecs = pca.fit_transform(latent_vectors)

    plt.figure(figsize=(10, 8))
    for i, label in enumerate(np.unique(labels)):
        mask = labels == label
        plt.scatter(pca_vecs[mask, 0], pca_vecs[mask, 1],
                   label=label_names[i], alpha=0.6)

    plt.title('PCA Visualization of Latent Space')
    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')
    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')
    plt.legend()
    plt.grid(True)
    plt.show()

"""### Training"""

# Cell 42: Smart Training Cell
def initialize_training():
    """Initialize or resume training based on checkpoint existence."""
    config = SSVAEConfig()
    model = SSVAE(
        latent_dim=config.latent_dim,
        num_classes=config.num_classes
    )

    # Check for existing checkpoint
    checkpoint_path = Path(config.checkpoint_dir) / f"{config.model_name}_checkpoint.pth"
    if checkpoint_path.exists():
        print("\nFound existing checkpoint. Resuming training...")
    else:
        print("\nNo checkpoint found. Starting new training...")

    return model, config

print("Initializing training process...")

try:
    # Initialize model and configuration
    model, config = initialize_training()

    # Move model to GPU and update loss function with class weights
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    print("\nStarting training process...")
    # Start training
    metrics = train_ssvae(model, train_loader, val_loader, config)

    print("\nTraining completed. Starting evaluation...")
    # Run evaluation
    evaluation_results = evaluate_ssvae(model, val_loader, config)
    print("\nEvaluation completed. Results saved in 'evaluation_results'")

except KeyboardInterrupt:
    print("\nTraining interrupted by user. Progress has been saved to checkpoint.")
    print("Run this cell again to resume training from the last checkpoint.")
except Exception as e:
    print(f"\nError during execution: {str(e)}")
    import traceback
    traceback.print_exc()

# Cell 43: Synthetic Data Generation
def load_trained_model(checkpoint_dir, model_name):
    """Load a trained model from a checkpoint."""
    config = SSVAEConfig()
    model = SSVAE(latent_dim=config.latent_dim, num_classes=config.num_classes)

    checkpoint_handler = SSVAECheckpointHandler(checkpoint_dir, model_name)
    checkpoint_data = checkpoint_handler.load(model)

    if checkpoint_data is None:
        raise FileNotFoundError(f"No checkpoint found for {model_name} in {checkpoint_dir}")

    print(f"Loaded model from epoch {checkpoint_data[0]}")

    return model, checkpoint_data[1]  # Return model and metrics


def generate_synthetic_samples(model, condition='PD', num_samples=5):
    """Generate synthetic brain scans by sampling from the latent space."""
    device = next(model.parameters()).device
    model.eval()

    # Get latent representations of real brains for the target condition
    real_vectors = []
    with torch.no_grad():
        for batch in val_loader:
            if len(real_vectors) >= 100:  # Collect up to 100 samples
                break

            volumes = batch['volume'].to(device)
            labels = process_labels(batch['label'])
            mask = labels == {'Control': 0, 'PD': 1, 'SWEDD': 2}[condition]

            if not mask.any():
                continue

            recon, _, mu, log_var = model(volumes[mask])
            real_vectors.append(mu.cpu().numpy())

    real_vectors = np.concatenate(real_vectors)

    # Fit a multivariate normal to the real vectors
    mu_vector = np.mean(real_vectors, axis=0)
    cov_matrix = np.cov(real_vectors.T)

    # Sample from the distribution
    synthetic_vectors = np.random.multivariate_normal(
        mu_vector,
        cov_matrix + 1e-6 * np.eye(cov_matrix.shape[0]),
        size=num_samples
    )

    # Generate images from the sampled vectors
    synthetic_vectors = torch.tensor(synthetic_vectors, dtype=torch.float32).to(device)
    with torch.no_grad():
        dummy_skip_connections = [
            torch.zeros(num_samples, 32, 64, 64, 64, device=device),
            torch.zeros(num_samples, 64, 32, 32, 32, device=device),
            torch.zeros(num_samples, 128, 16, 16, 16, device=device),
            torch.zeros(num_samples, 256, 8, 8, 8, device=device)
        ]
        synthetic_images = model.decoder(synthetic_vectors, dummy_skip_connections)

    # Visualize results
    plt.figure(figsize=(15, 3*num_samples))
    for i in range(num_samples):
        brain = synthetic_images[i, 0].cpu().numpy()

        # Get middle slices
        axial = brain[brain.shape[0]//2, :, :]
        sagittal = brain[:, brain.shape[1]//2, :]
        coronal = brain[:, :, brain.shape[2]//2]

        # Plot axial view
        plt.subplot(num_samples, 3, i*3 + 1)
        plt.imshow(axial, cmap='gray')
        plt.title(f'Synthetic {condition} - Axial' if i == 0 else '')
        plt.axis('off')

        # Plot sagittal view
        plt.subplot(num_samples, 3, i*3 + 2)
        plt.imshow(sagittal, cmap='gray')
        plt.title(f'Synthetic {condition} - Sagittal' if i == 0 else '')
        plt.axis('off')

        # Plot coronal view
        plt.subplot(num_samples, 3, i*3 + 3)
        plt.imshow(coronal, cmap='gray')
        plt.title(f'Synthetic {condition} - Coronal' if i == 0 else '')
        plt.axis('off')

    plt.tight_layout()
    plt.show()

    return synthetic_images

# Example usage for synthetic data generation
if __name__ == "__main__":
    # Load trained model
    model, _ = load_trained_model('checkpoints', 'ssvae')
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    # Generate synthetic samples for each condition
    print("\nGenerating synthetic Control samples...")
    synthetic_control = generate_synthetic_samples(model, condition='Control')

    print("\nGenerating synthetic PD samples...")
    synthetic_pd = generate_synthetic_samples(model, condition='PD')

    print("\nGenerating synthetic SWEDD samples...")
    synthetic_swedd = generate_synthetic_samples(model, condition='SWEDD')

# Cell 44: Reconstruction Quality Analysis
from skimage.metrics import structural_similarity as ssim

def load_trained_model(checkpoint_dir, model_name):
    """Load a trained model from a checkpoint."""
    config = SSVAEConfig()
    model = SSVAE(latent_dim=config.latent_dim, num_classes=config.num_classes)

    checkpoint_handler = SSVAECheckpointHandler(checkpoint_dir, model_name)
    checkpoint_data = checkpoint_handler.load(model, None, None)

    if checkpoint_data is None:
        raise FileNotFoundError(f"No checkpoint found for {model_name} in {checkpoint_dir}")

    print(f"Loaded model from epoch {checkpoint_data[0]}")

    return model, checkpoint_data[1]  # Return model and metrics

def analyze_reconstruction_quality(model, val_loader, num_samples=5):
    """Analyze reconstruction quality for a subset of validation samples."""
    device = next(model.parameters()).device
    model.eval()

    originals = []
    reconstructions = []
    labels = []

    with torch.no_grad():
        for batch in val_loader:
            if len(originals) >= num_samples:
                break

            volumes = batch['volume'].to(device)
            batch_labels = batch['label']

            recon, _, _, _ = model(volumes)

            originals.extend(volumes.cpu().numpy())
            reconstructions.extend(recon.cpu().numpy())
            labels.extend(batch_labels)

    # Compute metrics and visualize
    for i in range(num_samples):
        orig = originals[i][0]
        recon = reconstructions[i][0]

        # Compute metrics
        mse = np.mean((orig - recon) ** 2)
        ssim_score = ssim(orig, recon, data_range=orig.max() - orig.min())

        # Visualize
        plt.figure(figsize=(15, 5))

        # Original
        plt.subplot(231)
        plt.imshow(orig[orig.shape[0]//2], cmap='gray')
        plt.title('Original - Axial')
        plt.axis('off')

        plt.subplot(232)
        plt.imshow(orig[:, orig.shape[1]//2], cmap='gray')
        plt.title('Original - Sagittal')
        plt.axis('off')

        plt.subplot(233)
        plt.imshow(orig[:, :, orig.shape[2]//2], cmap='gray')
        plt.title('Original - Coronal')
        plt.axis('off')

        # Reconstruction
        plt.subplot(234)
        plt.imshow(recon[recon.shape[0]//2], cmap='gray')
        plt.title('Reconstructed - Axial')
        plt.axis('off')

        plt.subplot(235)
        plt.imshow(recon[:, recon.shape[1]//2], cmap='gray')
        plt.title('Reconstructed - Sagittal')
        plt.axis('off')

        plt.subplot(236)
        plt.imshow(recon[:, :, recon.shape[2]//2], cmap='gray')
        plt.title('Reconstructed - Coronal')
        plt.axis('off')

        plt.suptitle(f"Sample {i+1} ({labels[i]}) - MSE: {mse:.4f}, SSIM: {ssim_score:.4f}")
        plt.tight_layout()
        plt.show()

# Run reconstruction quality analysis
if __name__ == "__main__":
    model, _ = load_trained_model('checkpoints', 'ssvae')
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    print("\nAnalyzing reconstruction quality...")
    analyze_reconstruction_quality(model, val_loader)

# Cell 45: Latent Space Interpolation
def interpolate_latent_space(model, val_loader, start_condition='Control', end_condition='PD', num_steps=10):
    """Interpolate between two conditions in the latent space."""
    device = next(model.parameters()).device
    model.eval()

    # Get average latent vectors for start and end conditions
    condition_vectors = {start_condition: [], end_condition: []}

    with torch.no_grad():
        for batch in val_loader:
            volumes = batch['volume'].to(device)
            labels = batch['label']

            _, _, mu, _ = model(volumes)

            for label, vector in zip(labels, mu):
                if label in condition_vectors:
                    condition_vectors[label].append(vector.cpu().numpy())

    start_vector = np.mean(condition_vectors[start_condition], axis=0)
    end_vector = np.mean(condition_vectors[end_condition], axis=0)

    # Interpolate
    alphas = np.linspace(0, 1, num_steps)
    interpolated_vectors = [
        (1 - alpha) * start_vector + alpha * end_vector
        for alpha in alphas
    ]

    # Generate images from interpolated vectors
    interpolated_images = []
    with torch.no_grad():
        for vec in interpolated_vectors:
            latent = torch.tensor(vec, dtype=torch.float32).unsqueeze(0).to(device)
            dummy_skip_connections = [
                torch.zeros(1, 32, 64, 64, 64, device=device),
                torch.zeros(1, 64, 32, 32, 32, device=device),
                torch.zeros(1, 128, 16, 16, 16, device=device),
                torch.zeros(1, 256, 8, 8, 8, device=device)
            ]
            image = model.decoder(latent, dummy_skip_connections)
            interpolated_images.append(image.cpu().numpy())

    # Visualize interpolation
    plt.figure(figsize=(20, 4 * num_steps))
    for i, image in enumerate(interpolated_images):
        brain = image[0, 0]

        # Get middle slices
        axial = brain[brain.shape[0]//2, :, :]
        sagittal = brain[:, brain.shape[1]//2, :]
        coronal = brain[:, :, brain.shape[2]//2]

        # Plot
        plt.subplot(num_steps, 3, i*3 + 1)
        plt.imshow(axial, cmap='gray')
        plt.title(f'Step {i} - Axial' if i == 0 else '')
        plt.axis('off')

        plt.subplot(num_steps, 3, i*3 + 2)
        plt.imshow(sagittal, cmap='gray')
        plt.title(f'Step {i} - Sagittal' if i == 0 else '')
        plt.axis('off')

        plt.subplot(num_steps, 3, i*3 + 3)
        plt.imshow(coronal, cmap='gray')
        plt.title(f'Step {i} - Coronal' if i == 0 else '')
        plt.axis('off')

    plt.suptitle(f'Interpolation from {start_condition} to {end_condition}')
    plt.tight_layout()
    plt.show()

# Run latent space interpolation
if __name__ == "__main__":
    model, _ = load_trained_model('checkpoints', 'ssvae')
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    print("\nPerforming latent space interpolation...")
    interpolate_latent_space(model, val_loader)

# Cell 46: Uncertainty Estimation
def estimate_uncertainty(model, val_loader, num_samples=5, num_mc_samples=50):
    """Estimate model uncertainty using Monte Carlo dropout."""
    device = next(model.parameters()).device
    model.train()  # Set to train mode to enable dropout

    originals = []
    mean_reconstructions = []
    uncertainties = []
    labels = []

    with torch.no_grad():
        for batch in val_loader:
            if len(originals) >= num_samples:
                break

            volumes = batch['volume'].to(device)
            batch_labels = batch['label']

            # Perform multiple forward passes
            reconstructions = []
            for _ in range(num_mc_samples):
                recon, _, _, _ = model(volumes)
                reconstructions.append(recon.cpu().numpy())

            reconstructions = np.stack(reconstructions)
            mean_recon = np.mean(reconstructions, axis=0)
            uncertainty = np.std(reconstructions, axis=0)

            originals.extend(volumes.cpu().numpy())
            mean_reconstructions.extend(mean_recon)
            uncertainties.extend(uncertainty)
            labels.extend(batch_labels)

    # Visualize results
    for i in range(num_samples):
        orig = originals[i][0]
        recon = mean_reconstructions[i][0]
        uncert = uncertainties[i][0]

        plt.figure(figsize=(15, 5))

        # Original
        plt.subplot(331)
        plt.imshow(orig[orig.shape[0]//2], cmap='gray')
        plt.title('Original - Axial')
        plt.axis('off')

        plt.subplot(332)
        plt.imshow(orig[:, orig.shape[1]//2], cmap='gray')
        plt.title('Original - Sagittal')
        plt.axis('off')

        plt.subplot(333)
        plt.imshow(orig[:, :, orig.shape[2]//2], cmap='gray')
        plt.title('Original - Coronal')
        plt.axis('off')

        # Reconstruction
        plt.subplot(334)
        plt.imshow(recon[recon.shape[0]//2], cmap='gray')
        plt.title('Reconstructed - Axial')
        plt.axis('off')

        plt.subplot(335)
        plt.imshow(recon[:, recon.shape[1]//2], cmap='gray')
        plt.title('Reconstructed - Sagittal')
        plt.axis('off')

        plt.subplot(336)
        plt.imshow(recon[:, :, recon.shape[2]//2], cmap='gray')
        plt.title('Reconstructed - Coronal')
        plt.axis('off')

        # Uncertainty
        plt.subplot(337)
        plt.imshow(uncert[uncert.shape[0]//2], cmap='viridis')
        plt.title('Uncertainty - Axial')
        plt.colorbar()
        plt.axis('off')

        plt.subplot(338)
        plt.imshow(uncert[:, uncert.shape[1]//2], cmap='viridis')
        plt.title('Uncertainty - Sagittal')
        plt.colorbar()
        plt.axis('off')

        plt.subplot(339)
        plt.imshow(uncert[:, :, uncert.shape[2]//2], cmap='viridis')
        plt.title('Uncertainty - Coronal')
        plt.colorbar()
        plt.axis('off')

        plt.suptitle(f"Sample {i+1} ({labels[i]})")
        plt.tight_layout()
        plt.show()

# Run uncertainty estimation
if __name__ == "__main__":
    model, _ = load_trained_model('checkpoints', 'ssvae')
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    print("\nEstimating model uncertainty...")
    estimate_uncertainty(model, val_loader)

# Cell 47: Feature Importance Analysis
from sklearn.inspection import permutation_importance
from sklearn.svm import SVC

def analyze_feature_importance(model, val_loader):
    """Analyze the importance of latent space features for classification."""
    device = next(model.parameters()).device
    model.eval()

    latent_vectors = []
    labels = []

    # Extract latent vectors and labels
    with torch.no_grad():
        for batch in val_loader:
            volumes = batch['volume'].to(device)
            batch_labels = batch['label']

            _, _, mu, _ = model(volumes)
            latent_vectors.extend(mu.cpu().numpy())
            labels.extend(batch_labels)

    latent_vectors = np.array(latent_vectors)
    labels = np.array(labels)

    # Train a simple classifier on the latent space
    clf = SVC(kernel='rbf')
    clf.fit(latent_vectors, labels)

    # Perform permutation importance
    result = permutation_importance(clf, latent_vectors, labels, n_repeats=10, random_state=42)

    # Sort features by importance
    feature_importance = result.importances_mean
    sorted_idx = feature_importance.argsort()

    # Plot feature importance
    plt.figure(figsize=(10, 6))
    plt.barh(range(20), feature_importance[sorted_idx][-20:])
    plt.yticks(range(20), sorted_idx[-20:])
    plt.xlabel("Permutation Importance")
    plt.ylabel("Latent Dimension")
    plt.title("Top 20 Most Important Latent Dimensions")
    plt.tight_layout()
    plt.show()

    # Print top 10 important features
    print("\nTop 10 Most Important Latent Dimensions:")
    for idx in sorted_idx[-10:]:
        print(f"Dimension {idx}: {feature_importance[idx]:.4f}")

    return feature_importance

# Run feature importance analysis
if __name__ == "__main__":
    model, _ = load_trained_model('checkpoints', 'ssvae')
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    print("\nAnalyzing feature importance...")
    feature_importance = analyze_feature_importance(model, val_loader)

# Cell 48: Latent Space Clustering
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

def analyze_latent_clustering(model, val_loader, n_clusters=3):
    """Analyze clustering in the latent space."""
    device = next(model.parameters()).device
    model.eval()

    latent_vectors = []
    labels = []

    # Extract latent vectors and labels
    with torch.no_grad():
        for batch in val_loader:
            volumes = batch['volume'].to(device)
            batch_labels = batch['label']

            _, _, mu, _ = model(volumes)
            latent_vectors.extend(mu.cpu().numpy())
            labels.extend(batch_labels)

    latent_vectors = np.array(latent_vectors)
    labels = np.array(labels)

    # Perform K-means clustering
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    cluster_labels = kmeans.fit_predict(latent_vectors)

    # Calculate silhouette score
    silhouette_avg = silhouette_score(latent_vectors, cluster_labels)
    print(f"Silhouette Score: {silhouette_avg:.4f}")

    # Visualize clustering results
    tsne = TSNE(n_components=2, random_state=42)
    latent_2d = tsne.fit_transform(latent_vectors)

    plt.figure(figsize=(12, 5))

    # Plot true labels
    plt.subplot(121)
    scatter = plt.scatter(latent_2d[:, 0], latent_2d[:, 1], c=labels.astype('category').cat.codes, cmap='viridis')
    plt.colorbar(scatter)
    plt.title("True Labels")
    plt.xlabel("t-SNE 1")
    plt.ylabel("t-SNE 2")

    # Plot cluster assignments
    plt.subplot(122)
    scatter = plt.scatter(latent_2d[:, 0], latent_2d[:, 1], c=cluster_labels, cmap='viridis')
    plt.colorbar(scatter)
    plt.title("K-means Clusters")
    plt.xlabel("t-SNE 1")
    plt.ylabel("t-SNE 2")

    plt.tight_layout()
    plt.show()

    # Calculate cluster purity
    purity = calculate_cluster_purity(labels, cluster_labels)
    print(f"Cluster Purity: {purity:.4f}")

def calculate_cluster_purity(true_labels, cluster_labels):
    """Calculate the purity of clusters."""
    contingency_matrix = pd.crosstab(true_labels, cluster_labels)
    return np.sum(np.max(contingency_matrix, axis=0)) / np.sum(contingency_matrix)

# Run latent space clustering analysis
if __name__ == "__main__":
    model, _ = load_trained_model('checkpoints', 'ssvae')
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    print("\nAnalyzing latent space clustering...")
    analyze_latent_clustering(model, val_loader)

"""# Evaluation Phase"""

# Cell 49: Latent Space Analysis Utilities
import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import pandas as pd
from tqdm import tqdm

class LatentSpaceAnalyzer:
    """
    Comprehensive toolkit for analyzing latent space representations of
    neural network models trained on medical imaging data.
    """
    def __init__(self, model_name):
        self.model_name = model_name
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    def extract_latent_vectors(self, model, dataloader):
        """
        Extract latent vectors from a trained model using the validation dataset.

        Args:
            model: Trained model (AE, VAE, SSAE, or SSVAE)
            dataloader: DataLoader containing validation data

        Returns:
            latent_vectors: numpy array of latent vectors
            labels: list of corresponding labels
        """
        model.eval()
        latent_vectors = []
        labels = []

        with torch.no_grad():
            for batch in tqdm(dataloader, desc=f"Extracting latent vectors for {self.model_name}"):
                volumes = batch['volume'].to(self.device)
                batch_labels = batch['label']

                # Handle different model types
                if self.model_name in ['VAE', 'SSVAE']:
                    _, _, mu, _ = model(volumes)
                    latent_vectors.append(mu.cpu().numpy())
                else:  # AE or SSAE
                    _, _, z = model(volumes)
                    latent_vectors.append(z.cpu().numpy())

                labels.extend(batch_labels)

                # Clean up GPU memory
                del volumes
                torch.cuda.empty_cache()

        return np.vstack(latent_vectors), np.array(labels)

    def reduce_dimensionality(self, latent_vectors, method='tsne'):
        """
        Apply dimensionality reduction to latent vectors.

        Args:
            latent_vectors: numpy array of latent vectors
            method: 'tsne' or 'pca'

        Returns:
            reduced_vectors: 2D numpy array
        """
        if method.lower() == 'tsne':
            reducer = TSNE(n_components=2, random_state=42)
        else:
            reducer = PCA(n_components=2)

        return reducer.fit_transform(latent_vectors)

    def visualize_latent_space(self, reduced_vectors, labels, method='tsne'):
        """
        Create scatter plot of reduced dimensionality latent space.

        Args:
            reduced_vectors: 2D numpy array from dimensionality reduction
            labels: array of corresponding clinical labels
            method: string indicating the reduction method used
        """
        plt.figure(figsize=(12, 8))

        unique_labels = np.unique(labels)
        colors = plt.cm.tab10(np.linspace(0, 1, len(unique_labels)))

        for label, color in zip(unique_labels, colors):
            mask = labels == label
            plt.scatter(reduced_vectors[mask, 0], reduced_vectors[mask, 1],
                       label=label, color=color, alpha=0.7)

        plt.title(f'Latent Space Visualization ({method.upper()}) - {self.model_name}')
        plt.xlabel(f'{method.upper()} Dimension 1')
        plt.ylabel(f'{method.upper()} Dimension 2')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.show()

    def analyze_dimensions(self, latent_vectors, labels):
        """
        Perform statistical analysis on individual latent dimensions.

        Args:
            latent_vectors: numpy array of latent vectors
            labels: array of corresponding clinical labels
        """
        # Create DataFrame for analysis
        df = pd.DataFrame(latent_vectors)
        df['label'] = labels

        # Statistical summary
        stats = df.groupby('label').agg(['mean', 'std', 'min', 'max'])
        print(f"\nLatent Dimension Statistics for {self.model_name}:")
        print(stats)

        # Violin plots for top discriminative dimensions
        variances = np.var(latent_vectors, axis=0)
        top_dims = np.argsort(variances)[-5:]  # Top 5 most variable dimensions

        plt.figure(figsize=(15, 6))
        for i, dim in enumerate(top_dims):
            plt.subplot(1, 5, i+1)
            sns.violinplot(data=df, x='label', y=dim)
            plt.title(f'Dimension {dim}')
            plt.xticks(rotation=45)

        plt.suptitle(f'Top 5 Most Variable Dimensions - {self.model_name}')
        plt.tight_layout()
        plt.show()

    def perform_clustering(self, latent_vectors, true_labels, n_clusters=3):
        """
        Perform clustering analysis on latent vectors.

        Args:
            latent_vectors: numpy array of latent vectors
            true_labels: array of true clinical labels
            n_clusters: number of clusters for K-means

        Returns:
            dict containing clustering metrics
        """
        # Perform K-means clustering
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        cluster_labels = kmeans.fit_predict(latent_vectors)

        # Calculate silhouette score
        silhouette_avg = silhouette_score(latent_vectors, cluster_labels)

        # Calculate cluster purity
        contingency_matrix = pd.crosstab(true_labels, cluster_labels)
        purity = np.sum(np.max(contingency_matrix, axis=0)) / np.sum(contingency_matrix)

        # Visualize clustering results with t-SNE
        tsne_vectors = self.reduce_dimensionality(latent_vectors, 'tsne')

        plt.figure(figsize=(12, 5))

        # Plot true labels
        plt.subplot(121)
        scatter = plt.scatter(tsne_vectors[:, 0], tsne_vectors[:, 1],
                            c=pd.Categorical(true_labels).codes,
                            cmap='viridis')
        plt.colorbar(scatter)
        plt.title("True Labels")
        plt.xlabel("t-SNE 1")
        plt.ylabel("t-SNE 2")

        # Plot cluster assignments
        plt.subplot(122)
        scatter = plt.scatter(tsne_vectors[:, 0], tsne_vectors[:, 1],
                            c=cluster_labels, cmap='viridis')
        plt.colorbar(scatter)
        plt.title("K-means Clusters")
        plt.xlabel("t-SNE 1")
        plt.ylabel("t-SNE 2")

        plt.suptitle(f'Clustering Analysis - {self.model_name}')
        plt.tight_layout()
        plt.show()

        return {
            'silhouette_score': silhouette_avg,
            'purity': purity,
            'cluster_labels': cluster_labels
        }

# Cell 50: Comparative Analysis of Models
import torch
from collections import defaultdict

def load_model(model_class, checkpoint_path):
    """Load a trained model from checkpoint."""
    model = model_class()
    checkpoint = torch.load(checkpoint_path)
    model.load_state_dict(checkpoint['model_state_dict'])
    return model

def compare_models(models_dict, val_loader):
    """
    Perform comparative analysis across all models.

    Args:
        models_dict: Dictionary mapping model names to model instances
        val_loader: Validation data loader
    """
    results = defaultdict(dict)

    for model_name, model in models_dict.items():
        print(f"\nAnalyzing {model_name}...")
        analyzer = LatentSpaceAnalyzer(model_name)

        # Extract latent vectors
        latent_vectors, labels = analyzer.extract_latent_vectors(model, val_loader)

        # Dimensionality reduction
        print("\nPerforming dimensionality reduction...")
        tsne_vectors = analyzer.reduce_dimensionality(latent_vectors, 'tsne')
        pca_vectors = analyzer.reduce_dimensionality(latent_vectors, 'pca')

        # Visualize latent spaces
        print("\nVisualizing latent spaces...")
        analyzer.visualize_latent_space(tsne_vectors, labels, 'tsne')
        analyzer.visualize_latent_space(pca_vectors, labels, 'pca')

        # Analyze individual dimensions
        print("\nAnalyzing latent dimensions...")
        analyzer.analyze_dimensions(latent_vectors, labels)

        # Perform clustering
        print("\nPerforming clustering analysis...")
        clustering_results = analyzer.perform_clustering(latent_vectors, labels)

        # Store results
        results[model_name] = {
            'latent_vectors': latent_vectors,
            'clustering_metrics': clustering_results
        }

    return results

# Load and analyze all models
if __name__ == "__main__":
    try:
        print("Loading models from checkpoints...")
        models = {
            'AE': load_model(BaseAutoencoder, 'checkpoints/autoencoder_best.pth'),
            'VAE': load_model(VAE, 'checkpoints/vae_best.pth'),
            'SSAE': load_model(SemiSupervisedAE, 'checkpoints/ssae_best.pth'),
            'SSVAE': load_model(SSVAE, 'checkpoints/ssvae_best.pth')
        }

        # Move models to GPU
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        for model in models.values():
            model.to(device)
            model.eval()

        # Perform comparative analysis
        print("\nStarting comparative analysis...")
        results = compare_models(models, val_loader)

        # Print summary metrics
        print("\nSummary of Clustering Metrics:")
        for model_name in results:
            metrics = results[model_name]['clustering_metrics']
            print(f"\n{model_name}:")
            print(f"Silhouette Score: {metrics['silhouette_score']:.4f}")
            print(f"Cluster Purity: {metrics['purity']:.4f}")

    except Exception as e:
        print(f"Error during analysis: {str(e)}")
        import traceback
        traceback.print_exc()

# Cell 51: Generate Thesis Figures and Tables
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from matplotlib.gridspec import GridSpec
from sklearn.manifold import TSNE
import os

def generate_thesis_figures(results, save_dir='thesis_figures'):
    """
    Generate publication-quality figures for thesis.

    Args:
        results: Dictionary containing analysis results for each model
        save_dir: Directory to save generated figures
    """
    # Create save directory if it doesn't exist
    os.makedirs(save_dir, exist_ok=True)

    # Set publication-ready style
    plt.style.use('seaborn-whitegrid')
    sns.set_context("paper", font_scale=1.5)

    # 1. Combined Latent Space Visualization
    plt.figure(figsize=(20, 15))
    gs = GridSpec(2, 2)

    for i, (model_name, data) in enumerate(results.items()):
        plt.subplot(gs[i])
        vectors = data['latent_vectors']
        tsne = TSNE(n_components=2, random_state=42).fit_transform(vectors)

        scatter = plt.scatter(tsne[:, 0], tsne[:, 1],
                            c=pd.Categorical(labels).codes,
                            cmap='viridis', alpha=0.7)
        plt.title(f'{model_name} Latent Space')
        if i == 0:
            plt.colorbar(scatter, label='Clinical Group')

    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'latent_space_comparison.pdf'),
                dpi=300, bbox_inches='tight')
    plt.show()

    # 2. Clustering Metrics Comparison
    metrics_data = {
        'Model': [],
        'Silhouette Score': [],
        'Cluster Purity': []
    }

    for model_name, data in results.items():
        metrics = data['clustering_metrics']
        metrics_data['Model'].append(model_name)
        metrics_data['Silhouette Score'].append(metrics['silhouette_score'])
        metrics_data['Cluster Purity'].append(metrics['purity'])

    metrics_df = pd.DataFrame(metrics_data)

    plt.figure(figsize=(12, 6))
    metrics_df.plot(x='Model', kind='bar', y=['Silhouette Score', 'Cluster Purity'],
                   width=0.8)
    plt.title('Clustering Performance Comparison')
    plt.ylabel('Score')
    plt.xticks(rotation=45)
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'clustering_metrics.pdf'),
                dpi=300, bbox_inches='tight')
    plt.show()

    # 3. Dimensionality Analysis
    plt.figure(figsize=(15, 10))

    for i, (model_name, data) in enumerate(results.items()):
        vectors = data['latent_vectors']
        pca = PCA()
        pca.fit(vectors)

        plt.subplot(2, 2, i+1)
        cumsum = np.cumsum(pca.explained_variance_ratio_)
        plt.plot(range(1, len(cumsum) + 1), cumsum)
        plt.xlabel('Number of Components')
        plt.ylabel('Cumulative Explained Variance')
        plt.title(f'{model_name} - PCA Analysis')
        plt.grid(True)

    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'dimensionality_analysis.pdf'),
                dpi=300, bbox_inches='tight')
    plt.show()

    # 4. Generate LaTeX tables
    # Model comparison table
    table_df = metrics_df.round(4)
    latex_table = table_df.to_latex(index=False,
                                  caption='Comparison of Model Performance',
                                  label='tab:model_comparison')

    with open(os.path.join(save_dir, 'model_comparison_table.tex'), 'w') as f:
        f.write(latex_table)

    # Detailed statistics table
    stats_data = []
    for model_name, data in results.items():
        vectors = data['latent_vectors']
        stats_data.append({
            'Model': model_name,
            'Mean Dim': np.mean(vectors),
            'Std Dim': np.std(vectors),
            'Max Dim': np.max(vectors),
            'Min Dim': np.min(vectors)
        })

    stats_df = pd.DataFrame(stats_data)
    stats_latex = stats_df.round(4).to_latex(index=False,
                                           caption='Latent Space Statistics',
                                           label='tab:latent_stats')

    with open(os.path.join(save_dir, 'latent_statistics_table.tex'), 'w') as f:
        f.write(stats_latex)

    return metrics_df, stats_df

def generate_clinical_analysis(results, clinical_info):
    """
    Generate clinical correlation analysis figures and tables.

    Args:
        results: Dictionary containing analysis results for each model
        clinical_info: DataFrame containing clinical information
    """
    # Clinical correlation analysis
    plt.figure(figsize=(15, 10))

    for i, (model_name, data) in enumerate(results.items()):
        vectors = data['latent_vectors']

        # Compute correlations with clinical variables
        correlations = []
        for col in clinical_info.columns:
            if col != 'label':  # Skip the label column
                corr = np.corrcoef(vectors.mean(axis=1),
                                 clinical_info[col])[0, 1]
                correlations.append((col, corr))

        # Plot correlations
        plt.subplot(2, 2, i+1)
        corr_df = pd.DataFrame(correlations, columns=['Variable', 'Correlation'])
        sns.barplot(data=corr_df, x='Variable', y='Correlation')
        plt.title(f'{model_name} - Clinical Correlations')
        plt.xticks(rotation=45)

    plt.tight_layout()
    plt.savefig('thesis_figures/clinical_correlations.pdf',
                dpi=300, bbox_inches='tight')
    plt.show()

# Run analysis and generate figures
if __name__ == "__main__":
    try:
        print("Generating thesis figures and tables...")
        metrics_df, stats_df = generate_thesis_figures(results)

        # Print summary statistics
        print("\nModel Performance Summary:")
        print(metrics_df)
        print("\nLatent Space Statistics:")
        print(stats_df)

    except Exception as e:
        print(f"Error generating thesis figures: {str(e)}")
        import traceback
        traceback.print_exc()

# Cell 52: Generate Thesis Discussion Template

def generate_thesis_discussion(results):
    """
    Generate a structured discussion template based on the analysis results.
    """
    discussion = """
# Latent Space Analysis of DaTSCAN Images: A Comparative Study

## 1. Introduction
- Brief overview of the four models (AE, VAE, SSAE, SSVAE)
- Motivation for latent space analysis
- Importance in the context of Parkinson's Disease diagnosis

## 2. Methodology
### 2.1 Model Architecture Comparison
- Discussion of architectural differences
- Latent space dimensionality choices
- Training strategies and optimization

### 2.2 Analysis Framework
- Latent vector extraction process
- Dimensionality reduction techniques (t-SNE and PCA)
- Clustering analysis methodology
- Statistical measures and metrics

## 3. Results

### 3.1 Latent Space Visualization
- Interpretation of t-SNE plots for each model
- Discussion of cluster separation
- Qualitative assessment of group boundaries
"""

    # Add model-specific performance metrics
    performance_section = "\n### 3.2 Quantitative Analysis\n"
    for model_name, data in results.items():
        metrics = data['clustering_metrics']
        performance_section += f"""
#### {model_name}
- Silhouette Score: {metrics['silhouette_score']:.4f}
- Cluster Purity: {metrics['purity']:.4f}
- Key observations:
  * [Add specific observations about cluster quality]
  * [Note any particular strengths or weaknesses]
"""

    discussion += performance_section + """
### 3.3 Clinical Relevance
- Correlation with clinical outcomes
- Potential biomarker identification
- Diagnostic implications

## 4. Comparative Analysis
### 4.1 Model Strengths and Limitations
- Autoencoder vs VAE comparison
- Impact of semi-supervised learning
- Trade-offs between models

### 4.2 Clinical Applications
- Diagnostic potential
- Patient stratification
- Disease progression monitoring

## 5. Future Directions
- Suggestions for model improvements
- Additional clinical validations
- Integration into clinical workflow

## 6. Conclusion
- Summary of key findings
- Model recommendations
- Clinical impact assessment
"""

    # Write discussion template to file
    with open('thesis_figures/discussion_template.md', 'w') as f:
        f.write(discussion)

    return discussion

# Generate discussion template
if __name__ == "__main__":
    try:
        print("Generating thesis discussion template...")
        discussion = generate_thesis_discussion(results)
        print("\nDiscussion template has been saved to 'thesis_figures/discussion_template.md'")

    except Exception as e:
        print(f"Error generating discussion template: {str(e)}")
        import traceback
        traceback.print_exc()