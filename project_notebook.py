# -*- coding: utf-8 -*-
"""Project_Notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zkbb1EDlzyb3Th4K0IOr--4MKPeUN-z5

# Setup
"""

# Cell 1: Install dependencies
# Uncomment and run the following command if dependencies are not already installed.
# %pip install scikit-learn scikit-image SimpleITK nibabel nilearn albumentations seaborn pandas numpy matplotlib tqdm pydicom scipy
# %pip install umap-learn

# Cell 2: Import statements and environment setup
import torch

import torch.multiprocessing as mp
mp.set_start_method('spawn', force=True)

def configure_gpu():
    """
    Configures GPU settings:
    - Detects CUDA device
    - Enables CUDNN benchmarking for improved performance on NVIDIA 4070Ti
    """
    if torch.cuda.is_available():
        device = torch.device("cuda")
        # Enable CUDNN benchmark for optimized convolution algorithm selection
        torch.backends.cudnn.benchmark = True
        print(f"Using GPU: {torch.cuda.get_device_name(device)}")
    else:
        raise EnvironmentError("CUDA-compatible GPU not found. Please check your GPU configuration.")

def print_gpu_memory_stats():
    """
    Prints current GPU memory usage for monitoring.
    """
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / (1024 ** 2)
        reserved = torch.cuda.memory_reserved() / (1024 ** 2)
        print(f"GPU Memory Allocated: {allocated:.2f} MB")
        print(f"GPU Memory Reserved: {reserved:.2f} MB")
    else:
        print("CUDA not available.")

# Configure GPU on startup
configure_gpu()
print_gpu_memory_stats()

import warnings
warnings.filterwarnings('ignore')

# Cell 3: GPU Setup and Memory Management
import os
import logging
import warnings
import pandas as pd

# Configure logging for quality assurance (logs will be written to data_ingestion.log)
logging.basicConfig(level=logging.INFO, filename="data_ingestion.log", filemode="w",
                    format="%(asctime)s - %(levelname)s - %(message)s")

def collect_files(base_dir):
    """
    Recursively collects DICOM files only from the expected folders:
    - PPMI_Images_PD: Label "PD"
    - PPMI_Images_SWEDD: Label "SWEDD"
    - PPMI_Images_Cont: Label "Control"

    Excludes any file containing "br_raw" in its path and logs all skipped folders.

    :param base_dir: Base directory containing the Images folder.
    :return: (included_files, excluded_files)
             included_files: list of tuples (full_path, label)
             excluded_files: list of file paths that were excluded.
    """
    included_files = []
    excluded_files = []

    # Define the expected folders and corresponding labels
    expected_folders = {
        "PPMI_Images_PD": "PD",
        "PPMI_Images_SWEDD": "SWEDD",
        "PPMI_Images_Cont": "Control"
    }

    # Iterate over immediate subdirectories in base_dir
    for folder in os.listdir(base_dir):
        folder_path = os.path.join(base_dir, folder)
        if os.path.isdir(folder_path) and folder in expected_folders:
            logging.info(f"Processing folder: {folder_path}")
            # Recursively traverse the expected folder
            for root, dirs, files in os.walk(folder_path):
                for file in files:
                    if file.endswith(".dcm"):
                        full_path = os.path.join(root, file)
                        # Exclude any file with "br_raw" in its full path
                        if "br_raw" in full_path:
                            excluded_files.append(full_path)
                            logging.info(f"Excluding raw file: {full_path}")
                        else:
                            included_files.append((full_path, expected_folders[folder]))
        else:
            logging.info(f"Skipping folder: {folder_path}")

    return included_files, excluded_files

def generate_dataframe(included_files):
    """
    Creates a DataFrame from the list of validated file paths.

    :param included_files: List of tuples (file_path, label)
    :return: DataFrame with columns 'file_path' and 'label'
    """
    df = pd.DataFrame(included_files, columns=["file_path", "label"])
    return df

def save_qa_report(total_files, included_count, excluded_count, output_path="data_ingestion_QA_report.csv"):
    """
    Generates and saves a QA report of the file collection process.

    :param total_files: Total number of DICOM files encountered.
    :param included_count: Count of files included after filtering.
    :param excluded_count: Count of files excluded.
    :param output_path: File path for the QA report CSV.
    """
    exclusion_ratio = excluded_count / total_files if total_files > 0 else 0
    qa_report = {
        "total_files": total_files,
        "included_files": included_count,
        "excluded_files": excluded_count,
        "exclusion_ratio": exclusion_ratio,
    }
    qa_df = pd.DataFrame([qa_report])
    qa_df.to_csv(output_path, index=False)
    logging.info("QA report saved to %s", output_path)

    if exclusion_ratio > 0.5:
        warnings.warn(f"High proportion of raw files excluded: {exclusion_ratio:.2%}")

"""## Data Ingestion"""

#!pip install pydicom

# Cell 4: Data Ingestion Pipeline with GPU Processing
import pydicom
import numpy as np
import torch
import logging
from pathlib import Path

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def load_dicom(file_path, device):
    """
    Loads and processes a DICOM file with GPU acceleration.
    """
    try:
        logger.info(f"Loading DICOM file: {file_path}")
        ds = pydicom.dcmread(file_path)

        if not hasattr(ds, 'pixel_array'):
            logger.warning(f"No pixel array in {file_path}")
            return None, None

        pixel_array = ds.pixel_array
        logger.info(f"Initial pixel array shape: {pixel_array.shape}")

        # Convert to float32
        pixel_array = pixel_array.astype(np.float32)

        # Apply rescaling if attributes are present
        if hasattr(ds, 'RescaleSlope') and hasattr(ds, 'RescaleIntercept'):
            slope = float(ds.RescaleSlope)
            intercept = float(ds.RescaleIntercept)
            pixel_array = pixel_array * slope + intercept
            logger.info("Applied rescale slope and intercept")

        # Convert to PyTorch tensor and move to GPU
        tensor = torch.from_numpy(pixel_array).to(device)
        logger.info(f"Final tensor shape: {tensor.shape}")

        return tensor, ds

    except Exception as e:
        logger.error(f"Error reading DICOM file {file_path}: {str(e)}")
        return None, None

def process_volume(volume, target_shape=(64, 128, 128), device=None):
    """
    Process a 3D volume tensor using GPU operations:
    1. Extract specific axial slices [9:73]
    2. Adjust in-plane dimensions to 128x128
    3. Normalize intensities
    4. Apply Otsu thresholding and morphological operations
    """
    try:
        if device is None:
            device = volume.device
        logger.info(f"Initial volume shape: {volume.shape}")

        # Validate input shape
        if volume.ndim != 3:
            logger.error(f"Expected 3D volume, got shape {volume.shape}")
            return None

        # Check if we have enough slices
        if volume.shape[0] < 73:
            logger.error(f"Volume has insufficient slices: {volume.shape[0]} (need at least 73)")
            return None

        # 1. Extract axial slices [9:73]
        volume = volume[9:73, :, :]
        logger.info(f"After slicing shape: {volume.shape}")

        # 2. Adjust in-plane dimensions to 128x128
        current_h, current_w = volume.shape[1:]

        # Calculate padding or cropping
        pad_h = max(0, (target_shape[1] - current_h))
        pad_w = max(0, (target_shape[2] - current_w))
        crop_h = max(0, (current_h - target_shape[1]))
        crop_w = max(0, (current_w - target_shape[2]))

        # Apply padding if needed
        if pad_h > 0 or pad_w > 0:
            pad_h_half = pad_h // 2
            pad_w_half = pad_w // 2
            padding = (pad_w_half, pad_w - pad_w_half,
                    pad_h_half, pad_h - pad_h_half,
                    0, 0)  # (left, right, top, bottom, front, back)
            volume = torch.nn.functional.pad(volume, padding)
            logger.info(f"After padding shape: {volume.shape}")

        # Apply cropping if needed
        if crop_h > 0 or crop_w > 0:
            start_h = crop_h // 2
            start_w = crop_w // 2
            volume = volume[:, start_h:start_h + target_shape[1],
                        start_w:start_w + target_shape[2]]
            logger.info(f"After cropping shape: {volume.shape}")

        # 3. Normalize intensities
        volume = volume - volume.min()
        if volume.max() > 0:
            volume = volume / volume.max()

        # Create and apply mask
        mask = torch.zeros((64,128,128), dtype=torch.bool, device=device)
        mask[20:40,82:103,43:82] = True
        masked_vol = volume * mask.float()

        # Normalize based on mask mean
        mask_mean = volume[mask].mean()
        if mask_mean > 0:
            volume = volume / mask_mean

        logger.info(f"Final volume shape: {volume.shape}")
        return volume

    except Exception as e:
        logger.error(f"Error processing volume: {str(e)}")
        return None

# # Cell 5: Execute Data Ingestion Pipeline
# # Define the base directory containing the "Images" folder (adjust if necessary)
# base_dir = "Images"

# # Collect files from only the expected subdirectories
# included_files, excluded_files = collect_files(base_dir)

# # Create a DataFrame for the validated file paths and their labels
# df = generate_dataframe(included_files)

# # Final validation: Ensure that no "br_raw" files are included
# if df["file_path"].str.contains("br_raw").any():
#     raise ValueError("Validation failed: 'br_raw' files detected in the final dataset!")

# # Save the validated file paths to CSV for reproducibility
# df.to_csv("validated_file_paths.csv", index=False)
# print("Validated file paths saved to validated_file_paths.csv")

# # Generate and save the QA report
# total_files = len(included_files) + len(excluded_files)
# save_qa_report(total_files, len(included_files), len(excluded_files))
# print("QA report generated and saved as data_ingestion_QA_report.csv")

"""## Data Visualization"""

# Cell 6: Data Visualization with GPU Support
import pandas as pd
import random
import matplotlib.pyplot as plt
import numpy as np
import torch

# Read the validated file paths CSV generated earlier
df = pd.read_csv("validated_file_paths.csv")

# Function to extract the three orthogonal slices from a 3D volume
def extract_slices(volume):
    """
    Given a 3D volume, returns one axial, one coronal, and one sagittal slice.
    Assumes volume shape is (depth, height, width).

    Args:
        volume: 3D numpy array or tensor
    Returns:
        tuple: (axial, coronal, sagittal) slices
    """
    if isinstance(volume, torch.Tensor):
        volume = volume.cpu().numpy()

    d, h, w = volume.shape
    axial = volume[d // 2, :, :]         # Axial: slice along depth
    coronal = volume[:, h // 2, :]        # Coronal: slice along height
    sagittal = volume[:, :, w // 2]       # Sagittal: slice along width
    return axial, coronal, sagittal

# List of groups and their expected labels
groups = {"PD": "PD", "SWEDD": "SWEDD", "Control": "Control"}

# Set up device for GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Create a figure with one row per group and three columns for the views
fig, axes = plt.subplots(nrows=len(groups), ncols=3, figsize=(12, 4 * len(groups)))
fig.suptitle("Axial, Coronal, and Sagittal Slices for a Random Patient per Group", fontsize=16)

for i, (group_key, group_label) in enumerate(groups.items()):
    # Filter DataFrame for the current group
    group_df = df[df["label"] == group_label]
    if group_df.empty:
        print(f"No data found for group {group_label}")
        continue

    # Select a random file from the group
    random_file = group_df.sample(1)["file_path"].values[0]
    print(f"Loading file for group {group_label}: {random_file}")

    # Load the DICOM volume using the updated load_dicom() function
    volume_tensor, _ = load_dicom(random_file, device)

    if volume_tensor is None:
        print(f"Failed to load volume for group {group_label}")
        continue

    # Convert tensor to numpy for visualization
    volume = volume_tensor.cpu().numpy()

    # Verify the volume is 3D
    if volume.ndim != 3:
        raise ValueError(f"Expected 3D volume, got shape {volume.shape} for file: {random_file}")

    axial, coronal, sagittal = extract_slices(volume)

    # Plot Axial slice
    ax = axes[i, 0]
    ax.imshow(axial, cmap="gray")
    ax.set_title(f"{group_label} - Axial")
    ax.axis("off")

    # Plot Coronal slice
    ax = axes[i, 1]
    ax.imshow(coronal, cmap="gray")
    ax.set_title(f"{group_label} - Coronal")
    ax.axis("off")

    # Plot Sagittal slice
    ax = axes[i, 2]
    ax.imshow(sagittal, cmap="gray")
    ax.set_title(f"{group_label} - Sagittal")
    ax.axis("off")

plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()

# Clean up GPU memory
if torch.cuda.is_available():
    torch.cuda.empty_cache()

"""## Data Preprocessing

### Intensity Normalization and Volume Preprocessing

### Brain Masking
"""

#!pip install scikit-image

# Cell 7: Data Preprocessing and Brain Masking with GPU Support
import numpy as np
import matplotlib.pyplot as plt
from skimage.filters import threshold_otsu
from skimage.morphology import binary_closing, ball
import torch

def resize_volume(volume, target_shape=(64, 128, 128)):
    """
    Resizes the volume to the target shape using zero-padding or center cropping.

    Args:
        volume: Input 3D volume with shape (d, h, w)
        target_shape: Desired output shape as tuple (d_new, h_new, w_new)

    Returns:
        Resized volume with shape target_shape
    """
    def get_pad_amounts(current_size, target_size):
        """Helper to calculate padding amounts"""
        if current_size >= target_size:
            return 0, 0
        diff = target_size - current_size
        pad_before = diff // 2
        pad_after = diff - pad_before
        return pad_before, pad_after

    # Convert to numpy if needed
    is_tensor = isinstance(volume, torch.Tensor)
    if is_tensor:
        device = volume.device
        volume = volume.cpu().numpy()

    current_shape = volume.shape
    resized = volume.copy()

    # Calculate padding/cropping for each dimension
    pads = [get_pad_amounts(current_shape[i], target_shape[i]) for i in range(3)]

    # Apply padding if needed
    if any(sum(p) > 0 for p in pads):
        resized = np.pad(
            resized,
            pad_width=pads,
            mode="constant",
            constant_values=0
        )

    # Apply cropping if needed
    for i in range(3):
        if current_shape[i] > target_shape[i]:
            start = (current_shape[i] - target_shape[i]) // 2
            end = start + target_shape[i]
            if i == 0:
                resized = resized[start:end, :, :]
            elif i == 1:
                resized = resized[:, start:end, :]
            else:
                resized = resized[:, :, start:end]

    # Convert back to tensor if input was tensor
    if is_tensor:
        resized = torch.from_numpy(resized).to(device)

    return resized

def process_volume(volume, target_shape=(64, 128, 128), device=None):
    """
    Process a 3D volume:
    1. Normalizing intensity (truncating negatives and min-max scaling)
    2. Resizing to target_shape
    3. Generating a brain mask via Otsu thresholding

    Args:
        volume: Input 3D volume (numpy array or torch tensor)
        target_shape: Desired output shape (depth, height, width)
        device: PyTorch device (if None, uses same device as input tensor)

    Returns:
        tuple: (norm_vol, mask, masked_vol)
    """
    # Handle input type and device
    is_tensor = isinstance(volume, torch.Tensor)
    if is_tensor:
        device = device or volume.device
        volume = volume.cpu().numpy()

    # 1. Intensity normalization
    volume = volume - volume.min()

    # 2. Resize the normalized volume
    norm_vol = resize_volume(volume, target_shape=target_shape)

    # Create anatomical mask for normalization
    mask = np.zeros(target_shape, dtype=bool)
    mask[20:40, 82:103, 43:82] = True

    # 3. Normalize based on anatomical region
    norm_vol = norm_vol / np.mean(norm_vol[mask])

    # 4. Compute and apply brain mask
    thresh = threshold_otsu(norm_vol)
    final_mask = norm_vol > thresh
    final_mask = binary_closing(final_mask, footprint=ball(2))
    masked_vol = norm_vol * final_mask

    # Convert outputs to tensors if needed
    if is_tensor or device is not None:
        device = device or 'cpu'
        norm_vol = torch.from_numpy(norm_vol).to(device)
        final_mask = torch.from_numpy(final_mask).to(device)
        masked_vol = torch.from_numpy(masked_vol).to(device)

    return norm_vol, final_mask, masked_vol

# Demonstration
if __name__ == "__main__":
    # Set up device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # Load and process a sample file
    sample_file = df.iloc[0]["file_path"]
    original_volume, _ = load_dicom(sample_file, device)
    original_volume = original_volume[9:73,:,:]

    # Process the volume
    norm_vol, mask, masked_vol = process_volume(
        original_volume,
        target_shape=(64,128,128),
        device=device
    )

    print(f"Original shape: {original_volume.shape}")
    print(f"Processed shape: {norm_vol.shape}")

    # Extract axial slices for visualization
    axial_norm = norm_vol[norm_vol.shape[0]//2].cpu().numpy()
    axial_masked = masked_vol[masked_vol.shape[0]//2].cpu().numpy()

    # Plot comparison
    fig, axes = plt.subplots(1, 2, figsize=(12, 6))
    axes[0].imshow(axial_norm, cmap="gray")
    axes[0].set_title("Normalized Axial Slice")
    axes[0].axis("off")

    axes[1].imshow(axial_masked, cmap="gray")
    axes[1].set_title("Masked Axial Slice")
    axes[1].axis("off")

    plt.tight_layout()
    plt.show()

    # Additional visualization
    plt.figure()
    plt.imshow(norm_vol[32].cpu().numpy())
    plt.colorbar()
    plt.show()

