# -*- coding: utf-8 -*-
"""Project_Notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zkbb1EDlzyb3Th4K0IOr--4MKPeUN-z5

# Setup
"""

# Cell 1: Install dependencies
# Uncomment and run the following command if dependencies are not already installed.
# %pip install scikit-learn scikit-image SimpleITK nibabel nilearn albumentations seaborn pandas numpy matplotlib tqdm pydicom scipy
# %pip install umap-learn

# CUDA verification
import torch
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"CUDA device count: {torch.cuda.device_count()}")
if torch.cuda.is_available():
    print(f"CUDA device name: {torch.cuda.get_device_name(0)}")
    print(f"CUDA device capability: {torch.cuda.get_device_capability(0)}")

# Cell 2: Import statements and environment setup
import torch

def configure_gpu():
    """
    Configures GPU settings:
    - Detects CUDA device
    - Enables CUDNN benchmarking for improved performance on NVIDIA 4070Ti
    """
    if torch.cuda.is_available():
        device = torch.device("cuda")
        # Enable CUDNN benchmark for optimized convolution algorithm selection
        torch.backends.cudnn.benchmark = True
        print(f"Using GPU: {torch.cuda.get_device_name(device)}")
    else:
        raise EnvironmentError("CUDA-compatible GPU not found. Please check your GPU configuration.")

def print_gpu_memory_stats():
    """
    Prints current GPU memory usage for monitoring.
    """
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / (1024 ** 2)
        reserved = torch.cuda.memory_reserved() / (1024 ** 2)
        print(f"GPU Memory Allocated: {allocated:.2f} MB")
        print(f"GPU Memory Reserved: {reserved:.2f} MB")
    else:
        print("CUDA not available.")

# Configure GPU on startup
configure_gpu()
print_gpu_memory_stats()

import warnings
warnings.filterwarnings('ignore')

# Cell 3: GPU Setup and Memory Management
import os
import logging
import warnings
import pandas as pd

# Configure logging for quality assurance (logs will be written to data_ingestion.log)
logging.basicConfig(level=logging.INFO, filename="data_ingestion.log", filemode="w",
                    format="%(asctime)s - %(levelname)s - %(message)s")

def collect_files(base_dir):
    """
    Recursively collects DICOM files only from the expected folders:
    - PPMI_Images_PD: Label "PD"
    - PPMI_Images_SWEDD: Label "SWEDD"
    - PPMI_Images_Cont: Label "Control"

    Excludes any file containing "br_raw" in its path and logs all skipped folders.

    :param base_dir: Base directory containing the Images folder.
    :return: (included_files, excluded_files)
             included_files: list of tuples (full_path, label)
             excluded_files: list of file paths that were excluded.
    """
    included_files = []
    excluded_files = []

    # Define the expected folders and corresponding labels
    expected_folders = {
        "PPMI_Images_PD": "PD",
        "PPMI_Images_SWEDD": "SWEDD",
        "PPMI_Images_Cont": "Control"
    }

    # Iterate over immediate subdirectories in base_dir
    for folder in os.listdir(base_dir):
        folder_path = os.path.join(base_dir, folder)
        if os.path.isdir(folder_path) and folder in expected_folders:
            logging.info(f"Processing folder: {folder_path}")
            # Recursively traverse the expected folder
            for root, dirs, files in os.walk(folder_path):
                for file in files:
                    if file.endswith(".dcm"):
                        full_path = os.path.join(root, file)
                        # Exclude any file with "br_raw" in its full path
                        if "br_raw" in full_path:
                            excluded_files.append(full_path)
                            logging.info(f"Excluding raw file: {full_path}")
                        else:
                            included_files.append((full_path, expected_folders[folder]))
        else:
            logging.info(f"Skipping folder: {folder_path}")

    return included_files, excluded_files

def generate_dataframe(included_files):
    """
    Creates a DataFrame from the list of validated file paths.

    :param included_files: List of tuples (file_path, label)
    :return: DataFrame with columns 'file_path' and 'label'
    """
    df = pd.DataFrame(included_files, columns=["file_path", "label"])
    return df

def save_qa_report(total_files, included_count, excluded_count, output_path="data_ingestion_QA_report.csv"):
    """
    Generates and saves a QA report of the file collection process.

    :param total_files: Total number of DICOM files encountered.
    :param included_count: Count of files included after filtering.
    :param excluded_count: Count of files excluded.
    :param output_path: File path for the QA report CSV.
    """
    exclusion_ratio = excluded_count / total_files if total_files > 0 else 0
    qa_report = {
        "total_files": total_files,
        "included_files": included_count,
        "excluded_files": excluded_count,
        "exclusion_ratio": exclusion_ratio,
    }
    qa_df = pd.DataFrame([qa_report])
    qa_df.to_csv(output_path, index=False)
    logging.info("QA report saved to %s", output_path)

    if exclusion_ratio > 0.5:
        warnings.warn(f"High proportion of raw files excluded: {exclusion_ratio:.2%}")

"""## Data Ingestion"""

# !pip install pydicom
# !pip install nibabel

# Cell 4: Data Ingestion Pipeline
import pydicom
import numpy as np

def load_dicom(file_path):
    """
    Loads and processes a DICOM file:
    - Reads the file using pydicom.
    - Converts the pixel array to float32.
    - Applies RescaleSlope and RescaleIntercept if available.

    :param file_path: Path to the DICOM file.
    :return: Tuple (processed_pixel_array, dicom_metadata)
    """
    try:
        ds = pydicom.dcmread(file_path)
    except Exception as e:
        raise IOError(f"Error reading DICOM file {file_path}: {e}")

    # Extract pixel array and convert to float32
    pixel_array = ds.pixel_array.astype(np.float32)

    # Apply rescaling if attributes are present
    if hasattr(ds, 'RescaleSlope') and hasattr(ds, 'RescaleIntercept'):
        slope = ds.RescaleSlope
        intercept = ds.RescaleIntercept
        pixel_array = pixel_array * slope + intercept

    return pixel_array, ds

# Cell 5: Execute Data Ingestion Pipeline
# Define the base directory containing the "Images" folder (adjust if necessary)
base_dir = "Images"

# Collect files from only the expected subdirectories
included_files, excluded_files = collect_files(base_dir)

# Create a DataFrame for the validated file paths and their labels
df = generate_dataframe(included_files)

# Final validation: Ensure that no "br_raw" files are included
if df["file_path"].str.contains("br_raw").any():
    raise ValueError("Validation failed: 'br_raw' files detected in the final dataset!")

# Save the validated file paths to CSV for reproducibility
df.to_csv("validated_file_paths.csv", index=False)
print("Validated file paths saved to validated_file_paths.csv")

# Generate and save the QA report
total_files = len(included_files) + len(excluded_files)
save_qa_report(total_files, len(included_files), len(excluded_files))
print("QA report generated and saved as data_ingestion_QA_report.csv")

"""## Data Visualization"""

# Cell 6: Visualize One Axial, Coronal, and Sagittal Slice for a Random Patient per Group

import pandas as pd
import random
import matplotlib.pyplot as plt
import numpy as np
import nibabel as nib

# Read the validated file paths CSV generated earlier
df = pd.read_csv("validated_file_paths.csv")

# Function to extract the three orthogonal slices from a 3D volume
def extract_slices(volume):
    """
    Given a 3D volume, returns one axial, one coronal, and one sagittal slice.
    Assumes volume shape is (depth, height, width).
    """
    d, h, w = volume.shape
    axial = volume[d // 2, :, :]         # Axial: slice along depth
    coronal = volume[:, h // 2, :]        # Coronal: slice along height
    sagittal = volume[:, :, w // 2]       # Sagittal: slice along width
    return axial, coronal, sagittal

# List of groups and their expected labels
groups = {"PD": "PD", "SWEDD": "SWEDD", "Control": "Control"}
maskH = nib.load('rmask_ICV.nii')
mask = maskH.get_fdata()>0.5
mask = np.transpose(mask,[2, 1, 0])
mask = np.flip(mask,axis=1)
# Create a figure with one row per group and three columns for the views
fig, axes = plt.subplots(nrows=len(groups), ncols=3, figsize=(12, 4 * len(groups)))
fig.suptitle("Axial, Coronal, and Sagittal Slices for a Random Patient per Group", fontsize=16)

for i, (group_key, group_label) in enumerate(groups.items()):
    # Filter DataFrame for the current group
    group_df = df[df["label"] == group_label]
    if group_df.empty:
        print(f"No data found for group {group_label}")
        continue

    # Select a random file from the group
    random_file = group_df.sample(1)["file_path"].values[0]
    print(f"Loading file for group {group_label}: {random_file}")

    # Load the DICOM volume using the previously defined load_dicom() function
    volume, _ = load_dicom(random_file)

    # Verify the volume is 3D (if not, skip or raise an error)
    if volume.ndim != 3:
        raise ValueError(f"Expected 3D volume, got shape {volume.shape} for file: {random_file}")

    axial, coronal, sagittal = extract_slices(volume)

    # Plot Axial slice
    ax = axes[i, 0]
    ax.imshow(axial, cmap="gray")
    ax.set_title(f"{group_label} - Axial")
    ax.axis("off")

    # Plot Coronal slice
    ax = axes[i, 1]
    ax.imshow(coronal, cmap="gray")
    ax.set_title(f"{group_label} - Coronal")
    ax.axis("off")

    # Plot Sagittal slice
    ax = axes[i, 2]
    ax.imshow(sagittal, cmap="gray")
    ax.set_title(f"{group_label} - Sagittal")
    ax.axis("off")

plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()

"""## Data Preprocessing

### Intensity Normalization and Volume Preprocessing

### Brain Masking
"""

#!pip install scikit-image

# Cell 7: Data Preprocessing – Brain Masking

import numpy as np
import matplotlib.pyplot as plt
from skimage.filters import threshold_otsu
from skimage.morphology import binary_closing, ball

def resize_volume(volume, target_shape=(64, 128, 128)):
    """
    Resizes the volume to the target shape using zero-padding or center cropping.

    Args:
        volume: Input 3D volume as numpy array with shape (d, h, w)
        target_shape: Desired output shape as tuple (d_new, h_new, w_new)

    Returns:
        Resized volume with shape target_shape
    """
    def get_pad_amounts(current_size, target_size):
        """Helper to calculate padding amounts"""
        if current_size >= target_size:
            return 0, 0
        diff = target_size - current_size
        pad_before = diff // 2
        pad_after = diff - pad_before
        return pad_before, pad_after

    current_shape = volume.shape
    resized = volume.copy()

    # Calculate padding/cropping for each dimension
    pads = [get_pad_amounts(current_shape[i], target_shape[i]) for i in range(3)]

    # Apply padding if needed
    if any(sum(p) > 0 for p in pads):
        resized = np.pad(
            resized,
            pad_width=pads,
            mode="constant",
            constant_values=0
        )

    # Apply cropping if needed
    for i in range(3):
        if current_shape[i] > target_shape[i]:
            # Calculate slicing indices
            start = (current_shape[i] - target_shape[i]) // 2
            end = start + target_shape[i]
            # Apply slice
            if i == 0:
                resized = resized[start:end, :, :]
            elif i == 1:
                resized = resized[:, start:end, :]
            else:
                resized = resized[:, :, start:end]

    return resized

def process_volume(volume, target_shape=(64, 128, 128)):
    """
    Process a 3D volume by:
    1. Normalizing intensity (truncating negatives and min-max scaling)
    2. Resizing to target_shape
    3. Generating a brain mask via Otsu thresholding and morphological closing

    Args:
        volume: Input 3D volume
        target_shape: Desired output shape (depth, height, width)

    Returns:
        norm_vol: Normalized and resized volume
        mask: Brain mask
        masked_vol: Masked volume
    """
    # 1. Intensity normalization
    # volume = np.clip(volume, a_min=0, a_max=None)
    # vmin, vmax = volume.min(), volume.max()
    # if vmax > vmin:
    #     norm_vol = (volume - vmin) / (vmax - vmin)
    # else:
    #     norm_vol = volume - vmin


    # 2. Resize the normalized volume
    norm_vol = resize_volume(volume-volume.min(), target_shape=target_shape)
    mask = np.zeros((64,128,128),dtype=bool)
    mask[20:40,82:103,43:82]=1
    norm_vol /= np.mean(norm_vol[mask])

    # 3. Compute brain mask
    thresh = threshold_otsu(norm_vol)
    mask = norm_vol > thresh
    mask = binary_closing(mask, footprint=ball(2))
    masked_vol = norm_vol * mask

    return norm_vol, mask, masked_vol

# Demonstration: Load one sample DICOM file (using the first file in your validated DataFrame)
sample_file = df.iloc[0]["file_path"]
original_volume, _ = load_dicom(sample_file)
original_volume = original_volume[9:73,:,:]

# Process the volume with our new function
norm_vol, mask, masked_vol = process_volume(original_volume, target_shape=(64,128,128))

print(original_volume.shape)
print(norm_vol.shape)

# Extract an axial (middle) slice from both the normalized volume and the masked volume
axial_norm = norm_vol[norm_vol.shape[0]//2, :, :]
axial_masked = masked_vol[masked_vol.shape[0]//2, :, :]

# Plot side-by-side for comparison
fig, axes = plt.subplots(1, 2, figsize=(12, 6))
axes[0].imshow(axial_norm, cmap="gray")
axes[0].set_title("Normalized Axial Slice")
axes[0].axis("off")

axes[1].imshow(axial_masked, cmap="gray")
axes[1].set_title("Masked Axial Slice")
axes[1].axis("off")

plt.tight_layout()
plt.show()

# Cell 8: Data Preprocessing – Visualization (heatmap)
plt.imshow(norm_vol[32,:,:])
plt.colorbar()

"""## Dataloader Creation (with Shape Validation)"""

# !pip install ipywidgets

# Cell 9: Dataset Implementation with GPU Memory Management
import torch
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import gc
import numpy as np
import os
import psutil
import time
from sklearn.model_selection import train_test_split

def print_memory_stats():
    """Print memory usage statistics"""
    if torch.cuda.is_available():
        print("\nGPU Memory Usage:")
        print(f"Allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB")
        print(f"Cached: {torch.cuda.memory_reserved() / 1024**2:.2f} MB")
    print(f"CPU Memory Usage: {psutil.Process().memory_info().rss / 1024**2:.2f} MB")

class OnDemandDataset(Dataset):
    """
    Memory-efficient dataset that loads volumes on demand rather than all at once
    """
    def __init__(self, dataframe, transform=None):
        self.df = dataframe
        self.transform = transform

        # Load mask once
        try:
            maskH = nib.load('rmask_ICV.nii')
            self.mask = maskH.get_fdata() > 0.5
            self.mask = np.transpose(self.mask, [2, 1, 0])
            self.mask = np.flip(self.mask, axis=1)
            print("✅ Brain mask loaded successfully")
        except Exception as e:
            print(f"⚠️ Warning: Could not load brain mask: {e}")
            print("⚠️ Creating a dummy mask instead")
            self.mask = np.ones((128, 128, 128), dtype=bool)

        # Verify all paths exist
        missing_files = []
        for idx, row in dataframe.iterrows():
            if not os.path.exists(row["file_path"]):
                missing_files.append(row["file_path"])

        if missing_files:
            print(f"⚠️ Warning: {len(missing_files)} files not found!")
            print(f"First few missing files: {missing_files[:3]}")
        else:
            print(f"✅ All {len(dataframe)} file paths are valid")

    def __len__(self):
        """Return the total number of samples in the dataset"""
        return len(self.df)

    def __getitem__(self, idx):
        """Load a single volume on demand"""
        try:
            # Get file path
            file_path = self.df.iloc[idx]["file_path"]

            # Load and process volume
            volume, _ = load_dicom(file_path)

            # Apply mask
            volume -= volume.min()
            volume = volume * self.mask

            # Apply processing
            norm_vol, _, _ = process_volume(volume[9:73, :, :], target_shape=(64, 128, 128))

            # Convert to tensor
            volume_tensor = torch.from_numpy(np.expand_dims(norm_vol, axis=0)).float()

            return {
                "volume": volume_tensor,
                "label": self.df.iloc[idx]["label"],
                "path": self.df.iloc[idx]["file_path"]
            }
        except Exception as e:
            print(f"Error processing file {self.df.iloc[idx]['file_path']}: {e}")
            # Return a zero tensor as fallback
            return {
                "volume": torch.zeros((1, 64, 128, 128), dtype=torch.float32),
                "label": self.df.iloc[idx]["label"],
                "path": self.df.iloc[idx]["file_path"]
            }

class BatchLoadDataset(Dataset):
    """
    Memory-efficient dataset that processes data in small batches
    and caches the processed results
    """
    def __init__(self, dataframe, batch_size=32, transform=None):
        self.df = dataframe
        self.transform = transform
        self.data_cache = {}  # Cache for processed data

        # Load mask once
        try:
            maskH = nib.load('rmask_ICV.nii')
            self.mask = maskH.get_fdata() > 0.5
            self.mask = np.transpose(self.mask, [2, 1, 0])
            self.mask = np.flip(self.mask, axis=1)
            print("✅ Brain mask loaded successfully")
        except Exception as e:
            print(f"⚠️ Warning: Could not load brain mask: {e}")
            print("⚠️ Creating a dummy mask instead")
            self.mask = np.ones((128, 128, 128), dtype=bool)

        # Pre-process data in batches to avoid memory issues
        total_batches = (len(dataframe) + batch_size - 1) // batch_size
        print(f"Pre-processing {len(dataframe)} samples in {total_batches} batches...")

        for batch_idx in tqdm(range(total_batches)):
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, len(dataframe))

            for idx in range(start_idx, end_idx):
                try:
                    file_path = dataframe.iloc[idx]["file_path"]

                    # Load DICOM
                    volume, _ = load_dicom(file_path)
                    volume -= volume.min()
                    volume = volume * self.mask

                    # Process volume
                    norm_vol, _, _ = process_volume(volume[9:73, :, :], target_shape=(64, 128, 128))

                    # Store in cache (using index as key)
                    self.data_cache[idx] = norm_vol

                except Exception as e:
                    print(f"Error processing file {dataframe.iloc[idx]['file_path']}: {e}")
                    self.data_cache[idx] = np.zeros((64, 128, 128), dtype=np.float32)

            # Force garbage collection after each batch
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

            # Print memory stats every few batches
            if batch_idx % 5 == 0:
                print_memory_stats()

        print("✅ Data pre-processing complete")

    def __len__(self):
        """Return the total number of samples in the dataset"""
        return len(self.df)

    def __getitem__(self, idx):
        """Get a cached pre-processed volume"""
        try:
            # Get the pre-processed volume from cache
            norm_vol = self.data_cache[idx]

            # Convert to tensor
            volume_tensor = torch.from_numpy(np.expand_dims(norm_vol, axis=0)).float()

            return {
                "volume": volume_tensor,
                "label": self.df.iloc[idx]["label"],
                "path": self.df.iloc[idx]["file_path"]
            }
        except Exception as e:
            print(f"Error retrieving item {idx}: {e}")
            # Return a zero tensor as fallback
            return {
                "volume": torch.zeros((1, 64, 128, 128), dtype=torch.float32),
                "label": self.df.iloc[idx]["label"],
                "path": self.df.iloc[idx]["file_path"]
            }

def create_dataloaders(df, batch_size=4, train_split=0.8, on_demand=True):
    """Create train and validation dataloaders with stratified split"""
    # Stratified split to maintain group distributions
    train_df, val_df = train_test_split(
        df,
        test_size=1-train_split,
        stratify=df['label'],
        random_state=42
    )

    print("\nTraining set distribution:")
    print(train_df['label'].value_counts())
    print("\nValidation set distribution:")
    print(val_df['label'].value_counts())

    # Create datasets with appropriate strategy
    if on_demand:
        print("Using on-demand data loading strategy (lighter on memory but slower)")
        train_dataset = OnDemandDataset(train_df)
        val_dataset = OnDemandDataset(val_df)
    else:
        print("Using batch pre-processing strategy (faster but more memory intensive)")
        train_dataset = BatchLoadDataset(train_df)
        val_dataset = BatchLoadDataset(val_df)

    # Create dataloaders with optimized settings
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=2,  # Reduced from 6 to prevent hanging
        pin_memory=True,
        persistent_workers=False  # Changed from True to avoid memory issues
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=2,  # Reduced from 6 to prevent hanging
        pin_memory=True,
        persistent_workers=False  # Changed from True to avoid memory issues
    )

    return train_loader, val_loader

# Test the dataloader with a small batch
if __name__ == "__main__":
    print("Testing dataloader with small batch...")
    start_time = time.time()

    # Create dataloaders with a smaller batch size for testing
    train_loader, val_loader = create_dataloaders(df, batch_size=2, on_demand=True)

    # Try to load a single batch
    print("Fetching a single batch...")
    for batch in train_loader:
        print(f"Batch loaded successfully!")
        print(f"Batch shape: {batch['volume'].shape}")
        print(f"Labels: {batch['label']}")
        break

    print(f"Dataloader test completed in {time.time() - start_time:.2f} seconds")
    print_memory_stats()

"""### LP Ram Loader"""

# class LPRamDataset(Dataset):
#     def __init__(self, dataframe, transform=None):
#         self.df = dataframe
#         self.transform = transform
#         maskH = nib.load('rmask_ICV.nii')
#         mask = maskH.get_fdata()>0.5
#         mask = np.transpose(mask,[2, 1, 0])
#         mask = np.flip(mask,axis=1)
#         self.npArr = np.zeros((len(dataframe),1,64,128,128),dtype=np.float32)

#         counter=0
#         for _, row in dataframe.iterrows():
#             try:
#                 file_path = row["file_path"]

#             # Load DICOM
#                 volume, _ = load_dicom(file_path)
#                 volume -= volume.min()
#          #       print(counter)
#          #       print(volume.shape)
#         #        print(mask.shape)
#                 volume *= mask
#                 norm_vol, _, masked_vol = process_volume(volume[9:73,:,:], target_shape=(64, 128, 128))
#                 self.npArr[counter,0,:,:,:]=norm_vol
#                 counter +=1
#            #     delete volume, norm_vol, masked_vol
#             except Exception as e:
#                 print(f"Error processing file {row['file_path']}: {e}")
#         self.npArr = self.npArr[:counter,:,:,:,:]
#  #       self.immArr = torch.from_numpy(self.npArr).float().to("cuda")
#   #      delete volume, masked_vol, mask
#         gc.collect()

#     def __len__(self):
#         """Return the total number of samples in the dataset"""
#         return len(self.df)

#     def __getitem__(self, idx):
#       #      print(self.immArr.shape)
#  #           print(idx)
#             volume_tensor = torch.from_numpy(self.npArr[idx, :, :, :, :])
#             return {
#                 "volume": volume_tensor,
#                 "label": self.df.iloc[idx]["label"],
#                 "path":self.df.iloc[idx]["file_path"]
#             }


# def create_dataloaders(df, batch_size=4, train_split=0.8):
#     """Create train and validation dataloaders with stratified split"""
#     # Stratified split to maintain group distributions
#     train_df, val_df = train_test_split(
#         df,
#         test_size=1-train_split,
#         stratify=df['label'],
#         random_state=42
#     )

#     print("\nTraining set distribution:")
#     print(train_df['label'].value_counts())
#     print("\nValidation set distribution:")
#     print(val_df['label'].value_counts())

#     # Create datasets

#     # Create dataloaders
#     train_loader = DataLoader(
#         train_dataset,
#         batch_size=batch_size,
#         shuffle=True,
#         num_workers=0,  # No multiprocessing for debugging
#         pin_memory=True
#     )

#     val_loader = DataLoader(
#         val_dataset,
#         batch_size=batch_size,
#         shuffle=False,
#         num_workers=0,  # No multiprocessing for debugging
#         pin_memory=True
#     )

#     return train_loader, val_loader

"""# Exploratory Data Analysis (EDA)"""

# !pip install seaborn

# Cell 10: Optimized EDA Implementation with Stratified Sampling
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from tqdm import tqdm
from collections import defaultdict
import torch
import time
import gc
import random

def create_memory_efficient_dataloaders(df, batch_size=2, train_split=0.8):
    """
    Create train and validation dataloaders with optimized memory usage
    """
    # Reuse the create_dataloaders function from Cell 9
    return create_dataloaders(df, batch_size=batch_size, train_split=train_split, on_demand=True)

def analyze_dataset_statistics_efficiently(dataloader, max_samples=100, min_samples_per_group=15):
    """
    Analyzes dataset statistics with improved memory efficiency and ensures
    stratified sampling across all patient groups.

    Args:
        dataloader: The dataloader to sample from
        max_samples: Maximum total samples to process
        min_samples_per_group: Minimum samples to collect per group

    Returns:
        Dictionary of statistical measures with proper group representation
    """
    print("Analyzing dataset statistics (stratified, memory-efficient version)...")
    stats = defaultdict(list)
    samples_by_group = defaultdict(int)

    # First pass: Count occurrences of each group
    group_counts = {}
    print("Scanning dataset to count groups...")
    for batch in tqdm(dataloader, desc="Counting groups"):
        labels = batch['label']
        for label in labels:
            if label not in group_counts:
                group_counts[label] = 0
            group_counts[label] += 1

    print(f"Found groups: {group_counts}")

    # Second pass: Collect samples with stratification
    all_samples = []
    all_labels = []
    all_paths = []

    try:
        print("Collecting stratified samples...")
        for batch in tqdm(dataloader, desc="Collecting samples"):
            volumes = batch['volume']
            labels = batch['label']
            paths = batch['path']

            # Process each volume in the batch
            for vol_idx, (volume, label, path) in enumerate(zip(volumes, labels, paths)):
                # If we have enough samples from this group, skip unless we need more total samples
                if (samples_by_group[label] >= min_samples_per_group and
                    sum(samples_by_group.values()) >= max_samples):
                    continue

                # Add this sample
                all_samples.append(volume)
                all_labels.append(label)
                all_paths.append(path)
                samples_by_group[label] += 1

            # Memory cleanup after each batch
            del volumes, labels, paths
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

            # Check if we've collected enough samples from each group
            if all(samples_by_group[group] >= min_samples_per_group for group in group_counts):
                if sum(samples_by_group.values()) >= max_samples:
                    print(f"Collected sufficient samples from all groups")
                    break

    except Exception as e:
        print(f"Error during sample collection: {str(e)}")
        import traceback
        traceback.print_exc()

    # Process collected samples
    print(f"Processing {len(all_samples)} collected samples...")
    print(f"Samples per group: {dict(samples_by_group)}")

    for volume, label, path in zip(all_samples, all_labels, all_paths):
        # Extract statistics
        vol_data = volume.numpy().flatten()

        # Compute statistics
        stats['mean'].append(float(np.mean(vol_data)))
        stats['std'].append(float(np.std(vol_data)))
        stats['min'].append(float(np.min(vol_data)))
        stats['max'].append(float(np.max(vol_data)))
        stats['label'].append(label)
        stats['path'].append(path)

    # Convert to DataFrame for easier analysis
    stats_df = pd.DataFrame(stats)
    print(f"Successfully analyzed {len(stats_df)} samples")

    # Verify group representation
    group_dist = stats_df['label'].value_counts()
    print("Group distribution in analyzed samples:")
    print(group_dist)

    return stats_df

def plot_intensity_distributions(stats_df):
    """
    Creates violin plots of intensity distributions by group
    """
    plt.figure(figsize=(15, 6))

    # Plot intensity distributions
    plt.subplot(1, 2, 1)
    sns.violinplot(data=stats_df, x='label', y='mean', palette='viridis')
    plt.title('Distribution of Mean Intensities by Group')
    plt.xlabel('Group')
    plt.ylabel('Mean Intensity')
    plt.xticks(rotation=45)

    plt.subplot(1, 2, 2)
    sns.violinplot(data=stats_df, x='label', y='std', palette='viridis')
    plt.title('Distribution of Intensity Standard Deviations by Group')
    plt.xlabel('Group')
    plt.ylabel('Standard Deviation')
    plt.xticks(rotation=45)

    plt.tight_layout()
    plt.show()

def plot_group_statistics(stats_df):
    """
    Plots statistical summaries by group using lightweight operations
    """
    plt.figure(figsize=(15, 5))

    # Group counts
    plt.subplot(1, 3, 1)
    group_counts = stats_df['label'].value_counts()
    sns.barplot(x=group_counts.index, y=group_counts.values, palette='viridis')
    plt.title('Sample Count by Group')
    plt.xlabel('Group')
    plt.ylabel('Count')
    plt.xticks(rotation=45)

    # Box plots - more memory efficient than complex plots
    plt.subplot(1, 3, 2)
    sns.boxplot(data=stats_df, x='label', y='mean', palette='viridis')
    plt.title('Mean Intensity Distribution')
    plt.xlabel('Group')
    plt.ylabel('Mean Intensity')
    plt.xticks(rotation=45)

    # Simple boxplot instead of violin plot
    plt.subplot(1, 3, 3)
    sns.boxplot(data=stats_df, x='label', y='std', palette='viridis')
    plt.title('Intensity Variance Distribution')
    plt.xlabel('Group')
    plt.ylabel('Standard Deviation')
    plt.xticks(rotation=45)

    plt.tight_layout()
    plt.show()

def visualize_sample_slices(stats_df, dataloader, samples_per_group=1):
    """
    Visualizes a limited number of samples from each group
    with efficient memory handling, selecting from the pre-processed samples
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Get unique groups
    groups = stats_df['label'].unique()

    # Dictionary to hold samples for each group
    samples_data = {}

    # Select paths from stats_df, stratified by group
    selected_paths = {}
    for group in groups:
        group_paths = stats_df[stats_df['label'] == group]['path'].values
        if len(group_paths) > 0:
            selected_paths[group] = random.sample(list(group_paths),
                                                min(samples_per_group, len(group_paths)))

    # Find these samples in the dataloader
    for batch in dataloader:
        volumes = batch['volume']
        paths = batch['path']
        labels = batch['label']

        for i, (vol, path, label) in enumerate(zip(volumes, paths, labels)):
            # Check if this path is in our selected paths
            for group, group_paths in selected_paths.items():
                if path in group_paths:
                    # Store the sample
                    key = f"{group}_{len(samples_data)}"
                    samples_data[key] = vol.cpu().numpy()
                    # Remove from selected_paths to avoid duplicates
                    selected_paths[group].remove(path)

        # Check if we have all samples
        if all(len(paths) == 0 for paths in selected_paths.values()):
            break

    # Visualize the samples
    num_groups = len(groups)
    plt.figure(figsize=(15, 5 * num_groups))

    for i, (key, vol) in enumerate(samples_data.items()):
        # Extract label
        label = key.split('_')[0]

        # Get middle slices
        vol = vol.squeeze()  # Remove channel dimension
        axial_slice = vol[vol.shape[0]//2, :, :]
        sagittal_slice = vol[:, vol.shape[1]//2, :]
        coronal_slice = vol[:, :, vol.shape[2]//2]

        # Plot slices
        plt.subplot(len(samples_data), 3, i*3 + 1)
        plt.imshow(axial_slice, cmap='gray')
        plt.title(f'{label} - Axial')
        plt.axis('off')

        plt.subplot(len(samples_data), 3, i*3 + 2)
        plt.imshow(sagittal_slice, cmap='gray')
        plt.title(f'{label} - Sagittal')
        plt.axis('off')

        plt.subplot(len(samples_data), 3, i*3 + 3)
        plt.imshow(coronal_slice, cmap='gray')
        plt.title(f'{label} - Coronal')
        plt.axis('off')

    plt.tight_layout()
    plt.show()

# Run optimized EDA
if __name__ == "__main__":
    start_time = time.time()
    print("Starting Memory-Efficient EDA with Stratified Sampling...")

    # Create dataloaders with small batch size
    print("\nCreating memory-efficient dataloaders...")
    train_loader, val_loader = create_memory_efficient_dataloaders(df, batch_size=2)

    # Analyze with stratified sampling and more samples
    print("\nAnalyzing training dataset (with stratification)...")
    train_stats = analyze_dataset_statistics_efficiently(
        train_loader,
        max_samples=100,  # Increased from 50 to 100
        min_samples_per_group=15  # Ensure at least 15 samples per group
    )

    # Plot distributions and statistics
    print("\nPlotting intensity distributions...")
    plot_intensity_distributions(train_stats)

    print("\nPlotting group statistics...")
    plot_group_statistics(train_stats)

    # Visualize a few examples
    print("\nVisualizing example slices...")
    visualize_sample_slices(train_stats, train_loader, samples_per_group=1)

    # Print summary statistics by group
    print("\nSummary Statistics by Group:")
    summary_stats = train_stats.groupby('label').agg({
        'mean': ['mean', 'std'],
        'std': ['mean', 'std'],
        'min': ['mean', 'std'],
        'max': ['mean', 'std']
    }).round(3)
    print(summary_stats)

    # Memory cleanup
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    print(f"\nEDA completed in {time.time() - start_time:.2f} seconds!")
    print_memory_stats()

"""### Slice Intensity Variance Analysis"""

def analyze_slice_variance(dataloader, num_samples_per_group=5):
    """
    Analyzes slice-wise variance across different views for each patient group
    """
    print("Analyzing slice-wise variance patterns...")

    # Initialize storage for variances
    group_variances = {
        'PD': {'axial': [], 'coronal': [], 'sagittal': []},
        'Control': {'axial': [], 'coronal': [], 'sagittal': []},
        'SWEDD': {'axial': [], 'coronal': [], 'sagittal': []}
    }
    sample_counts = {'PD': 0, 'Control': 0, 'SWEDD': 0}

    try:
        for batch in tqdm(dataloader, desc="Computing slice variances"):
            volumes = batch['volume']
            labels = batch['label']

            for volume, label in zip(volumes, labels):
                label = label if isinstance(label, str) else label.item()

                if sample_counts[label] >= num_samples_per_group:
                    continue

                # Get volume data
                vol_data = volume.squeeze().numpy()
                d, h, w = vol_data.shape

                # Compute variance for each slice in each view
                axial_var = [np.var(vol_data[i, :, :]) for i in range(d)]
                coronal_var = [np.var(vol_data[:, i, :]) for i in range(h)]
                sagittal_var = [np.var(vol_data[:, :, i]) for i in range(w)]

                # Store variances
                group_variances[label]['axial'].append(axial_var)
                group_variances[label]['coronal'].append(coronal_var)
                group_variances[label]['sagittal'].append(sagittal_var)

                sample_counts[label] += 1

            # Check if we have enough samples from each group
            if all(count >= num_samples_per_group for count in sample_counts.values()):
                break

            # Memory cleanup
            del volumes, labels
            gc.collect()
            torch.cuda.empty_cache()

    except Exception as e:
        print(f"Error during variance analysis: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

    # Compute average variances across samples for each group
    avg_variances = {}
    for group in group_variances:
        avg_variances[group] = {
            view: np.mean(variances, axis=0)
            for view, variances in group_variances[group].items()
        }

    return avg_variances

# Plot the slice variance results
def plot_slice_variances(avg_variances):
    """
    Creates line plots for slice-wise variance analysis
    """
    views = ['axial', 'coronal', 'sagittal']
    fig, axes = plt.subplots(1, 3, figsize=(20, 6))

    for idx, view in enumerate(views):
        ax = axes[idx]

        for group in avg_variances:
            variances = avg_variances[group][view]
            ax.plot(range(len(variances)), variances, label=group)

        ax.set_title(f'{view.capitalize()} View - Slice-wise Variance')
        ax.set_xlabel('Slice Index')
        ax.set_ylabel('Average Variance')
        ax.legend()
        ax.grid(True)

    plt.tight_layout()
    plt.show()

# Analyze slice-wise variance
print("\nAnalyzing slice-wise variance patterns...")
avg_variances = analyze_slice_variance(train_loader, num_samples_per_group=5)

if avg_variances is not None:
    print("\nPlotting slice-wise variance analysis...")
    plot_slice_variances(avg_variances)

"""# Model Phase

## 1. Autoencoder

### Model Setup
"""

# Cell 11: Optimized Autoencoder with Memory Management
import torch
import torch.nn as nn
import torch.nn.functional as F
from collections import OrderedDict

class ConvBlock(nn.Module):
    """Memory-efficient convolutional block with batch normalization and ReLU activation."""
    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):
        super().__init__()
        self.block = nn.Sequential(OrderedDict([
            ('conv', nn.Conv3d(in_channels, out_channels, kernel_size, stride, padding)),
            ('bn', nn.BatchNorm3d(out_channels)),
            ('relu', nn.ReLU(inplace=True))  # inplace ReLU for memory efficiency
        ]))

    def forward(self, x):
        return self.block(x)

class Encoder(nn.Module):
    """3D Encoder network optimized for 64×128×128 input volumes."""
    def __init__(self, latent_dim=256):
        super().__init__()

        # Initial feature extraction
        self.init_conv = ConvBlock(1, 16)  # (1, 64, 128, 128) -> (16, 64, 128, 128)

        # Downsampling path with progressive channel increase
        self.down1 = nn.Sequential(
            ConvBlock(16, 32, stride=2),    # -> (32, 32, 64, 64)
            ConvBlock(32, 32)               # -> (32, 32, 64, 64)
        )

        self.down2 = nn.Sequential(
            ConvBlock(32, 64, stride=2),    # -> (64, 16, 32, 32)
            ConvBlock(64, 64)               # -> (64, 16, 32, 32)
        )

        self.down3 = nn.Sequential(
            ConvBlock(64, 128, stride=2),   # -> (128, 8, 16, 16)
            ConvBlock(128, 128)             # -> (128, 8, 16, 16)
        )

        self.down4 = nn.Sequential(
            ConvBlock(128, 256, stride=2),  # -> (256, 4, 8, 8)
            ConvBlock(256, 256)             # -> (256, 4, 8, 8)
        )

        # Project to latent space
        self.flatten_size = 256 * 4 * 8 * 8
        self.fc = nn.Linear(self.flatten_size, latent_dim)

    def forward(self, x):
        x = self.init_conv(x)
        d1 = self.down1(x)
        d2 = self.down2(d1)
        d3 = self.down3(d2)
        d4 = self.down4(d3)

        # Flatten and project to latent space
        flat = torch.flatten(d4, start_dim=1)
        z = self.fc(flat)

        return z

class Decoder(nn.Module):
    """3D Decoder network optimized for 64×128×128 output volumes."""
    def __init__(self, latent_dim=256):
        super().__init__()

        self.flatten_size = 256 * 4 * 8 * 8
        self.fc = nn.Linear(latent_dim, self.flatten_size)

        # Upsampling path
        self.up1 = nn.Sequential(
            nn.Upsample(scale_factor=2, mode='trilinear', align_corners=False),
            ConvBlock(256, 128),
            ConvBlock(128, 128)
        )

        self.up2 = nn.Sequential(
            nn.Upsample(scale_factor=2, mode='trilinear', align_corners=False),
            ConvBlock(128, 64),
            ConvBlock(64, 64)
        )

        self.up3 = nn.Sequential(
            nn.Upsample(scale_factor=2, mode='trilinear', align_corners=False),
            ConvBlock(64, 32),
            ConvBlock(32, 32)
        )

        self.up4 = nn.Sequential(
            nn.Upsample(scale_factor=2, mode='trilinear', align_corners=False),
            ConvBlock(32, 16),
            ConvBlock(16, 16)
        )

        # Final convolution
        self.final_conv = nn.Conv3d(16, 1, kernel_size=1)

    def forward(self, z):
        # Reshape from latent space
        x = self.fc(z)
        x = x.view(-1, 256, 4, 8, 8)

        # Upsampling
        x = self.up1(x)
        x = self.up2(x)
        x = self.up3(x)
        x = self.up4(x)

        # Final convolution
        x = self.final_conv(x)

        return x

class BaseAutoencoder(nn.Module):
    """Memory-optimized 3D Autoencoder for 64×128×128 medical volumes."""
    def __init__(self, latent_dim=256):
        super().__init__()
        self.encoder = Encoder(latent_dim)
        self.decoder = Decoder(latent_dim)

    def forward(self, x):
        z = self.encoder(x)
        reconstruction = self.decoder(z)
        return reconstruction

    def encode(self, x):
        """Encode input to latent space"""
        z = self.encoder(x)
        return z

    def decode(self, z):
        """Decode from latent space (for generation)"""
        return self.decoder(z)

# Memory and GPU test function
def test_autoencoder(batch_size=2):
    """Test the autoencoder with dummy data and verify memory usage."""
    print("\nTesting Autoencoder Architecture...")

    try:
        # Create model and move to GPU
        model = BaseAutoencoder(latent_dim=256)
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = model.to(device)

        # Print model summary
        num_params = sum(p.numel() for p in model.parameters())
        print(f"\nModel Parameters: {num_params:,}")

        # Create dummy input (64x128x128 volume)
        dummy_input = torch.randn(batch_size, 1, 64, 128, 128, device=device)

        # Print initial memory usage
        print("\nInitial GPU Memory Usage:")
        print_memory_stats()

        # Test encoding
        print("\nTesting encoder...")
        with torch.no_grad():
            latent = model.encode(dummy_input)
        print(f"Input shape: {dummy_input.shape}")
        print(f"Latent shape: {latent.shape}")

        # Test full forward pass
        print("\nTesting forward pass...")
        with torch.no_grad():
            output = model(dummy_input)

        # Print output shape and final memory usage
        print(f"Output shape: {output.shape}")
        print("\nFinal GPU Memory Usage:")
        print_memory_stats()

        # Verify shapes
        assert output.shape == dummy_input.shape, f"Shape mismatch: {output.shape} vs {dummy_input.shape}"

        # Clean up
        del model, dummy_input, output, latent
        torch.cuda.empty_cache()

        print("\nAutoencoder test completed successfully!")
        return True

    except Exception as e:
        print(f"Error testing autoencoder: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

# Run test if this cell is executed
if __name__ == "__main__":
    test_autoencoder(batch_size=2)

# Cell 12: Training Configuration and Utilities
import os
import json
import time
from pathlib import Path
import torch.optim as optim
from torch.optim.lr_scheduler import ReduceLROnPlateau
import torch.cuda.amp as amp
import math

class TrainingConfig:
    """Training configuration optimized for NVIDIA 4070Ti"""
    def __init__(self, **kwargs):
        # Model parameters
        self.latent_dim = kwargs.get('latent_dim', 256)

        # Training parameters
        self.learning_rate = kwargs.get('learning_rate', 1e-4)
        self.batch_size = kwargs.get('batch_size', 4)
        self.accumulation_steps = kwargs.get('accumulation_steps', 4)
        self.epochs = kwargs.get('epochs', 100)
        self.early_stopping_patience = kwargs.get('early_stopping_patience', 10)

        # Optimization
        self.use_mixed_precision = kwargs.get('use_mixed_precision', True)
        self.weight_decay = kwargs.get('weight_decay', 1e-5)
        self.gradient_clip = kwargs.get('gradient_clip', 1.0)

        # Dataloader parameters
        self.num_workers = kwargs.get('num_workers', 2)
        self.pin_memory = kwargs.get('pin_memory', True)

        # Checkpoint parameters
        self.checkpoint_dir = kwargs.get('checkpoint_dir', 'checkpoints')
        self.model_name = kwargs.get('model_name', 'autoencoder')
        self.save_interval = kwargs.get('save_interval', 5)

        # Create checkpoint directory
        Path(self.checkpoint_dir).mkdir(parents=True, exist_ok=True)

        # Print configuration summary
        print(f"\n{'='*50}")
        print(f"TRAINING CONFIGURATION")
        print(f"{'='*50}")
        print(f"Model: {self.model_name} with latent dim {self.latent_dim}")
        print(f"Batch size: {self.batch_size} × {self.accumulation_steps} steps = {self.batch_size * self.accumulation_steps} effective")
        print(f"Learning rate: {self.learning_rate}")
        print(f"Mixed precision: {'Enabled' if self.use_mixed_precision else 'Disabled'}")
        print(f"Epochs: {self.epochs} with patience {self.early_stopping_patience}")
        print(f"Dataloader workers: {self.num_workers}")
        print(f"Checkpoints saved to: {self.checkpoint_dir}")
        print(f"{'='*50}\n")

class EarlyStopping:
    """Early stopping handler with patience"""
    def __init__(self, patience=10, min_delta=0, verbose=True):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = float('inf')
        self.early_stop = False
        self.verbose = verbose
        self.best_epoch = 0

    def __call__(self, val_loss, epoch):
        if val_loss < self.best_loss - self.min_delta:
            if self.verbose:
                improvement = self.best_loss - val_loss
                print(f"Validation loss improved by {improvement:.6f}")
            self.best_loss = val_loss
            self.counter = 0
            self.best_epoch = epoch
            return True  # Model improved
        else:
            self.counter += 1
            if self.verbose:
                print(f"Early stopping counter: {self.counter}/{self.patience}")
            if self.counter >= self.patience:
                self.early_stop = True
                if self.verbose:
                    print(f"Early stopping triggered. Best epoch was {self.best_epoch}.")
            return False  # Model didn't improve

class CheckpointHandler:
    """Handles saving and loading of model checkpoints"""
    def __init__(self, checkpoint_dir, model_name):
        self.checkpoint_dir = Path(checkpoint_dir)
        self.model_name = model_name
        self.checkpoint_path = self.checkpoint_dir / f"{model_name}_checkpoint.pth"
        self.best_model_path = self.checkpoint_dir / f"{model_name}_best.pth"
        self.metadata_path = self.checkpoint_dir / f"{model_name}_metadata.json"

        # Ensure directory exists
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)

    def save(self, model, optimizer, scheduler, epoch, train_losses, val_losses, is_best=False):
        """Save model checkpoint and training metadata"""
        # Save model checkpoint
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
            'train_losses': train_losses,
            'val_losses': val_losses
        }

        # Always save latest checkpoint
        torch.save(checkpoint, self.checkpoint_path)

        # Save best model separately if this is the best model
        if is_best:
            torch.save(model.state_dict(), self.best_model_path)
            print(f"Saved best model to {self.best_model_path}")

        # Save metadata
        metadata = {
            'last_epoch': epoch,
            'train_losses': train_losses,
            'val_losses': val_losses,
            'timestamp': time.strftime("%Y-%m-%d %H:%M:%S")
        }
        with open(self.metadata_path, 'w') as f:
            json.dump(metadata, f, indent=4)

    def load(self, model, optimizer=None, scheduler=None, device=None):
        """Load model checkpoint and return training metadata"""
        if not self.checkpoint_path.exists():
            print(f"No checkpoint found at {self.checkpoint_path}")
            return None

        if device is None:
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # Load checkpoint
        checkpoint = torch.load(self.checkpoint_path, map_location=device)

        # Load model state
        model.load_state_dict(checkpoint['model_state_dict'])

        # Optionally load optimizer and scheduler states
        if optimizer is not None and 'optimizer_state_dict' in checkpoint:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

        if scheduler is not None and checkpoint['scheduler_state_dict'] is not None:
            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])

        print(f"Loaded checkpoint from epoch {checkpoint['epoch']}")

        return {
            'epoch': checkpoint['epoch'],
            'train_losses': checkpoint['train_losses'],
            'val_losses': checkpoint['val_losses']
        }

def create_optimizer(model, config):
    """Create optimizer with weight decay and parameter grouping"""
    # Separate parameters that should have weight decay from those that shouldn't
    decay_params = []
    no_decay_params = []

    for name, param in model.named_parameters():
        if 'bias' in name or 'bn' in name:
            no_decay_params.append(param)
        else:
            decay_params.append(param)

    # Create parameter groups
    param_groups = [
        {'params': decay_params, 'weight_decay': config.weight_decay},
        {'params': no_decay_params, 'weight_decay': 0.0}
    ]

    # Create optimizer
    optimizer = optim.AdamW(param_groups, lr=config.learning_rate)

    return optimizer

def create_scheduler(optimizer, config):
    """Create learning rate scheduler"""
    return ReduceLROnPlateau(
        optimizer,
        mode='min',
        factor=0.5,
        patience=5,
        verbose=True,
        min_lr=1e-6
    )

def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, min_lr=0.0, last_epoch=-1):
    """Create a cosine learning rate schedule with warmup"""
    def lr_lambda(current_step):
        # Warmup
        if current_step < num_warmup_steps:
            return float(current_step) / float(max(1, num_warmup_steps))

        # Cosine decay
        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))
        factor = 0.5 * (1.0 + math.cos(math.pi * progress))
        return max(min_lr, factor)

    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch)

# Test configuration if this cell is executed
if __name__ == "__main__":
    config = TrainingConfig(
        latent_dim=256,
        batch_size=2,
        accumulation_steps=8,
        learning_rate=1e-4,
        epochs=100,
        model_name="autoencoder_test"
    )

    model = BaseAutoencoder(config.latent_dim)
    optimizer = create_optimizer(model, config)
    scheduler = create_scheduler(optimizer, config)

    print("Configuration and utilities loaded successfully!")

# Cell 13: Training Loop Implementation
import torch
import torch.nn as nn
from tqdm import tqdm
import matplotlib.pyplot as plt
import numpy as np
import gc
import time

def train_autoencoder(model, train_loader, val_loader, config=None):
    """Optimized training loop with GPU memory management and progress tracking"""
    if config is None:
        config = TrainingConfig()

    # Set up device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    # Initialize components
    criterion = nn.MSELoss()
    optimizer = create_optimizer(model, config)
    scheduler = create_scheduler(optimizer, config)
    early_stopping = EarlyStopping(patience=config.early_stopping_patience)
    checkpoint_handler = CheckpointHandler(config.checkpoint_dir, config.model_name)

    # Mixed precision setup
    scaler = amp.GradScaler(enabled=config.use_mixed_precision)

    # Training tracking variables
    train_losses = []
    val_losses = []
    best_val_loss = float('inf')
    start_time = time.time()

    # Load checkpoint if available
    start_epoch = 0
    checkpoint_data = checkpoint_handler.load(model, optimizer, scheduler, device)
    if checkpoint_data:
        start_epoch = checkpoint_data['epoch'] + 1
        train_losses = checkpoint_data['train_losses']
        val_losses = checkpoint_data['val_losses']
        print(f"Resuming training from epoch {start_epoch}")

    # Calculate total steps for progress tracking
    total_steps = len(train_loader) * config.epochs

    # Training loop
    try:
        # Create progress bar for total training
        total_pbar = tqdm(total=total_steps, desc="Total Progress", position=0)
        total_pbar.update(start_epoch * len(train_loader))

        for epoch in range(start_epoch, config.epochs):
            # Training phase
            model.train()
            epoch_loss = 0
            optimizer.zero_grad()  # Zero gradients at epoch start

            train_pbar = tqdm(train_loader,
                            desc=f'Epoch {epoch+1}/{config.epochs} [Train]',
                            leave=False,
                            position=1)

            for batch_idx, batch in enumerate(train_pbar):
                try:
                    # Move data to device
                    volumes = batch['volume'].to(device, non_blocking=True)

                    # Mixed precision forward pass
                    with amp.autocast(enabled=config.use_mixed_precision):
                        reconstructed = model(volumes)
                        loss = criterion(reconstructed, volumes)
                        # Scale loss by accumulation steps
                        loss = loss / config.accumulation_steps

                    # Mixed precision backward pass
                    scaler.scale(loss).backward()

                    # Gradient accumulation
                    if (batch_idx + 1) % config.accumulation_steps == 0 or (batch_idx + 1 == len(train_loader)):
                        # Clip gradients to prevent exploding gradients
                        if config.gradient_clip > 0:
                            scaler.unscale_(optimizer)
                            torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip)

                        # Update weights
                        scaler.step(optimizer)
                        scaler.update()
                        optimizer.zero_grad()

                    # Track loss (using the non-scaled loss for reporting)
                    batch_loss = loss.item() * config.accumulation_steps
                    epoch_loss += batch_loss

                    # Update progress bars
                    train_pbar.set_postfix({'loss': f"{batch_loss:.6f}"})
                    total_pbar.update(1)

                    # Memory cleanup
                    del volumes, reconstructed, loss
                    torch.cuda.empty_cache()

                except RuntimeError as e:
                    if "out of memory" in str(e):
                        print(f"\nOOM in batch {batch_idx}. Cleaning up...")
                        torch.cuda.empty_cache()
                        gc.collect()
                        # Skip this batch and continue
                        continue
                    raise e

            # Calculate average training loss
            avg_train_loss = epoch_loss / len(train_loader)
            train_losses.append(avg_train_loss)

            # Validation phase
            model.eval()
            val_loss = 0

            val_pbar = tqdm(val_loader,
                          desc=f'Epoch {epoch+1}/{config.epochs} [Val]',
                          leave=False,
                          position=1)

            with torch.no_grad():
                for batch in val_pbar:
                    try:
                        volumes = batch['volume'].to(device)
                        reconstructed = model(volumes)
                        loss = criterion(reconstructed, volumes)
                        val_loss += loss.item()

                        val_pbar.set_postfix({'loss': f"{loss.item():.6f}"})

                        # Memory cleanup
                        del volumes, reconstructed, loss
                        torch.cuda.empty_cache()

                    except RuntimeError as e:
                        if "out of memory" in str(e):
                            print("\nOOM during validation. Cleaning up...")
                            torch.cuda.empty_cache()
                            gc.collect()
                            continue
                        raise e

            # Calculate average validation loss
            avg_val_loss = val_loss / len(val_loader)
            val_losses.append(avg_val_loss)

            # Update learning rate
            scheduler.step(avg_val_loss)

            # Check if this is the best model
            is_best = avg_val_loss < best_val_loss
            if is_best:
                best_val_loss = avg_val_loss

            # Save checkpoint
            if (epoch + 1) % config.save_interval == 0 or is_best or (epoch + 1 == config.epochs):
                checkpoint_handler.save(
                    model, optimizer, scheduler,
                    epoch, train_losses, val_losses,
                    is_best=is_best
                )

            # Print epoch summary
            elapsed_time = time.time() - start_time
            time_per_epoch = elapsed_time / (epoch - start_epoch + 1) if epoch >= start_epoch else 0
            est_time_left = time_per_epoch * (config.epochs - epoch - 1)

            print(f"\nEpoch {epoch+1}/{config.epochs} completed in {time_per_epoch:.2f}s")
            print(f"Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f}")
            print(f"Learning rate: {optimizer.param_groups[0]['lr']:.8f}")
            print(f"Est. time remaining: {est_time_left/60:.2f} minutes")
            print_memory_stats()

            # Early stopping check
            if early_stopping(avg_val_loss, epoch):
                if early_stopping.early_stop:
                    print("\nEarly stopping triggered!")
                    break

    except KeyboardInterrupt:
        print("\nTraining interrupted by user!")
        # Still save the model
        checkpoint_handler.save(
            model, optimizer, scheduler,
            epoch, train_losses, val_losses,
            is_best=False
        )

    finally:
        # Close progress bars
        if 'total_pbar' in locals():
            total_pbar.close()

        # Plot training history
        plt.figure(figsize=(12, 5))

        # Plot full history
        plt.subplot(1, 2, 1)
        plt.plot(train_losses, label='Train Loss')
        plt.plot(val_losses, label='Validation Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title('Full Training History')
        plt.legend()
        plt.grid(True)

        # Plot recent history (last 30 epochs or full history if < 30 epochs)
        plt.subplot(1, 2, 2)
        recent = min(30, len(train_losses))
        if recent > 5:  # Only plot recent if we have enough epochs
            plt.plot(train_losses[-recent:], label='Train Loss')
            plt.plot(val_losses[-recent:], label='Validation Loss')
            plt.xlabel('Epoch')
            plt.ylabel('Loss')
            plt.title(f'Last {recent} Epochs')
            plt.legend()
            plt.grid(True)

        plt.tight_layout()
        plt.savefig(os.path.join(config.checkpoint_dir, f"{config.model_name}_training_history.png"))
        plt.show()

        # Print final training summary
        total_time = time.time() - start_time
        print(f"\nTraining completed in {total_time/60:.2f} minutes")
        print(f"Best validation loss: {best_val_loss:.6f}")

        return train_losses, val_losses, model

"""### Training"""

# Run training if this cell is executed
if __name__ == "__main__":

    # Initialize model
    model = BaseAutoencoder(latent_dim=256)

    # Create configuration
    config = TrainingConfig(
    latent_dim=256,
    batch_size=8,              # Increased from 2 to 8
    accumulation_steps=8,      # Increased from 4 to 8 (effective batch size = 64)
    learning_rate=1e-4,
    epochs=200,
    early_stopping_patience=10,
    use_mixed_precision=True,
    num_workers=4,             # Increased from 2 to 4
    model_name="autoencoder_v1"
)

    # Train model
    print("\nStarting training...")
    train_losses, val_losses, trained_model = train_autoencoder(
        model, train_loader, val_loader, config
    )

"""### Evaluation and Result Visualization"""

# Cell 14: Updated Autoencoder Evaluation with Anatomical Slices
import torch
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path
import json
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import seaborn as sns
from tqdm import tqdm

def load_trained_model(checkpoint_dir, model_name, latent_dim=256):
    """Load best trained model for evaluation"""
    model_path = Path(checkpoint_dir) / f"{model_name}_best.pth"
    metadata_path = Path(checkpoint_dir) / f"{model_name}_metadata.json"

    if not model_path.exists():
        # Try loading checkpoint if best model doesn't exist
        model_path = Path(checkpoint_dir) / f"{model_name}_checkpoint.pth"
        if not model_path.exists():
            raise FileNotFoundError(f"No model found at {model_path}")

        # Load from checkpoint
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = BaseAutoencoder(latent_dim=latent_dim)
        checkpoint = torch.load(model_path, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
    else:
        # Load best model
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = BaseAutoencoder(latent_dim=latent_dim)
        model.load_state_dict(torch.load(model_path, map_location=device))

    # Load training history
    if metadata_path.exists():
        with open(metadata_path, 'r') as f:
            metadata = json.load(f)
    else:
        metadata = {"train_losses": [], "val_losses": []}

    model.eval()
    model.to(device)

    return model, metadata

def plot_training_history(metadata):
    """Plot training and validation loss history"""
    plt.figure(figsize=(12, 5))

    train_losses = metadata["train_losses"]
    val_losses = metadata["val_losses"]

    # Plot full history
    plt.subplot(1, 2, 1)
    plt.plot(train_losses, label='Train Loss')
    plt.plot(val_losses, label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Full Training History')
    plt.legend()
    plt.grid(True)

    # Plot recent history (last 30 epochs or all if < 30)
    plt.subplot(1, 2, 2)
    recent = min(30, len(train_losses))
    if recent > 5:  # Only plot recent history if we have enough epochs
        plt.plot(train_losses[-recent:], label='Train Loss')
        plt.plot(val_losses[-recent:], label='Validation Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title(f'Last {recent} Epochs')
        plt.legend()
        plt.grid(True)

    plt.tight_layout()
    plt.show()

def visualize_reconstruction_samples(model, dataloader, num_samples=3):
    """Visualize original vs reconstructed volumes for samples from the dataset using anatomically relevant slices"""
    device = next(model.parameters()).device

    # Get samples from dataloader
    samples = []
    labels = []

    for batch in dataloader:
        volumes = batch['volume']
        batch_labels = batch['label']

        for i in range(min(len(volumes), num_samples - len(samples))):
            samples.append(volumes[i:i+1])
            labels.append(batch_labels[i])

        if len(samples) >= num_samples:
            break

    # Visualize each sample
    with torch.no_grad():
        for idx, (sample, label) in enumerate(zip(samples, labels)):
            # Get original volume
            orig_vol = sample.to(device)

            # Generate reconstruction
            reconstructed = model(orig_vol)

            # Move to CPU for visualization
            orig_vol = orig_vol.cpu().squeeze().numpy()
            recon_vol = reconstructed.cpu().squeeze().numpy()

            # Create figure for this sample
            fig = plt.figure(figsize=(16, 12))
            plt.suptitle(f"Sample {idx+1} - Group: {label}", fontsize=16)

            # Define anatomically relevant slices
            axial_slice = 32      # Axial view - slice 32
            coronal_slice = 50    # Coronal view - slice 50
            sagittal_slice1 = 55  # Sagittal view - slice 55
            sagittal_slice2 = 70  # Sagittal view - slice 70

            # Plot original slices - top row
            plt.subplot(2, 4, 1)
            plt.imshow(orig_vol[axial_slice], cmap='gray', vmin=0, vmax=3)
            plt.title(f"Original - Axial (z={axial_slice})")
            plt.axis('off')

            plt.subplot(2, 4, 2)
            plt.imshow(orig_vol[:, coronal_slice, :], cmap='gray', vmin=0, vmax=3)
            plt.title(f"Original - Coronal (y={coronal_slice})")
            plt.axis('off')

            plt.subplot(2, 4, 3)
            plt.imshow(orig_vol[:, :, sagittal_slice1], cmap='gray', vmin=0, vmax=3)
            plt.title(f"Original - Sagittal (x={sagittal_slice1})")
            plt.axis('off')

            plt.subplot(2, 4, 4)
            plt.imshow(orig_vol[:, :, sagittal_slice2], cmap='gray', vmin=0, vmax=3)
            plt.title(f"Original - Sagittal (x={sagittal_slice2})")
            plt.axis('off')

            # Plot reconstructed slices - bottom row
            plt.subplot(2, 4, 5)
            plt.imshow(recon_vol[axial_slice], cmap='gray', vmin=0, vmax=3)
            plt.title(f"Reconstructed - Axial (z={axial_slice})")
            plt.axis('off')

            plt.subplot(2, 4, 6)
            plt.imshow(recon_vol[:, coronal_slice, :], cmap='gray', vmin=0, vmax=3)
            plt.title(f"Reconstructed - Coronal (y={coronal_slice})")
            plt.axis('off')

            plt.subplot(2, 4, 7)
            plt.imshow(recon_vol[:, :, sagittal_slice1], cmap='gray', vmin=0, vmax=3)
            plt.title(f"Reconstructed - Sagittal (x={sagittal_slice1})")
            plt.axis('off')

            plt.subplot(2, 4, 8)
            plt.imshow(recon_vol[:, :, sagittal_slice2], cmap='gray', vmin=0, vmax=3)
            plt.title(f"Reconstructed - Sagittal (x={sagittal_slice2})")
            plt.axis('off')

            plt.tight_layout()
            plt.show()

def compute_reconstruction_error(model, dataloader):
    """Compute detailed reconstruction error metrics on validation set"""
    device = next(model.parameters()).device
    criterion = nn.MSELoss(reduction='none')

    total_mse = 0
    total_samples = 0
    error_by_label = {}

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Computing metrics"):
            volumes = batch['volume'].to(device)
            labels = batch['label']

            # Get reconstructions
            reconstructed = model(volumes)

            # Compute MSE loss per sample
            mse = criterion(reconstructed, volumes)

            # Average over dimensions
            mse = mse.mean(dim=(1, 2, 3, 4)).cpu().numpy()

            # Track overall error
            total_mse += mse.sum()
            total_samples += volumes.shape[0]

            # Track error by label
            for i, label in enumerate(labels):
                if label not in error_by_label:
                    error_by_label[label] = []
                error_by_label[label].append(mse[i])

            # Memory cleanup
            del volumes, reconstructed, mse
            torch.cuda.empty_cache()

    # Calculate overall metrics
    avg_mse = total_mse / total_samples
    rmse = np.sqrt(avg_mse)

    print("\nReconstruction Error Metrics:")
    print(f"Overall MSE: {avg_mse:.6f}")
    print(f"Overall RMSE: {rmse:.6f}")

    # Calculate metrics by group
    print("\nReconstruction Error by Group:")
    for label, errors in error_by_label.items():
        group_mse = np.mean(errors)
        group_rmse = np.sqrt(group_mse)
        group_std = np.std(errors)
        print(f"{label}:")
        print(f"  MSE: {group_mse:.6f} ± {group_std:.6f}")
        print(f"  RMSE: {group_rmse:.6f}")

    # Plot error distribution by group
    plt.figure(figsize=(10, 6))
    for label, errors in error_by_label.items():
        sns.histplot(errors, alpha=0.5, label=label, bins=20, kde=True)

    plt.title("Reconstruction Error Distribution by Group")
    plt.xlabel("Mean Squared Error")
    plt.ylabel("Count")
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()

    return avg_mse, error_by_label

def extract_latent_vectors(model, dataloader, max_samples=None):
    """Extract latent vectors from all samples in the dataloader"""
    device = next(model.parameters()).device

    latent_vectors = []
    labels = []
    paths = []

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Extracting latent vectors"):
            volumes = batch['volume'].to(device)
            batch_labels = batch['label']
            batch_paths = batch['path']

            # Extract latent vectors
            z = model.encode(volumes)

            # Store results
            latent_vectors.append(z.cpu().numpy())
            labels.extend(batch_labels)
            paths.extend(batch_paths)

            # Memory cleanup
            del volumes, z
            torch.cuda.empty_cache()

            # Check if we have enough samples
            if max_samples and len(labels) >= max_samples:
                latent_vectors = np.vstack(latent_vectors)
                latent_vectors = latent_vectors[:max_samples]
                labels = labels[:max_samples]
                paths = paths[:max_samples]
                break

    # Stack all latent vectors if we didn't break early
    if isinstance(latent_vectors[0], np.ndarray):
        latent_vectors = np.vstack(latent_vectors)

    return latent_vectors, labels, paths

def visualize_latent_space(latent_vectors, labels, method='tsne'):
    """Visualize latent space using t-SNE or PCA"""
    plt.figure(figsize=(10, 8))

    # Create label-to-color mapping for consistent colors
    unique_labels = list(set(labels))
    colors = plt.cm.tab10(np.linspace(0, 1, len(unique_labels)))
    label_to_color = {label: colors[i] for i, label in enumerate(unique_labels)}

    # Apply dimensionality reduction
    if method.lower() == 'tsne':
        print("Computing t-SNE projection...")
        reducer = TSNE(n_components=2, random_state=42, perplexity=min(30, len(latent_vectors) - 1))
        title = 't-SNE Visualization of Latent Space'
    else:
        print("Computing PCA projection...")
        reducer = PCA(n_components=2, random_state=42)
        title = 'PCA Visualization of Latent Space'

    # Apply reduction
    reduced_vecs = reducer.fit_transform(latent_vectors)

    # Create scatter plot
    for label in unique_labels:
        # Get indices where this label appears
        indices = [i for i, l in enumerate(labels) if l == label]

        # Plot these points
        plt.scatter(
            reduced_vecs[indices, 0],
            reduced_vecs[indices, 1],
            label=label,
            color=label_to_color[label],
            alpha=0.7,
            edgecolor='w',
            s=100
        )

    plt.title(title, fontsize=14)
    plt.xlabel("Dimension 1", fontsize=12)
    plt.ylabel("Dimension 2", fontsize=12)
    plt.legend(title="Group", fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

    return reduced_vecs

def plot_latent_dimension_activation(latent_vectors, labels):
    """Analyze activation patterns of latent dimensions"""
    # Create a DataFrame with latent dimensions and labels
    import pandas as pd

    # First, convert labels to categorical for better plotting
    unique_labels = list(set(labels))
    label_map = {label: i for i, label in enumerate(unique_labels)}
    label_indices = [label_map[label] for label in labels]

    # Create DataFrames
    latent_df = pd.DataFrame(latent_vectors)
    latent_df['label'] = labels

    # Compute mean activation by group
    mean_activations = {}
    for label in unique_labels:
        group_vectors = latent_vectors[np.array(labels) == label]
        mean_activations[label] = np.mean(group_vectors, axis=0)

    # Identify top discriminative dimensions
    activation_matrix = np.vstack([mean_activations[label] for label in unique_labels])
    variance = np.var(activation_matrix, axis=0)
    top_dims = np.argsort(variance)[-10:]  # Top 10 dimensions

    # Plot mean activation for top dimensions
    plt.figure(figsize=(14, 6))

    # Plot heatmap
    plt.subplot(1, 2, 1)
    heatmap_data = pd.DataFrame({
        f"Dim {i}": [mean_activations[label][i] for label in unique_labels]
        for i in top_dims
    })
    heatmap_data.index = unique_labels

    sns.heatmap(heatmap_data, cmap='coolwarm', center=0,
               annot=True, fmt=".2f", cbar_kws={'label': 'Mean Activation'})
    plt.title("Mean Activation of Top Discriminative Dimensions")

    # Plot box plots for top 5 dimensions
    plt.subplot(1, 2, 2)

    # Create data for boxplot
    plot_data = []
    labels_for_plot = []
    positions = []

    for i, dim in enumerate(top_dims[:5]):  # Top 5 for clarity
        for j, label in enumerate(unique_labels):
            group_values = latent_vectors[np.array(labels) == label, dim]
            plot_data.append(group_values)
            labels_for_plot.append(f"{label}")
            positions.append(i + j * 0.25)

    # Create boxplot
    boxplot = plt.boxplot(plot_data, positions=positions, patch_artist=True, widths=0.15)

    # Customize boxplot colors
    colors = plt.cm.tab10(np.linspace(0, 1, len(unique_labels)))
    for i, label in enumerate(unique_labels):
        indices = [j for j, l in enumerate(labels_for_plot) if l == label]
        for idx in indices:
            boxplot['boxes'][idx].set_facecolor(colors[i])

    # Add labels and ticks
    plt.xticks([i + (len(unique_labels) - 1) * 0.125 for i in range(5)],
              [f"Dim {d}" for d in top_dims[:5]])
    plt.title("Distribution of Top 5 Discriminative Dimensions")
    plt.ylabel("Activation Value")

    # Add legend
    for i, label in enumerate(unique_labels):
        plt.plot([], [], 'o', color=colors[i], label=label)
    plt.legend(title="Group")

    plt.tight_layout()
    plt.show()

    return top_dims

def find_outliers(model, dataloader, threshold_std=2.5):
    """Identify outliers based on reconstruction error"""
    device = next(model.parameters()).device
    criterion = nn.MSELoss(reduction='none')

    all_errors = []
    all_paths = []
    all_labels = []

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Finding outliers"):
            volumes = batch['volume'].to(device)
            paths = batch['path']
            labels = batch['label']

            # Get reconstructions
            reconstructed = model(volumes)

            # Compute MSE loss per sample
            mse = criterion(reconstructed, volumes)

            # Average over dimensions
            mse = mse.mean(dim=(1, 2, 3, 4)).cpu().numpy()

            # Store results
            all_errors.extend(mse)
            all_paths.extend(paths)
            all_labels.extend(labels)

            # Memory cleanup
            del volumes, reconstructed, mse
            torch.cuda.empty_cache()

    # Convert to numpy arrays
    all_errors = np.array(all_errors)

    # Compute statistics
    mean_error = np.mean(all_errors)
    std_error = np.std(all_errors)

    # Find outliers (samples with error > mean + threshold_std * std)
    threshold = mean_error + threshold_std * std_error
    outlier_indices = np.where(all_errors > threshold)[0]

    print(f"\nOutlier Analysis:")
    print(f"Mean error: {mean_error:.6f}")
    print(f"Error standard deviation: {std_error:.6f}")
    print(f"Outlier threshold: {threshold:.6f}")
    print(f"Found {len(outlier_indices)} outliers out of {len(all_errors)} samples ({len(outlier_indices)/len(all_errors)*100:.2f}%)")

    # Create dictionary of outliers
    outliers = {
        all_paths[i]: {
            'error': all_errors[i],
            'label': all_labels[i],
            'z_score': (all_errors[i] - mean_error) / std_error
        }
        for i in outlier_indices
    }

    # Plot error distribution with outlier threshold
    plt.figure(figsize=(10, 6))
    plt.hist(all_errors, bins=30, alpha=0.7)
    plt.axvline(threshold, color='r', linestyle='--', label=f'Threshold ({threshold_std} std)')
    plt.title("Reconstruction Error Distribution")
    plt.xlabel("Mean Squared Error")
    plt.ylabel("Count")
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()

    return outliers, all_errors, all_paths, all_labels

def visualize_outliers(model, outliers, threshold=0.01):
    """Visualize the top outliers with highest reconstruction error using anatomically relevant slices"""
    device = next(model.parameters()).device

    # Sort outliers by error (descending)
    sorted_outliers = sorted(outliers.items(), key=lambda x: x[1]['error'], reverse=True)

    # Visualize top outliers
    num_outliers = min(5, len(sorted_outliers))

    for i in range(num_outliers):
        path, info = sorted_outliers[i]
        error = info['error']
        label = info['label']
        z_score = info['z_score']

        try:
            # Load the original volume
            with torch.no_grad():
                # Load DICOM file
                original_volume, _ = load_dicom(path)
                original_volume = original_volume[9:73, :, :]

                # Process volume
                norm_vol, _, _ = process_volume(original_volume, target_shape=(64, 128, 128))

                # Convert to tensor and add batch dimension
                vol_tensor = torch.from_numpy(np.expand_dims(norm_vol, axis=(0, 1))).float().to(device)

                # Get reconstruction
                reconstructed = model(vol_tensor)

                # Move tensors to CPU and remove batch and channel dimensions
                vol_np = vol_tensor.cpu().squeeze().numpy()
                recon_np = reconstructed.cpu().squeeze().numpy()

                # Create figure
                fig = plt.figure(figsize=(16, 12))
                plt.suptitle(f"Outlier {i+1}: {path.split('/')[-1]}\nGroup: {label}, Error: {error:.6f}, Z-score: {z_score:.2f}", fontsize=14)

                # Define anatomically relevant slices
                axial_slice = 32      # Axial view - slice 32
                coronal_slice = 50    # Coronal view - slice 50
                sagittal_slice1 = 55  # Sagittal view - slice 55
                sagittal_slice2 = 70  # Sagittal view - slice 70

                # Plot original slices - top row
                plt.subplot(2, 4, 1)
                plt.imshow(vol_np[axial_slice], cmap='gray', vmin=0, vmax=3)
                plt.title(f"Original - Axial (z={axial_slice})")
                plt.axis('off')

                plt.subplot(2, 4, 2)
                plt.imshow(vol_np[:, coronal_slice, :], cmap='gray', vmin=0, vmax=3)
                plt.title(f"Original - Coronal (y={coronal_slice})")
                plt.axis('off')

                plt.subplot(2, 4, 3)
                plt.imshow(vol_np[:, :, sagittal_slice1], cmap='gray', vmin=0, vmax=3)
                plt.title(f"Original - Sagittal (x={sagittal_slice1})")
                plt.axis('off')

                plt.subplot(2, 4, 4)
                plt.imshow(vol_np[:, :, sagittal_slice2], cmap='gray', vmin=0, vmax=3)
                plt.title(f"Original - Sagittal (x={sagittal_slice2})")
                plt.axis('off')

                # Plot reconstructed slices - bottom row
                plt.subplot(2, 4, 5)
                plt.imshow(recon_np[axial_slice], cmap='gray', vmin=0, vmax=3)
                plt.title(f"Reconstructed - Axial (z={axial_slice})")
                plt.axis('off')

                plt.subplot(2, 4, 6)
                plt.imshow(recon_np[:, coronal_slice, :], cmap='gray', vmin=0, vmax=3)
                plt.title(f"Reconstructed - Coronal (y={coronal_slice})")
                plt.axis('off')

                plt.subplot(2, 4, 7)
                plt.imshow(recon_np[:, :, sagittal_slice1], cmap='gray', vmin=0, vmax=3)
                plt.title(f"Reconstructed - Sagittal (x={sagittal_slice1})")
                plt.axis('off')

                plt.subplot(2, 4, 8)
                plt.imshow(recon_np[:, :, sagittal_slice2], cmap='gray', vmin=0, vmax=3)
                plt.title(f"Reconstructed - Sagittal (x={sagittal_slice2})")
                plt.axis('off')

                plt.tight_layout()
                plt.show()

        except Exception as e:
            print(f"Error visualizing outlier {path}: {e}")

# Run evaluation if this cell is executed
if __name__ == "__main__":
    # Load model and metadata
    model, metadata = load_trained_model('checkpoints', 'autoencoder_v1', latent_dim=256)

    # Plot training history
    plot_training_history(metadata)

    # Visualize reconstructions
    visualize_reconstruction_samples(model, val_loader, num_samples=3)

    # Compute reconstruction error
    avg_mse, error_by_label = compute_reconstruction_error(model, val_loader)

    # Extract and visualize latent space
    latent_vectors, labels, paths = extract_latent_vectors(model, val_loader, max_samples=200)
    reduced_vecs = visualize_latent_space(latent_vectors, labels, method='tsne')
    reduced_vecs = visualize_latent_space(latent_vectors, labels, method='pca')

    # Analyze latent dimensions
    top_dims = plot_latent_dimension_activation(latent_vectors, labels)

    # Find and visualize outliers
    outliers, all_errors, all_paths, all_labels = find_outliers(model, val_loader, threshold_std=2.5)
    visualize_outliers(model, outliers)

# Cell 15: Updated Autoencoder Latent Dimension Visualization with Anatomical Slices
import torch
import matplotlib.pyplot as plt
import numpy as np
from matplotlib.colors import LinearSegmentedColormap
import torch.nn.functional as F

def visualize_latent_dimension(model, dataloader, dimension_idx, alpha=5.0, group=None):
    """
    Visualize what a specific latent dimension represents by modifying it
    and observing the effect on brain reconstruction using anatomically relevant slices.

    Parameters:
        model: Trained autoencoder model
        dataloader: DataLoader containing samples
        dimension_idx: The latent dimension to manipulate (e.g., 231)
        alpha: Strength of the dimension manipulation
        group: Optional filter for specific patient group (e.g., 'PD', 'Control')
    """
    device = next(model.parameters()).device
    model.eval()

    # Find a suitable sample (optionally from specific group)
    for batch in dataloader:
        volumes = batch['volume']
        labels = batch['label']
        paths = batch['path']

        if group is not None:
            # Find samples from the specified group
            group_indices = [i for i, label in enumerate(labels) if label == group]
            if not group_indices:
                continue
            # Use the first matching sample
            idx = group_indices[0]
            sample = volumes[idx:idx+1].to(device)
            sample_label = labels[idx]
            sample_path = paths[idx]
        else:
            # Just use the first sample
            sample = volumes[0:1].to(device)
            sample_label = labels[0]
            sample_path = paths[0]

        break  # Exit after finding a sample

    with torch.no_grad():
        # Encode the sample to get its latent representation
        z = model.encode(sample)

        # Create modified latent vectors
        z_plus = z.clone()
        z_minus = z.clone()

        # Modify the specific dimension
        z_plus[0, dimension_idx] += alpha
        z_minus[0, dimension_idx] -= alpha

        # Decode the original and modified latent vectors
        original_reconstruction = model.decode(z)
        plus_reconstruction = model.decode(z_plus)
        minus_reconstruction = model.decode(z_minus)

        # Move tensors to CPU and convert to numpy for visualization
        original_vol = original_reconstruction.cpu().squeeze().numpy()
        plus_vol = plus_reconstruction.cpu().squeeze().numpy()
        minus_vol = minus_reconstruction.cpu().squeeze().numpy()

        # Calculate the difference maps
        plus_diff = plus_vol - original_vol
        minus_diff = minus_vol - original_vol

        # Set up the figure
        fig = plt.figure(figsize=(16, 15))
        plt.suptitle(f"Visualization of Dimension {dimension_idx} in Brain Space\nPatient Group: {sample_label}", fontsize=16)

        # Define anatomically relevant slices
        slices = {
            'axial': 32,       # Axial z=32
            'coronal': 50,     # Coronal y=50
            'sagittal1': 55,   # Sagittal x=55
            'sagittal2': 70    # Sagittal x=70
        }

        # Create a custom colormap for difference maps
        diff_cmap = plt.cm.RdBu_r  # Red-Blue colormap with red for negative, blue for positive

        # Row 1: Original reconstruction - 4 views
        plt.subplot(5, 4, 1)
        plt.imshow(original_vol[slices['axial']], cmap='gray', vmin=0, vmax=3)
        plt.title(f"Original (Axial z={slices['axial']})")
        plt.axis('off')

        plt.subplot(5, 4, 2)
        plt.imshow(original_vol[:, slices['coronal'], :], cmap='gray', vmin=0, vmax=3)
        plt.title(f"Original (Coronal y={slices['coronal']})")
        plt.axis('off')

        plt.subplot(5, 4, 3)
        plt.imshow(original_vol[:, :, slices['sagittal1']], cmap='gray', vmin=0, vmax=3)
        plt.title(f"Original (Sagittal x={slices['sagittal1']})")
        plt.axis('off')

        plt.subplot(5, 4, 4)
        plt.imshow(original_vol[:, :, slices['sagittal2']], cmap='gray', vmin=0, vmax=3)
        plt.title(f"Original (Sagittal x={slices['sagittal2']})")
        plt.axis('off')

        # Row 2: Increased dimension - 4 views
        plt.subplot(5, 4, 5)
        plt.imshow(plus_vol[slices['axial']], cmap='gray', vmin=0, vmax=3)
        plt.title(f"Dim {dimension_idx} + {alpha}")
        plt.axis('off')

        plt.subplot(5, 4, 6)
        plt.imshow(plus_vol[:, slices['coronal'], :], cmap='gray', vmin=0, vmax=3)
        plt.title(f"Dim {dimension_idx} + {alpha}")
        plt.axis('off')

        plt.subplot(5, 4, 7)
        plt.imshow(plus_vol[:, :, slices['sagittal1']], cmap='gray', vmin=0, vmax=3)
        plt.title(f"Dim {dimension_idx} + {alpha}")
        plt.axis('off')

        plt.subplot(5, 4, 8)
        plt.imshow(plus_vol[:, :, slices['sagittal2']], cmap='gray', vmin=0, vmax=3)
        plt.title(f"Dim {dimension_idx} + {alpha}")
        plt.axis('off')

        # Row 3: Decreased dimension - 4 views
        plt.subplot(5, 4, 9)
        plt.imshow(minus_vol[slices['axial']], cmap='gray', vmin=0, vmax=3)
        plt.title(f"Dim {dimension_idx} - {alpha}")
        plt.axis('off')

        plt.subplot(5, 4, 10)
        plt.imshow(minus_vol[:, slices['coronal'], :], cmap='gray', vmin=0, vmax=3)
        plt.title(f"Dim {dimension_idx} - {alpha}")
        plt.axis('off')

        plt.subplot(5, 4, 11)
        plt.imshow(minus_vol[:, :, slices['sagittal1']], cmap='gray', vmin=0, vmax=3)
        plt.title(f"Dim {dimension_idx} - {alpha}")
        plt.axis('off')

        plt.subplot(5, 4, 12)
        plt.imshow(minus_vol[:, :, slices['sagittal2']], cmap='gray', vmin=0, vmax=3)
        plt.title(f"Dim {dimension_idx} - {alpha}")
        plt.axis('off')

        # Row 4: Difference map (increased - original) - 4 views
        plt.subplot(5, 4, 13)
        im1 = plt.imshow(plus_diff[slices['axial']], cmap=diff_cmap, vmin=-0.5, vmax=0.5)
        plt.title(f"Difference (+)")
        plt.axis('off')

        plt.subplot(5, 4, 14)
        plt.imshow(plus_diff[:, slices['coronal'], :], cmap=diff_cmap, vmin=-0.5, vmax=0.5)
        plt.title(f"Difference (+)")
        plt.axis('off')

        plt.subplot(5, 4, 15)
        plt.imshow(plus_diff[:, :, slices['sagittal1']], cmap=diff_cmap, vmin=-0.5, vmax=0.5)
        plt.title(f"Difference (+)")
        plt.axis('off')

        plt.subplot(5, 4, 16)
        plt.imshow(plus_diff[:, :, slices['sagittal2']], cmap=diff_cmap, vmin=-0.5, vmax=0.5)
        plt.title(f"Difference (+)")
        plt.axis('off')

        # Row 5: Difference map (decreased - original) - 4 views
        plt.subplot(5, 4, 17)
        im2 = plt.imshow(minus_diff[slices['axial']], cmap=diff_cmap, vmin=-0.5, vmax=0.5)
        plt.title(f"Difference (-)")
        plt.axis('off')

        plt.subplot(5, 4, 18)
        plt.imshow(minus_diff[:, slices['coronal'], :], cmap=diff_cmap, vmin=-0.5, vmax=0.5)
        plt.title(f"Difference (-)")
        plt.axis('off')

        plt.subplot(5, 4, 19)
        plt.imshow(minus_diff[:, :, slices['sagittal1']], cmap=diff_cmap, vmin=-0.5, vmax=0.5)
        plt.title(f"Difference (-)")
        plt.axis('off')

        plt.subplot(5, 4, 20)
        plt.imshow(minus_diff[:, :, slices['sagittal2']], cmap=diff_cmap, vmin=-0.5, vmax=0.5)
        plt.title(f"Difference (-)")
        plt.axis('off')

        # Add colorbar for difference maps
        cbar_ax = fig.add_axes([0.93, 0.15, 0.02, 0.3])
        cbar = plt.colorbar(im1, cax=cbar_ax)
        cbar.set_label('Difference Intensity')

        plt.tight_layout()
        plt.subplots_adjust(top=0.92, right=0.9)
        plt.show()

        # Return both the sample info and reconstructions for further analysis
        return {
            'label': sample_label,
            'path': sample_path,
            'original': original_vol,
            'plus': plus_vol,
            'minus': minus_vol,
            'plus_diff': plus_diff,
            'minus_diff': minus_diff
        }

def explore_top_dimensions(model, dataloader, dimensions, groups=None):
    """
    Explore the top discriminative dimensions across different patient groups using anatomically relevant slices.

    Parameters:
        model: Trained autoencoder model
        dataloader: DataLoader containing samples
        dimensions: List of dimension indices to explore
        groups: List of groups to include (default is all groups)
    """
    if groups is None:
        # Get all unique groups from the first batch
        for batch in dataloader:
            groups = list(set(batch['label']))
            break

    for dimension in dimensions:
        print(f"\n{'='*80}")
        print(f"Exploring Dimension {dimension}")
        print(f"{'='*80}")

        for group in groups:
            print(f"\nVisualizing for group: {group}")
            results = visualize_latent_dimension(model, dataloader, dimension, alpha=8.0, group=group)

            # Optional: compute statistics about the affected regions
            plus_diff = results['plus_diff']
            minus_diff = results['minus_diff']

            # Calculate the average absolute effect size
            mean_effect = (np.mean(np.abs(plus_diff)) + np.mean(np.abs(minus_diff))) / 2

            # Find the regions most affected (top 5% of voxels)
            top_voxels_plus = np.percentile(np.abs(plus_diff), 95)
            top_voxels_minus = np.percentile(np.abs(minus_diff), 95)

            print(f"  Mean effect size: {mean_effect:.5f}")
            print(f"  Top 5% threshold: +{top_voxels_plus:.5f}, -{top_voxels_minus:.5f}")

            # Optional: Add a small delay for better visualization
            import time
            time.sleep(1)

def generate_feature_importance_map(model, dataloader, dimension_idx, group=None, num_samples=5):
    """
    Generate a more robust feature importance map for a specific dimension
    by aggregating effects across multiple samples using anatomically relevant slices.

    Parameters:
        model: Trained autoencoder model
        dataloader: DataLoader containing samples
        dimension_idx: The latent dimension to analyze
        group: Optional filter for specific patient group
        num_samples: Number of samples to aggregate
    """
    device = next(model.parameters()).device
    model.eval()

    # Storage for aggregated results
    aggregated_plus_diff = None
    aggregated_minus_diff = None
    sample_count = 0

    # Find samples (optionally from specific group)
    for batch in dataloader:
        volumes = batch['volume']
        labels = batch['label']

        if group is not None:
            # Find samples from the specified group
            group_indices = [i for i, label in enumerate(labels) if label == group]
            indices = group_indices
        else:
            # Use all samples in batch
            indices = range(len(volumes))

        for idx in indices:
            if sample_count >= num_samples:
                break

            sample = volumes[idx:idx+1].to(device)

            with torch.no_grad():
                # Encode the sample
                z = model.encode(sample)

                # Create modified latent vectors
                z_plus = z.clone()
                z_minus = z.clone()

                # Modify the specific dimension
                z_plus[0, dimension_idx] += 5.0
                z_minus[0, dimension_idx] -= 5.0

                # Decode the vectors
                original_reconstruction = model.decode(z)
                plus_reconstruction = model.decode(z_plus)
                minus_reconstruction = model.decode(z_minus)

                # Calculate the difference maps
                plus_diff = (plus_reconstruction - original_reconstruction).cpu().squeeze().numpy()
                minus_diff = (minus_reconstruction - original_reconstruction).cpu().squeeze().numpy()

                # Aggregate the difference maps
                if aggregated_plus_diff is None:
                    aggregated_plus_diff = plus_diff
                    aggregated_minus_diff = minus_diff
                else:
                    aggregated_plus_diff += plus_diff
                    aggregated_minus_diff += minus_diff

                sample_count += 1

        if sample_count >= num_samples:
            break

    # Average the difference maps
    aggregated_plus_diff /= sample_count
    aggregated_minus_diff /= sample_count

    # Compute absolute importance map (average of plus and minus effects)
    importance_map = (np.abs(aggregated_plus_diff) + np.abs(aggregated_minus_diff)) / 2

    # Define anatomically relevant slices
    axial_slice = 32      # Axial view - slice 32
    coronal_slice = 50    # Coronal view - slice 50
    sagittal_slice1 = 55  # Sagittal view - slice 55
    sagittal_slice2 = 70  # Sagittal view - slice 70

    # Visualize the importance map
    fig = plt.figure(figsize=(16, 8))
    plt.suptitle(f"Feature Importance Map for Dimension {dimension_idx}" +
                (f" (Group: {group})" if group else ""), fontsize=16)

    # Plot axial, coronal, and sagittal views
    plt.subplot(2, 4, 1)
    plt.imshow(importance_map[axial_slice], cmap='hot')
    plt.title(f"Axial (z={axial_slice})")
    plt.colorbar(fraction=0.046, pad=0.04)
    plt.axis('off')

    plt.subplot(2, 4, 2)
    plt.imshow(importance_map[:, coronal_slice, :], cmap='hot')
    plt.title(f"Coronal (y={coronal_slice})")
    plt.colorbar(fraction=0.046, pad=0.04)
    plt.axis('off')

    plt.subplot(2, 4, 3)
    plt.imshow(importance_map[:, :, sagittal_slice1], cmap='hot')
    plt.title(f"Sagittal (x={sagittal_slice1})")
    plt.colorbar(fraction=0.046, pad=0.04)
    plt.axis('off')

    plt.subplot(2, 4, 4)
    plt.imshow(importance_map[:, :, sagittal_slice2], cmap='hot')
    plt.title(f"Sagittal (x={sagittal_slice2})")
    plt.colorbar(fraction=0.046, pad=0.04)
    plt.axis('off')

    # Plot activating direction (positive difference)
    plt.subplot(2, 4, 5)
    plt.imshow(aggregated_plus_diff[axial_slice], cmap='bwr', vmin=-0.2, vmax=0.2)
    plt.title(f"Activating Direction - Axial")
    plt.colorbar(fraction=0.046, pad=0.04)
    plt.axis('off')

    plt.subplot(2, 4, 6)
    plt.imshow(aggregated_plus_diff[:, coronal_slice, :], cmap='bwr', vmin=-0.2, vmax=0.2)
    plt.title(f"Activating Direction - Coronal")
    plt.colorbar(fraction=0.046, pad=0.04)
    plt.axis('off')

    plt.subplot(2, 4, 7)
    plt.imshow(aggregated_plus_diff[:, :, sagittal_slice1], cmap='bwr', vmin=-0.2, vmax=0.2)
    plt.title(f"Activating Direction - Sagittal 1")
    plt.colorbar(fraction=0.046, pad=0.04)
    plt.axis('off')

    plt.subplot(2, 4, 8)
    plt.imshow(aggregated_plus_diff[:, :, sagittal_slice2], cmap='bwr', vmin=-0.2, vmax=0.2)
    plt.title(f"Activating Direction - Sagittal 2")
    plt.colorbar(fraction=0.046, pad=0.04)
    plt.axis('off')

    plt.tight_layout()
    plt.subplots_adjust(top=0.9)
    plt.show()

    return importance_map, aggregated_plus_diff, aggregated_minus_diff

# Example usage
if __name__ == "__main__":
    # Load your trained model first
    model, metadata = load_trained_model('checkpoints', 'autoencoder_v1', latent_dim=256)

    # Example 1: Visualize one specific important dimension for a specific group
    # This shows what happens when you increase or decrease this dimension
    visualize_latent_dimension(model, val_loader, dimension_idx=231, alpha=8.0, group='PD')

    # Example 2: Compare the same dimension across different patient groups
    # Uncomment to see how the same dimension affects different groups
    explore_top_dimensions(model, val_loader, dimensions=[231, 94, 154], groups=['PD', 'Control', 'SWEDD'])

    # Example 3: Generate a feature importance map by aggregating across multiple samples
    # This shows which brain regions are most affected by this dimension
    generate_feature_importance_map(model, val_loader, dimension_idx=94, group='PD', num_samples=10)
    generate_feature_importance_map(model, val_loader, dimension_idx=94, group='Control', num_samples=10)
    generate_feature_importance_map(model, val_loader, dimension_idx=231, group='PD', num_samples=10)
    generate_feature_importance_map(model, val_loader, dimension_idx=231, group='Control', num_samples=10)

"""## 2. Variational Autoencoder (VAE)

### Model Setup
"""

# Cell 16: Corrected VAE Implementation
import torch
import torch.nn as nn
import torch.nn.functional as F
from collections import OrderedDict

class VAE(nn.Module):
    """
    3D Variational Autoencoder optimized for 64×128×128 medical volumes with
    memory-efficient implementation for NVIDIA 4070Ti.
    """
    def __init__(self, latent_dim=256):
        super().__init__()
        self.encoder = VAEEncoder(latent_dim)
        self.decoder = VAEDecoder(latent_dim)
        # Enable CUDNN benchmarking for optimal performance
        torch.backends.cudnn.benchmark = True
        self.latent_dim = latent_dim

    def reparameterize(self, mu, log_var):
        """
        Reparameterization trick to enable backpropagation through sampling.
        """
        if self.training:
            std = torch.exp(0.5 * log_var)
            eps = torch.randn_like(std)
            return mu + eps * std
        return mu

    def forward(self, x):
        # Encode input to get mean and variance of latent distribution
        mu, log_var = self.encoder(x)

        # Sample from latent distribution
        z = self.reparameterize(mu, log_var)

        # Decode sampled latent vector
        reconstruction = self.decoder(z)

        return reconstruction, mu, log_var

    def encode(self, x):
        """Encode input to latent parameters without sampling"""
        mu, log_var = self.encoder(x)
        return mu, log_var

    def generate(self, z=None, num_samples=1):
        """Generate samples from latent space or random samples"""
        device = next(self.parameters()).device

        if z is None:
            # Generate random latent vectors
            z = torch.randn(num_samples, self.latent_dim, device=device)

        # Decode latent vectors to generate samples
        with torch.no_grad():
            samples = self.decoder(z)

        return samples

class ConvBlock(nn.Module):
    """Memory-efficient convolutional block with batch normalization and ReLU."""
    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):
        super().__init__()
        self.block = nn.Sequential(OrderedDict([
            ('conv', nn.Conv3d(in_channels, out_channels, kernel_size, stride, padding)),
            ('bn', nn.BatchNorm3d(out_channels)),
            ('relu', nn.ReLU(inplace=True))  # inplace ReLU for memory efficiency
        ]))

    def forward(self, x):
        return self.block(x)

class VAEEncoder(nn.Module):
    """3D Encoder network with probabilistic latent space."""
    def __init__(self, latent_dim=256):
        super().__init__()

        # Initial feature extraction
        self.init_conv = ConvBlock(1, 16)  # 64×128×128 -> same size

        # Downsampling path with progressive channel increase
        self.down1 = nn.Sequential(
            ConvBlock(16, 32, stride=2),    # -> 32×64×64
            ConvBlock(32, 32)
        )

        self.down2 = nn.Sequential(
            ConvBlock(32, 64, stride=2),    # -> 16×32×32
            ConvBlock(64, 64)
        )

        self.down3 = nn.Sequential(
            ConvBlock(64, 128, stride=2),   # -> 8×16×16
            ConvBlock(128, 128)
        )

        self.down4 = nn.Sequential(
            ConvBlock(128, 256, stride=2),  # -> 4×8×8
            ConvBlock(256, 256)
        )

        # Project to latent parameters
        self.flatten_size = 256 * 4 * 8 * 8
        self.fc_mu = nn.Linear(self.flatten_size, latent_dim)
        self.fc_var = nn.Linear(self.flatten_size, latent_dim)

    def forward(self, x):
        # Initial convolution
        x = self.init_conv(x)

        # Downsampling
        x = self.down1(x)
        x = self.down2(x)
        x = self.down3(x)
        x = self.down4(x)

        # Flatten and project to latent parameters
        flat = torch.flatten(x, start_dim=1)
        mu = self.fc_mu(flat)
        log_var = self.fc_var(flat)

        return mu, log_var

class VAEDecoder(nn.Module):
    """3D Decoder network without enhanced capacity - matching the saved model."""
    def __init__(self, latent_dim=256):
        super().__init__()

        self.flatten_size = 256 * 4 * 8 * 8
        self.fc = nn.Linear(latent_dim, self.flatten_size)

        # Upsampling path without enhanced capacity (matches the saved model)
        self.up1 = nn.Sequential(
            nn.Upsample(scale_factor=2, mode='trilinear', align_corners=False),
            ConvBlock(256, 128),  # Original channel size from saved model
            ConvBlock(128, 128)
        )

        self.up2 = nn.Sequential(
            nn.Upsample(scale_factor=2, mode='trilinear', align_corners=False),
            ConvBlock(128, 64),   # Original channel size from saved model
            ConvBlock(64, 64)
        )

        self.up3 = nn.Sequential(
            nn.Upsample(scale_factor=2, mode='trilinear', align_corners=False),
            ConvBlock(64, 32),    # Original channel size from saved model
            ConvBlock(32, 32)
        )

        self.up4 = nn.Sequential(
            nn.Upsample(scale_factor=2, mode='trilinear', align_corners=False),
            ConvBlock(32, 16),    # Original channel size from saved model
            ConvBlock(16, 16)
        )

        # No refinement layer in the original model

        # Final convolution
        self.final_conv = nn.Conv3d(16, 1, kernel_size=1)

    def forward(self, z):
        # Reshape from latent space
        x = self.fc(z)
        x = x.view(-1, 256, 4, 8, 8)

        # Upsampling
        x = self.up1(x)
        x = self.up2(x)
        x = self.up3(x)
        x = self.up4(x)

        # Final convolution (no activation to allow for negative values if needed)
        x = self.final_conv(x)

        return x

# Custom loss function for VAE
class VAELoss(nn.Module):
    """
    VAE loss combining reconstruction loss and KL divergence
    with beta parameter to control the trade-off and free bits approach.
    """
    def __init__(self, beta=0.0005, beta_warmup_steps=5000, free_bits=3.0):
        super().__init__()
        self.beta_base = beta
        self.beta_warmup_steps = beta_warmup_steps
        self.current_step = 0
        self.free_bits = free_bits  # Free bits parameter to prevent posterior collapse

    def forward(self, recon_x, x, mu, log_var):
        """
        Calculate VAE loss with enhanced beta annealing and free bits.
        """
        # Reconstruction loss (mean squared error)
        recon_loss = F.mse_loss(recon_x, x, reduction='mean')

        # KL divergence with free bits
        # Instead of penalizing all KL divergence, allow some "free bits"
        kl_raw = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp(), dim=1)

        # Apply free bits: only penalize when KL is above threshold
        kl_free = torch.maximum(kl_raw - self.free_bits, torch.zeros_like(kl_raw))
        kl_loss = torch.mean(kl_free)

        # Calculate beta with cyclical warmup
        # Cyclical annealing gives the model periods to focus on reconstruction
        if self.beta_warmup_steps > 0:
            # Implement cyclical annealing
            cycle_length = self.beta_warmup_steps // 2
            cycle_position = (self.current_step % cycle_length) / cycle_length
            if self.current_step // cycle_length % 2 == 0:  # Even cycles: warmup
                beta = self.beta_base * cycle_position
            else:  # Odd cycles: constant
                beta = self.beta_base
        else:
            beta = self.beta_base

        # Increment step counter
        self.current_step += 1

        # Total loss
        total_loss = recon_loss + beta * kl_loss

        return total_loss, recon_loss, kl_loss, beta

# Test VAE with dummy data
def test_vae(batch_size=2, latent_dim=256):
    """Test the VAE architecture and memory usage with dummy data."""
    print("\nTesting VAE Architecture...")

    try:
        # Create model and move to GPU
        model = VAE(latent_dim=latent_dim)
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = model.to(device)

        # Create loss function
        vae_loss = VAELoss(beta=0.005)

        # Print model summary
        num_params = sum(p.numel() for p in model.parameters())
        print(f"\nVAE Model Parameters: {num_params:,}")

        # Create dummy input
        dummy_input = torch.randn(batch_size, 1, 64, 128, 128, device=device)

        # Test forward pass
        print("\nTesting forward pass...")
        print_memory_stats()
        with torch.no_grad():
            recon, mu, log_var = model(dummy_input)

        # Test loss calculation
        loss, recon_loss, kl_loss, beta = vae_loss(recon, dummy_input, mu, log_var)

        print(f"\nInput shape: {dummy_input.shape}")
        print(f"Latent mu shape: {mu.shape}")
        print(f"Output shape: {recon.shape}")
        print(f"\nLoss values:")
        print(f"Total loss: {loss.item():.6f}")
        print(f"Reconstruction loss: {recon_loss.item():.6f}")
        print(f"KL loss: {kl_loss.item():.6f}")
        print(f"Beta value: {beta:.6f}")

        # Check memory usage
        print("\nGPU Memory After Forward Pass:")
        print_memory_stats()

        # Clean up
        del model, dummy_input, recon, mu, log_var
        torch.cuda.empty_cache()

        print("\nVAE test completed successfully!")
        return True

    except Exception as e:
        print(f"Error testing VAE: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

# Run test if this cell is executed
if __name__ == "__main__":
    test_vae(batch_size=2, latent_dim=256)

# Cell 17: VAE Training Configuration and Training Loop
import os
import json
import time
from pathlib import Path
import torch.optim as optim
from torch.optim.lr_scheduler import ReduceLROnPlateau
import torch.cuda.amp as amp
import gc
import torch

class VAEConfig:
    """Configuration for VAE training optimized for NVIDIA 4070Ti"""
    def __init__(self, **kwargs):
        # Model parameters
        self.latent_dim = kwargs.get('latent_dim', 256)

        # Training parameters
        self.learning_rate = kwargs.get('learning_rate', 5e-5)
        self.batch_size = kwargs.get('batch_size', 8)
        self.accumulation_steps = kwargs.get('accumulation_steps', 8)  # Effective batch size of 64
        self.epochs = kwargs.get('epochs', 100)
        self.early_stopping_patience = kwargs.get('early_stopping_patience', 15)

        # Loss parameters
        self.beta = kwargs.get('beta', 0.005)  # KL divergence weight
        self.beta_warmup_steps = kwargs.get('beta_warmup_steps', 2000)

        # Optimization
        self.use_mixed_precision = kwargs.get('use_mixed_precision', True)
        self.weight_decay = kwargs.get('weight_decay', 1e-6)
        self.gradient_clip = kwargs.get('gradient_clip', 1.0)

        # Dataloader parameters
        self.num_workers = kwargs.get('num_workers', 4)
        self.pin_memory = kwargs.get('pin_memory', True)

        # Checkpoint parameters
        self.checkpoint_dir = kwargs.get('checkpoint_dir', 'checkpoints')
        self.model_name = kwargs.get('model_name', 'vae_model')
        self.save_interval = kwargs.get('save_interval', 5)

        # Create checkpoint directory
        Path(self.checkpoint_dir).mkdir(parents=True, exist_ok=True)

        # Print configuration summary
        print(f"\n{'='*50}")
        print(f"VAE TRAINING CONFIGURATION")
        print(f"{'='*50}")
        print(f"Model: {self.model_name} with latent dim {self.latent_dim}")
        print(f"Batch size: {self.batch_size} × {self.accumulation_steps} steps = {self.batch_size * self.accumulation_steps} effective")
        print(f"Learning rate: {self.learning_rate}")
        print(f"Beta (KL weight): {self.beta} with {self.beta_warmup_steps} warmup steps")
        print(f"Mixed precision: {'Enabled' if self.use_mixed_precision else 'Disabled'}")
        print(f"Epochs: {self.epochs} with patience {self.early_stopping_patience}")
        print(f"Dataloader workers: {self.num_workers}")
        print(f"Checkpoints saved to: {self.checkpoint_dir}")
        print(f"{'='*50}\n")

class VAECheckpointHandler:
    """Handles saving and loading of VAE model checkpoints"""
    def __init__(self, checkpoint_dir, model_name):
        self.checkpoint_dir = Path(checkpoint_dir)
        self.model_name = model_name
        self.checkpoint_path = self.checkpoint_dir / f"{model_name}_checkpoint.pth"
        self.best_model_path = self.checkpoint_dir / f"{model_name}_best.pth"
        self.metadata_path = self.checkpoint_dir / f"{model_name}_metadata.json"

        # Ensure directory exists
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)

    def save(self, model, optimizer, scheduler, epoch, train_losses, val_losses,
           train_recon_losses, train_kl_losses, val_recon_losses, val_kl_losses,
           is_best=False):
        """Save VAE checkpoint with additional VAE-specific metrics"""
        # Save model checkpoint
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
            'train_losses': train_losses,
            'val_losses': val_losses,
            'train_recon_losses': train_recon_losses,
            'train_kl_losses': train_kl_losses,
            'val_recon_losses': val_recon_losses,
            'val_kl_losses': val_kl_losses
        }

        # Save checkpoint
        torch.save(checkpoint, self.checkpoint_path)

        # Save best model separately if this is the best model
        if is_best:
            torch.save(model.state_dict(), self.best_model_path)
            print(f"Saved best model to {self.best_model_path}")

        # Save metadata
        metadata = {
            'last_epoch': epoch,
            'train_losses': train_losses,
            'val_losses': val_losses,
            'train_recon_losses': train_recon_losses,
            'train_kl_losses': train_kl_losses,
            'val_recon_losses': val_recon_losses,
            'val_kl_losses': val_kl_losses,
            'timestamp': time.strftime("%Y-%m-%d %H:%M:%S")
        }
        with open(self.metadata_path, 'w') as f:
            json.dump(metadata, f, indent=4)

    def load(self, model, optimizer=None, scheduler=None, device=None):
        """Load model checkpoint and return training metadata"""
        if not self.checkpoint_path.exists():
            print(f"No checkpoint found at {self.checkpoint_path}")
            return None

        if device is None:
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # Load checkpoint
        checkpoint = torch.load(self.checkpoint_path, map_location=device)

        # Load model state
        model.load_state_dict(checkpoint['model_state_dict'])

        # Optionally load optimizer and scheduler states
        if optimizer is not None and 'optimizer_state_dict' in checkpoint:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

        if scheduler is not None and checkpoint.get('scheduler_state_dict') is not None:
            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])

        print(f"Loaded checkpoint from epoch {checkpoint['epoch']}")

        return {
            'epoch': checkpoint['epoch'],
            'train_losses': checkpoint.get('train_losses', []),
            'val_losses': checkpoint.get('val_losses', []),
            'train_recon_losses': checkpoint.get('train_recon_losses', []),
            'train_kl_losses': checkpoint.get('train_kl_losses', []),
            'val_recon_losses': checkpoint.get('val_recon_losses', []),
            'val_kl_losses': checkpoint.get('val_kl_losses', [])
        }

def create_vae_optimizer(model, config):
    """Create optimizer with parameter groups for VAE"""
    # Separate parameters that should have weight decay from those that shouldn't
    decay_params = []
    no_decay_params = []

    for name, param in model.named_parameters():
        if 'bias' in name or 'bn' in name:
            no_decay_params.append(param)
        else:
            decay_params.append(param)

    # Create parameter groups
    param_groups = [
        {'params': decay_params, 'weight_decay': config.weight_decay},
        {'params': no_decay_params, 'weight_decay': 0.0}
    ]

    # Create optimizer
    optimizer = optim.AdamW(param_groups, lr=config.learning_rate)

    return optimizer

def create_vae_scheduler(optimizer, config):
    """Create learning rate scheduler"""
    return ReduceLROnPlateau(
        optimizer,
        mode='min',
        factor=0.5,
        patience=5,
        verbose=True,
        min_lr=1e-6
    )

class VAEEarlyStopping:
    """Early stopping handler with patience for VAE"""
    def __init__(self, patience=10, min_delta=0, verbose=True):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = float('inf')
        self.early_stop = False
        self.verbose = verbose
        self.best_epoch = 0

    def __call__(self, val_loss, epoch):
        if val_loss < self.best_loss - self.min_delta:
            if self.verbose:
                improvement = self.best_loss - val_loss
                print(f"Validation loss improved by {improvement:.6f}")
            self.best_loss = val_loss
            self.counter = 0
            self.best_epoch = epoch
            return True  # Model improved
        else:
            self.counter += 1
            if self.verbose:
                print(f"Early stopping counter: {self.counter}/{self.patience}")
            if self.counter >= self.patience:
                self.early_stop = True
                if self.verbose:
                    print(f"Early stopping triggered. Best epoch was {self.best_epoch}.")
            return False  # Model didn't improve

def train_vae(model, train_loader, val_loader, config=None):
    """Memory-optimized VAE training loop with mixed precision and gradient accumulation"""
    if config is None:
        config = VAEConfig()

    # Set up device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    # Initialize components
    criterion = VAELoss(beta=config.beta, beta_warmup_steps=config.beta_warmup_steps)
    optimizer = create_vae_optimizer(model, config)
    scheduler = create_vae_scheduler(optimizer, config)
    early_stopping = VAEEarlyStopping(patience=config.early_stopping_patience)
    checkpoint_handler = VAECheckpointHandler(config.checkpoint_dir, config.model_name)

    # Mixed precision setup
    scaler = amp.GradScaler(enabled=config.use_mixed_precision)

    # Training tracking variables
    train_losses = []
    val_losses = []
    train_recon_losses = []
    train_kl_losses = []
    val_recon_losses = []
    val_kl_losses = []
    best_val_loss = float('inf')
    start_time = time.time()

    # Load checkpoint if available
    start_epoch = 0
    checkpoint_data = checkpoint_handler.load(model, optimizer, scheduler, device)
    if checkpoint_data:
        start_epoch = checkpoint_data['epoch'] + 1
        train_losses = checkpoint_data.get('train_losses', [])
        val_losses = checkpoint_data.get('val_losses', [])
        train_recon_losses = checkpoint_data.get('train_recon_losses', [])
        train_kl_losses = checkpoint_data.get('train_kl_losses', [])
        val_recon_losses = checkpoint_data.get('val_recon_losses', [])
        val_kl_losses = checkpoint_data.get('val_kl_losses', [])
        print(f"Resuming training from epoch {start_epoch}")

    # Calculate total steps for progress tracking
    total_steps = len(train_loader) * config.epochs

    # Training loop
    try:
        # Create progress bar for total training
        total_pbar = tqdm(total=total_steps, desc="Total Progress", position=0)
        total_pbar.update(start_epoch * len(train_loader))

        for epoch in range(start_epoch, config.epochs):
            # Training phase
            model.train()
            epoch_loss = 0
            epoch_recon_loss = 0
            epoch_kl_loss = 0
            current_beta = 0
            optimizer.zero_grad()  # Zero gradients at epoch start

            train_pbar = tqdm(train_loader,
                            desc=f'Epoch {epoch+1}/{config.epochs} [Train]',
                            leave=False,
                            position=1)

            for batch_idx, batch in enumerate(train_pbar):
                try:
                    # Move data to device
                    volumes = batch['volume'].to(device, non_blocking=True)

                    # Mixed precision forward pass
                    with amp.autocast(enabled=config.use_mixed_precision):
                        reconstructed, mu, log_var = model(volumes)
                        loss, recon_loss, kl_loss, beta = criterion(reconstructed, volumes, mu, log_var)
                        current_beta = beta
                        # Scale loss by accumulation steps
                        loss = loss / config.accumulation_steps

                    # Mixed precision backward pass
                    scaler.scale(loss).backward()

                    # Gradient accumulation
                    if (batch_idx + 1) % config.accumulation_steps == 0 or (batch_idx + 1 == len(train_loader)):
                        # Clip gradients to prevent exploding gradients
                        if config.gradient_clip > 0:
                            scaler.unscale_(optimizer)
                            torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip)

                        # Update weights
                        scaler.step(optimizer)
                        scaler.update()
                        optimizer.zero_grad()

                    # Track loss (using the non-scaled loss for reporting)
                    batch_loss = loss.item() * config.accumulation_steps
                    epoch_loss += batch_loss
                    epoch_recon_loss += recon_loss.item()
                    epoch_kl_loss += kl_loss.item()

                    # Update progress bars
                    train_pbar.set_postfix({
                        'loss': f"{batch_loss:.6f}",
                        'recon': f"{recon_loss.item():.6f}",
                        'kl': f"{kl_loss.item():.6f}",
                        'beta': f"{beta:.6f}"
                    })
                    total_pbar.update(1)

                    # Memory cleanup
                    del volumes, reconstructed, mu, log_var, loss, recon_loss, kl_loss
                    torch.cuda.empty_cache()

                except RuntimeError as e:
                    if "out of memory" in str(e):
                        print(f"\nOOM in batch {batch_idx}. Cleaning up...")
                        torch.cuda.empty_cache()
                        gc.collect()
                        # Skip this batch and continue
                        continue
                    else:
                        print(f"RuntimeError: {str(e)}")
                        raise e
                except Exception as e:
                    print(f"Unexpected error: {str(e)}")
                    import traceback
                    traceback.print_exc()
                    raise e

            # Calculate average training losses
            avg_train_loss = epoch_loss / len(train_loader)
            avg_train_recon_loss = epoch_recon_loss / len(train_loader)
            avg_train_kl_loss = epoch_kl_loss / len(train_loader)

            train_losses.append(avg_train_loss)
            train_recon_losses.append(avg_train_recon_loss)
            train_kl_losses.append(avg_train_kl_loss)

            # Validation phase
            model.eval()
            val_loss = 0
            val_recon_loss_sum = 0
            val_kl_loss_sum = 0

            val_pbar = tqdm(val_loader,
                          desc=f'Epoch {epoch+1}/{config.epochs} [Val]',
                          leave=False,
                          position=1)

            with torch.no_grad():
                for batch in val_pbar:
                    try:
                        volumes = batch['volume'].to(device)
                        reconstructed, mu, log_var = model(volumes)
                        loss, recon_loss, kl_loss, _ = criterion(reconstructed, volumes, mu, log_var)

                        val_loss += loss.item()
                        val_recon_loss_sum += recon_loss.item()
                        val_kl_loss_sum += kl_loss.item()

                        val_pbar.set_postfix({
                            'loss': f"{loss.item():.6f}",
                            'recon': f"{recon_loss.item():.6f}",
                            'kl': f"{kl_loss.item():.6f}"
                        })

                        # Memory cleanup
                        del volumes, reconstructed, mu, log_var, loss, recon_loss, kl_loss
                        torch.cuda.empty_cache()

                    except RuntimeError as e:
                        if "out of memory" in str(e):
                            print("\nOOM during validation. Cleaning up...")
                            torch.cuda.empty_cache()
                            gc.collect()
                            continue
                        else:
                            print(f"RuntimeError during validation: {str(e)}")
                            raise e
                    except Exception as e:
                        print(f"Unexpected error during validation: {str(e)}")
                        import traceback
                        traceback.print_exc()
                        raise e

            # Calculate average validation losses
            avg_val_loss = val_loss / len(val_loader)
            avg_val_recon_loss = val_recon_loss_sum / len(val_loader)
            avg_val_kl_loss = val_kl_loss_sum / len(val_loader)

            val_losses.append(avg_val_loss)
            val_recon_losses.append(avg_val_recon_loss)
            val_kl_losses.append(avg_val_kl_loss)

            # Update learning rate
            scheduler.step(avg_val_loss)

            # Check if this is the best model
            is_best = avg_val_loss < best_val_loss
            if is_best:
                best_val_loss = avg_val_loss

            # Save checkpoint
            if (epoch + 1) % config.save_interval == 0 or is_best or (epoch + 1 == config.epochs):
                # Save additional loss components
                checkpoint_handler.save(
                    model, optimizer, scheduler,
                    epoch, train_losses, val_losses,
                    train_recon_losses, train_kl_losses,
                    val_recon_losses, val_kl_losses,
                    is_best=is_best
                )

            # Print epoch summary
            elapsed_time = time.time() - start_time
            time_per_epoch = elapsed_time / (epoch - start_epoch + 1) if epoch >= start_epoch else 0
            est_time_left = time_per_epoch * (config.epochs - epoch - 1)

            print(f"\nEpoch {epoch+1}/{config.epochs} completed in {time_per_epoch:.2f}s")
            print(f"Train Loss: {avg_train_loss:.6f} (Recon: {avg_train_recon_loss:.6f}, KL: {avg_train_kl_loss:.6f})")
            print(f"Val Loss: {avg_val_loss:.6f} (Recon: {avg_val_recon_loss:.6f}, KL: {avg_val_kl_loss:.6f})")
            print(f"Learning rate: {optimizer.param_groups[0]['lr']:.8f}")
            print(f"Beta value: {current_beta:.6f}")
            print(f"Est. time remaining: {est_time_left/60:.2f} minutes")
            print_memory_stats()

            # Early stopping check
            if early_stopping(avg_val_loss, epoch):
                if early_stopping.early_stop:
                    print("\nEarly stopping triggered!")
                    break

    except KeyboardInterrupt:
        print("\nTraining interrupted by user!")
        # Still save the model
        checkpoint_handler.save(
            model, optimizer, scheduler,
            epoch, train_losses, val_losses,
            train_recon_losses, train_kl_losses,
            val_recon_losses, val_kl_losses,
            is_best=False
        )

    finally:
        # Close progress bars
        if 'total_pbar' in locals():
            total_pbar.close()

        # Plot training history
        plt.figure(figsize=(15, 10))

        # Plot total loss
        plt.subplot(2, 2, 1)
        plt.plot(train_losses, label='Train Loss')
        plt.plot(val_losses, label='Validation Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Total Loss')
        plt.title('Total Loss History')
        plt.legend()
        plt.grid(True)

        # Plot reconstruction loss
        plt.subplot(2, 2, 2)
        plt.plot(train_recon_losses, label='Train Recon Loss')
        plt.plot(val_recon_losses, label='Val Recon Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Reconstruction Loss')
        plt.title('Reconstruction Loss History')
        plt.legend()
        plt.grid(True)

        # Plot KL loss
        plt.subplot(2, 2, 3)
        plt.plot(train_kl_losses, label='Train KL Loss')
        plt.plot(val_kl_losses, label='Val KL Loss')
        plt.xlabel('Epoch')
        plt.ylabel('KL Divergence Loss')
        plt.title('KL Divergence Loss History')
        plt.legend()
        plt.grid(True)

        # Plot recent total loss (last 30 epochs or all if < 30)
        plt.subplot(2, 2, 4)
        recent = min(30, len(train_losses))
        if recent > 5:  # Only plot recent history if we have enough epochs
            plt.plot(train_losses[-recent:], label='Train Loss')
            plt.plot(val_losses[-recent:], label='Validation Loss')
            plt.xlabel('Epoch')
            plt.ylabel('Total Loss')
            plt.title(f'Last {recent} Epochs')
            plt.legend()
            plt.grid(True)

        plt.tight_layout()
        plt.savefig(os.path.join(config.checkpoint_dir, f"{config.model_name}_training_history.png"))
        plt.show()

        # Print final training summary
        total_time = time.time() - start_time
        print(f"\nTraining completed in {total_time/60:.2f} minutes")
        print(f"Best validation loss: {best_val_loss:.6f}")

        return {
            'train_losses': train_losses,
            'val_losses': val_losses,
            'train_recon_losses': train_recon_losses,
            'train_kl_losses': train_kl_losses,
            'val_recon_losses': val_recon_losses,
            'val_kl_losses': val_kl_losses,
            'best_val_loss': best_val_loss,
            'model': model
        }

"""### Training"""

# Example usage to train the VAE
if __name__ == "__main__":
    # Create the VAE model
    model = VAE(latent_dim=256)

    # Create VAE configuration
    config = VAEConfig(
        latent_dim=256,
        batch_size=8,
        accumulation_steps=8,        # Effective batch size = 64
        learning_rate=2e-5,
        epochs=300,
        beta=0.0005,                 # Reduced from 0.005 to 0.0005 (10x smaller)
        beta_warmup_steps=10000,      # Increased from 2000 to 10000
        early_stopping_patience=20,
        use_mixed_precision=True,
        num_workers=4,
        model_name="vae_model_v2"    # New model name to avoid overwriting
    )

    # Train the VAE
    results = train_vae(model, train_loader, val_loader, config)

# Extra test - Vae v5
if __name__ == "__main__":
    # Create the VAE model
    model = VAE(latent_dim=256)

    # Create VAE configuration
    config = VAEConfig(
        latent_dim=256,
        batch_size=8,
        accumulation_steps=8,        # Effective batch size = 64
        learning_rate=2e-5,
        epochs=300,
        beta=0.0005,                 # Reduced from 0.005 to 0.0005 (10x smaller)
        beta_warmup_steps=10000,      # Increased from 2000 to 10000
        early_stopping_patience=20,
        use_mixed_precision=True,
        num_workers=4,
        model_name="vae_model_v5"    # New model name to avoid overwriting
    )

    # Train the VAE
    results = train_vae(model, train_loader, val_loader, config)

"""### Evaluation and Result Visualization"""

# Cell 18: VAE Evaluation and Visualization
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path
import json
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import seaborn as sns
from tqdm import tqdm
import gc

def load_trained_vae(checkpoint_dir, model_name='vae_model_v2', latent_dim=256):
    """Load trained VAE model for evaluation with robust error handling"""
    model_path = Path(checkpoint_dir) / f"{model_name}_best.pth"
    checkpoint_path = Path(checkpoint_dir) / f"{model_name}_checkpoint.pth"
    metadata_path = Path(checkpoint_dir) / f"{model_name}_metadata.json"

    # Set device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # Create VAE model
    model = VAE(latent_dim=latent_dim)

    # Load model weights with error handling
    try:
        if model_path.exists():
            print(f"Loading best VAE model from {model_path}")
            model.load_state_dict(torch.load(model_path, map_location=device))
        elif checkpoint_path.exists():
            print(f"Best model not found. Loading from checkpoint at {checkpoint_path}")
            checkpoint = torch.load(checkpoint_path, map_location=device)
            model.load_state_dict(checkpoint['model_state_dict'])
            print(f"Loaded VAE checkpoint from epoch {checkpoint.get('epoch', 'unknown')}")
        else:
            raise FileNotFoundError(f"No VAE model found at {model_path} or {checkpoint_path}")
    except Exception as e:
        print(f"Error loading VAE model: {str(e)}")
        print("Available files in directory:")
        for file in Path(checkpoint_dir).glob("*"):
            print(f" - {file.name}")
        raise

    # Load training history
    if metadata_path.exists():
        with open(metadata_path, 'r') as f:
            metadata = json.load(f)
        print(f"Loaded training history with {len(metadata.get('train_losses', []))} epochs")
    else:
        metadata = {"train_losses": [], "val_losses": [],
                   "train_recon_losses": [], "train_kl_losses": [],
                   "val_recon_losses": [], "val_kl_losses": []}
        print("No metadata found, using empty history")

    # Move model to device and set to evaluation mode
    model.eval()
    model.to(device)

    return model, metadata

def plot_vae_training_history(metadata):
    """Plot full VAE training history including component losses"""
    plt.figure(figsize=(15, 10))

    # Extract loss data from metadata
    train_losses = metadata.get("train_losses", [])
    val_losses = metadata.get("val_losses", [])
    train_recon_losses = metadata.get("train_recon_losses", [])
    train_kl_losses = metadata.get("train_kl_losses", [])
    val_recon_losses = metadata.get("val_recon_losses", [])
    val_kl_losses = metadata.get("val_kl_losses", [])

    # 1. Plot total loss (train & validation)
    plt.subplot(2, 2, 1)
    plt.plot(train_losses, label='Train Loss')
    plt.plot(val_losses, label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Total Loss')
    plt.title('Total Loss History')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # 2. Plot reconstruction loss (train & validation)
    plt.subplot(2, 2, 2)
    plt.plot(train_recon_losses, label='Train Recon Loss')
    plt.plot(val_recon_losses, label='Val Recon Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Reconstruction Loss')
    plt.title('Reconstruction Loss History')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # 3. Plot KL loss (train & validation)
    plt.subplot(2, 2, 3)
    plt.plot(train_kl_losses, label='Train KL Loss')
    plt.plot(val_kl_losses, label='Val KL Loss')
    plt.xlabel('Epoch')
    plt.ylabel('KL Divergence Loss')
    plt.title('KL Divergence Loss History')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # 4. Plot recent total loss (last 30 epochs or all if < 30)
    plt.subplot(2, 2, 4)
    recent = min(30, len(train_losses))
    if recent > 5:  # Only plot recent history if we have enough epochs
        plt.plot(train_losses[-recent:], label='Train Loss')
        plt.plot(val_losses[-recent:], label='Validation Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Total Loss')
        plt.title(f'Last {recent} Epochs')
        plt.legend()
        plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

def visualize_vae_reconstructions(model, dataloader, num_samples=3):
    """Visualize original vs reconstructed volumes from VAE with anatomically relevant slices"""
    device = next(model.parameters()).device

    # Get samples from dataloader
    samples = []
    labels = []

    for batch in dataloader:
        volumes = batch['volume']
        batch_labels = batch['label']

        for i in range(min(len(volumes), num_samples - len(samples))):
            samples.append(volumes[i:i+1])
            labels.append(batch_labels[i])

        if len(samples) >= num_samples:
            break

    # Visualize each sample
    with torch.no_grad():
        for idx, (sample, label) in enumerate(zip(samples, labels)):
            # Get original volume
            orig_vol = sample.to(device)

            # Generate reconstruction
            reconstructed, mu, log_var = model(orig_vol)

            # Move to CPU for visualization
            orig_vol_np = orig_vol.cpu().squeeze().numpy()
            recon_vol_np = reconstructed.cpu().squeeze().numpy()

            # Create figure for this sample
            fig = plt.figure(figsize=(16, 12))
            plt.suptitle(f"VAE Sample {idx+1} - Group: {label}", fontsize=16)

            # Define anatomically relevant slices
            axial_slice = 32      # Axial view - slice 32
            coronal_slice = 50    # Coronal view - slice 50
            sagittal_slice1 = 55  # Sagittal view - slice 55
            sagittal_slice2 = 70  # Sagittal view - slice 70

            # Plot original slices - top row
            plt.subplot(2, 4, 1)
            plt.imshow(orig_vol_np[axial_slice], cmap='gray', vmin=0, vmax=3)
            plt.title(f"Original - Axial (z={axial_slice})")
            plt.axis('off')

            plt.subplot(2, 4, 2)
            plt.imshow(orig_vol_np[:, coronal_slice, :], cmap='gray', vmin=0, vmax=3)
            plt.title(f"Original - Coronal (y={coronal_slice})")
            plt.axis('off')

            plt.subplot(2, 4, 3)
            plt.imshow(orig_vol_np[:, :, sagittal_slice1], cmap='gray', vmin=0, vmax=3)
            plt.title(f"Original - Sagittal (x={sagittal_slice1})")
            plt.axis('off')

            plt.subplot(2, 4, 4)
            plt.imshow(orig_vol_np[:, :, sagittal_slice2], cmap='gray', vmin=0, vmax=3)
            plt.title(f"Original - Sagittal (x={sagittal_slice2})")
            plt.axis('off')

            # Plot reconstructed slices - bottom row
            plt.subplot(2, 4, 5)
            plt.imshow(recon_vol_np[axial_slice], cmap='gray', vmin=0, vmax=3)
            plt.title(f"Reconstructed - Axial (z={axial_slice})")
            plt.axis('off')

            plt.subplot(2, 4, 6)
            plt.imshow(recon_vol_np[:, coronal_slice, :], cmap='gray', vmin=0, vmax=3)
            plt.title(f"Reconstructed - Coronal (y={coronal_slice})")
            plt.axis('off')

            plt.subplot(2, 4, 7)
            plt.imshow(recon_vol_np[:, :, sagittal_slice1], cmap='gray', vmin=0, vmax=3)
            plt.title(f"Reconstructed - Sagittal (x={sagittal_slice1})")
            plt.axis('off')

            plt.subplot(2, 4, 8)
            plt.imshow(recon_vol_np[:, :, sagittal_slice2], cmap='gray', vmin=0, vmax=3)
            plt.title(f"Reconstructed - Sagittal (x={sagittal_slice2})")
            plt.axis('off')

            plt.tight_layout()
            plt.show()

def extract_vae_latent_vectors(model, dataloader, max_samples=200):
    """Extract latent vectors from VAE encoder"""
    device = next(model.parameters()).device

    latent_means = []
    latent_logvars = []
    labels = []
    paths = []

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Extracting VAE latent vectors"):
            try:
                volumes = batch['volume'].to(device)
                batch_labels = batch['label']
                batch_paths = batch['path']

                # Extract latent vectors (both mean and log_var)
                mu, log_var = model.encode(volumes)

                # Store results
                latent_means.append(mu.cpu().numpy())
                latent_logvars.append(log_var.cpu().numpy())
                labels.extend(batch_labels)
                paths.extend(batch_paths)

                # Memory cleanup
                del volumes, mu, log_var
                torch.cuda.empty_cache()

                # Check if we have enough samples
                if max_samples and len(labels) >= max_samples:
                    latent_means = np.vstack(latent_means)
                    latent_logvars = np.vstack(latent_logvars)
                    latent_means = latent_means[:max_samples]
                    latent_logvars = latent_logvars[:max_samples]
                    labels = labels[:max_samples]
                    paths = paths[:max_samples]
                    break

            except Exception as e:
                print(f"Error processing batch: {str(e)}")
                continue

    # Stack all latent vectors if we didn't break early
    if isinstance(latent_means[0], np.ndarray):
        latent_means = np.vstack(latent_means)
        latent_logvars = np.vstack(latent_logvars)

    return latent_means, latent_logvars, labels, paths

def visualize_latent_space(latent_vectors, labels, method='tsne'):
    """Visualize latent space using t-SNE or PCA"""
    plt.figure(figsize=(10, 8))

    # Create label-to-color mapping for consistent colors
    unique_labels = list(set(labels))
    colors = plt.cm.tab10(np.linspace(0, 1, len(unique_labels)))
    label_to_color = {label: colors[i] for i, label in enumerate(unique_labels)}

    # Apply dimensionality reduction
    if method.lower() == 'tsne':
        print("Computing t-SNE projection...")
        reducer = TSNE(n_components=2, random_state=42, perplexity=min(30, len(latent_vectors) - 1))
        title = 't-SNE Visualization of VAE Latent Space'
    else:
        print("Computing PCA projection...")
        reducer = PCA(n_components=2, random_state=42)
        title = 'PCA Visualization of VAE Latent Space'

    # Apply reduction
    reduced_vecs = reducer.fit_transform(latent_vectors)

    # Create scatter plot
    for label in unique_labels:
        # Get indices where this label appears
        indices = [i for i, l in enumerate(labels) if l == label]

        # Plot these points
        plt.scatter(
            reduced_vecs[indices, 0],
            reduced_vecs[indices, 1],
            label=label,
            color=label_to_color[label],
            alpha=0.7,
            edgecolor='w',
            s=100
        )

    plt.title(title, fontsize=14)
    plt.xlabel("Dimension 1", fontsize=12)
    plt.ylabel("Dimension 2", fontsize=12)
    plt.legend(title="Group", fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

    return reduced_vecs

def plot_latent_dimension_activation(latent_vectors, labels):
    """Analyze activation patterns of latent dimensions"""
    # Create a DataFrame with latent dimensions and labels
    import pandas as pd

    # First, convert labels to categorical for better plotting
    unique_labels = list(set(labels))
    label_map = {label: i for i, label in enumerate(unique_labels)}
    label_indices = [label_map[label] for label in labels]

    # Create DataFrames
    latent_df = pd.DataFrame(latent_vectors)
    latent_df['label'] = labels

    # Compute mean activation by group
    mean_activations = {}
    for label in unique_labels:
        group_vectors = latent_vectors[np.array(labels) == label]
        mean_activations[label] = np.mean(group_vectors, axis=0)

    # Identify top discriminative dimensions
    activation_matrix = np.vstack([mean_activations[label] for label in unique_labels])
    variance = np.var(activation_matrix, axis=0)
    top_dims = np.argsort(variance)[-10:]  # Top 10 dimensions

    # Plot mean activation for top dimensions
    plt.figure(figsize=(14, 6))

    # Plot heatmap
    plt.subplot(1, 2, 1)
    heatmap_data = pd.DataFrame({
        f"Dim {i}": [mean_activations[label][i] for label in unique_labels]
        for i in top_dims
    })
    heatmap_data.index = unique_labels

    sns.heatmap(heatmap_data, cmap='coolwarm', center=0,
               annot=True, fmt=".2f", cbar_kws={'label': 'Mean Activation'})
    plt.title("Mean Activation of Top Discriminative Dimensions")

    # Plot box plots for top 5 dimensions
    plt.subplot(1, 2, 2)

    # Create data for boxplot
    plot_data = []
    labels_for_plot = []
    positions = []

    for i, dim in enumerate(top_dims[:5]):  # Top 5 for clarity
        for j, label in enumerate(unique_labels):
            group_values = latent_vectors[np.array(labels) == label, dim]
            plot_data.append(group_values)
            labels_for_plot.append(f"{label}")
            positions.append(i + j * 0.25)

    # Create boxplot
    boxplot = plt.boxplot(plot_data, positions=positions, patch_artist=True, widths=0.15)

    # Customize boxplot colors
    colors = plt.cm.tab10(np.linspace(0, 1, len(unique_labels)))
    for i, label in enumerate(unique_labels):
        indices = [j for j, l in enumerate(labels_for_plot) if l == label]
        for idx in indices:
            boxplot['boxes'][idx].set_facecolor(colors[i])

    # Add labels and ticks
    plt.xticks([i + (len(unique_labels) - 1) * 0.125 for i in range(5)],
              [f"Dim {d}" for d in top_dims[:5]])
    plt.title("Distribution of Top 5 Discriminative Dimensions")
    plt.ylabel("Activation Value")

    # Add legend
    for i, label in enumerate(unique_labels):
        plt.plot([], [], 'o', color=colors[i], label=label)
    plt.legend(title="Group")

    plt.tight_layout()
    plt.show()

    return top_dims

def compute_reconstruction_error(model, dataloader):
    """Compute detailed reconstruction error metrics on validation set"""
    device = next(model.parameters()).device
    criterion = nn.MSELoss(reduction='none')

    total_mse = 0
    total_samples = 0
    error_by_label = {}
    kl_by_label = {}

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Computing metrics"):
            volumes = batch['volume'].to(device)
            labels = batch['label']

            # Get reconstructions and latent variables
            reconstructed, mu, log_var = model(volumes)

            # Compute MSE loss per sample
            mse = criterion(reconstructed, volumes)

            # Average over dimensions
            mse = mse.mean(dim=(1, 2, 3, 4)).cpu().numpy()

            # Compute KL divergence
            kl_div = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp(), dim=1).cpu().numpy()

            # Track overall error
            total_mse += mse.sum()
            total_samples += volumes.shape[0]

            # Track error by label
            for i, label in enumerate(labels):
                if label not in error_by_label:
                    error_by_label[label] = []
                    kl_by_label[label] = []
                error_by_label[label].append(mse[i])
                kl_by_label[label].append(kl_div[i])

    # Calculate overall metrics
    avg_mse = total_mse / total_samples
    rmse = np.sqrt(avg_mse)

    print("\nReconstruction Error Metrics:")
    print(f"Overall MSE: {avg_mse:.6f}")
    print(f"Overall RMSE: {rmse:.6f}")

    # Calculate metrics by group
    print("\nReconstruction Error by Group:")
    for label, errors in error_by_label.items():
        group_mse = np.mean(errors)
        group_rmse = np.sqrt(group_mse)
        group_std = np.std(errors)
        group_kl = np.mean(kl_by_label[label])
        print(f"{label}:")
        print(f"  MSE: {group_mse:.6f} ± {group_std:.6f}")
        print(f"  RMSE: {group_rmse:.6f}")
        print(f"  KL Divergence: {group_kl:.6f}")

    # Plot error distribution by group
    plt.figure(figsize=(15, 6))

    # MSE Distribution
    plt.subplot(1, 2, 1)
    for label, errors in error_by_label.items():
        sns.histplot(errors, alpha=0.5, label=label, bins=20, kde=True)
    plt.title("Reconstruction Error Distribution by Group")
    plt.xlabel("Mean Squared Error")
    plt.ylabel("Count")
    plt.legend()
    plt.grid(True, alpha=0.3)

    # KL Divergence Distribution
    plt.subplot(1, 2, 2)
    for label, kl_values in kl_by_label.items():
        sns.histplot(kl_values, alpha=0.5, label=label, bins=20, kde=True)
    plt.title("KL Divergence Distribution by Group")
    plt.xlabel("KL Divergence")
    plt.ylabel("Count")
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    return avg_mse, error_by_label, kl_by_label

def find_vae_outliers(model, dataloader, threshold_std=2.5):
    """Identify outliers based on reconstruction error"""
    device = next(model.parameters()).device
    criterion = nn.MSELoss(reduction='none')

    all_errors = []
    all_paths = []
    all_labels = []
    all_kl_divs = []

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Finding outliers"):
            volumes = batch['volume'].to(device)
            paths = batch['path']
            labels = batch['label']

            # Get reconstructions and latent variables
            reconstructed, mu, log_var = model(volumes)

            # Compute MSE loss per sample
            mse = criterion(reconstructed, volumes)

            # Average over dimensions
            mse = mse.mean(dim=(1, 2, 3, 4)).cpu().numpy()

            # Compute KL divergence
            kl_div = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp(), dim=1).cpu().numpy()

            # Store results
            all_errors.extend(mse)
            all_paths.extend(paths)
            all_labels.extend(labels)
            all_kl_divs.extend(kl_div)

    # Convert to numpy arrays
    all_errors = np.array(all_errors)
    all_kl_divs = np.array(all_kl_divs)

    # Compute statistics
    mean_error = np.mean(all_errors)
    std_error = np.std(all_errors)

    # Find outliers (samples with error > mean + threshold_std * std)
    threshold = mean_error + threshold_std * std_error
    outlier_indices = np.where(all_errors > threshold)[0]

    print(f"\nOutlier Analysis:")
    print(f"Mean error: {mean_error:.6f}")
    print(f"Error standard deviation: {std_error:.6f}")
    print(f"Outlier threshold: {threshold:.6f}")
    print(f"Found {len(outlier_indices)} outliers out of {len(all_errors)} samples ({len(outlier_indices)/len(all_errors)*100:.2f}%)")

    # Create dictionary of outliers
    outliers = {
        all_paths[i]: {
            'error': all_errors[i],
            'label': all_labels[i],
            'kl_div': all_kl_divs[i],
            'z_score': (all_errors[i] - mean_error) / std_error
        }
        for i in outlier_indices
    }

    # Plot error distribution with outlier threshold
    plt.figure(figsize=(15, 6))

    # Plot MSE distribution
    plt.subplot(1, 2, 1)
    plt.hist(all_errors, bins=30, alpha=0.7)
    plt.axvline(threshold, color='r', linestyle='--', label=f'Threshold ({threshold_std} std)')
    plt.title("Reconstruction Error Distribution")
    plt.xlabel("Mean Squared Error")
    plt.ylabel("Count")
    plt.legend()
    plt.grid(True, alpha=0.3)

    # Plot error vs KL divergence, highlighting outliers
    plt.subplot(1, 2, 2)
    plt.scatter(all_errors, all_kl_divs, alpha=0.5, s=20, label="Normal samples")

    # Highlight outliers
    outlier_errors = [all_errors[i] for i in outlier_indices]
    outlier_kl_divs = [all_kl_divs[i] for i in outlier_indices]
    plt.scatter(outlier_errors, outlier_kl_divs, color='r', s=50, label="Outliers")

    plt.axvline(threshold, color='r', linestyle='--')
    plt.title("Error vs KL Divergence")
    plt.xlabel("Reconstruction Error (MSE)")
    plt.ylabel("KL Divergence")
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    return outliers, all_errors, all_paths, all_labels

def visualize_vae_outliers(model, outliers, top_n=5):
    """Visualize the top outliers with highest reconstruction error"""
    device = next(model.parameters()).device

    # Sort outliers by error (descending)
    sorted_outliers = sorted(outliers.items(), key=lambda x: x[1]['error'], reverse=True)

    # Visualize top outliers
    num_outliers = min(top_n, len(sorted_outliers))

    for i in range(num_outliers):
        path, info = sorted_outliers[i]
        error = info['error']
        label = info['label']
        z_score = info['z_score']
        kl_div = info['kl_div']

        try:
            # Load the original volume
            with torch.no_grad():
                # Load DICOM file
                original_volume, _ = load_dicom(path)
                original_volume = original_volume[9:73, :, :]

                # Process volume
                norm_vol, _, _ = process_volume(original_volume, target_shape=(64, 128, 128))

                # Convert to tensor and add batch dimension
                vol_tensor = torch.from_numpy(np.expand_dims(norm_vol, axis=(0, 1))).float().to(device)

                # Get reconstruction
                reconstructed, mu, log_var = model(vol_tensor)

                # Move tensors to CPU and remove batch and channel dimensions
                vol_np = vol_tensor.cpu().squeeze().numpy()
                recon_np = reconstructed.cpu().squeeze().numpy()

                # Create figure
                fig = plt.figure(figsize=(16, 12))
                plt.suptitle(f"Outlier {i+1}: {path.split('/')[-1]}\nGroup: {label}, Error: {error:.6f}, Z-score: {z_score:.2f}, KL Div: {kl_div:.2f}", fontsize=14)

                # Define anatomically relevant slices
                axial_slice = 32      # Axial view - slice 32
                coronal_slice = 50    # Coronal view - slice 50
                sagittal_slice1 = 55  # Sagittal view - slice 55
                sagittal_slice2 = 70  # Sagittal view - slice 70

                # Plot original slices - top row
                plt.subplot(2, 4, 1)
                plt.imshow(vol_np[axial_slice], cmap='gray', vmin=0, vmax=3)
                plt.title(f"Original - Axial (z={axial_slice})")
                plt.axis('off')

                plt.subplot(2, 4, 2)
                plt.imshow(vol_np[:, coronal_slice, :], cmap='gray', vmin=0, vmax=3)
                plt.title(f"Original - Coronal (y={coronal_slice})")
                plt.axis('off')

                plt.subplot(2, 4, 3)
                plt.imshow(vol_np[:, :, sagittal_slice1], cmap='gray', vmin=0, vmax=3)
                plt.title(f"Original - Sagittal (x={sagittal_slice1})")
                plt.axis('off')

                plt.subplot(2, 4, 4)
                plt.imshow(vol_np[:, :, sagittal_slice2], cmap='gray', vmin=0, vmax=3)
                plt.title(f"Original - Sagittal (x={sagittal_slice2})")
                plt.axis('off')

                # Plot reconstructed slices - bottom row
                plt.subplot(2, 4, 5)
                plt.imshow(recon_np[axial_slice], cmap='gray', vmin=0, vmax=3)
                plt.title(f"Reconstructed - Axial (z={axial_slice})")
                plt.axis('off')

                plt.subplot(2, 4, 6)
                plt.imshow(recon_np[:, coronal_slice, :], cmap='gray', vmin=0, vmax=3)
                plt.title(f"Reconstructed - Coronal (y={coronal_slice})")
                plt.axis('off')

                plt.subplot(2, 4, 7)
                plt.imshow(recon_np[:, :, sagittal_slice1], cmap='gray', vmin=0, vmax=3)
                plt.title(f"Reconstructed - Sagittal (x={sagittal_slice1})")
                plt.axis('off')

                plt.subplot(2, 4, 8)
                plt.imshow(recon_np[:, :, sagittal_slice2], cmap='gray', vmin=0, vmax=3)
                plt.title(f"Reconstructed - Sagittal (x={sagittal_slice2})")
                plt.axis('off')

                plt.tight_layout()
                plt.show()

        except Exception as e:
            print(f"Error visualizing outlier {path}: {e}")

def visualize_vae_latent_dimension(model, dataloader, dimension_idx, alpha=5.0, group=None):
    """
    Visualize what a specific VAE latent dimension represents by modifying it
    and observing the effect on brain reconstruction.

    Parameters:
        model: Trained VAE model
        dataloader: DataLoader containing samples
        dimension_idx: The latent dimension to manipulate (e.g., 63)
        alpha: Strength of the dimension manipulation
        group: Optional filter for specific patient group (e.g., 'PD', 'Control')
    """
    device = next(model.parameters()).device
    model.eval()

    # Find a suitable sample (optionally from specific group)
    for batch in dataloader:
        volumes = batch['volume']
        labels = batch['label']
        paths = batch['path']

        if group is not None:
            # Find samples from the specified group
            group_indices = [i for i, label in enumerate(labels) if label == group]
            if not group_indices:
                continue
            # Use the first matching sample
            idx = group_indices[0]
            sample = volumes[idx:idx+1].to(device)
            sample_label = labels[idx]
            sample_path = paths[idx]
        else:
            # Just use the first sample
            sample = volumes[0:1].to(device)
            sample_label = labels[0]
            sample_path = paths[0]

        break  # Exit after finding a sample

    with torch.no_grad():
        # Encode the sample to get its latent representation
        mu, log_var = model.encode(sample)
        z = model.reparameterize(mu, log_var)

        # Create modified latent vectors
        z_plus = z.clone()
        z_minus = z.clone()

        # Modify the specific dimension
        z_plus[0, dimension_idx] += alpha
        z_minus[0, dimension_idx] -= alpha

        # Decode the original and modified latent vectors
        original_reconstruction = model.decoder(z)
        plus_reconstruction = model.decoder(z_plus)
        minus_reconstruction = model.decoder(z_minus)

        # Move tensors to CPU and convert to numpy for visualization
        original_vol = original_reconstruction.cpu().squeeze().numpy()
        plus_vol = plus_reconstruction.cpu().squeeze().numpy()
        minus_vol = minus_reconstruction.cpu().squeeze().numpy()

        # Calculate the difference maps
        plus_diff = plus_vol - original_vol
        minus_diff = minus_vol - original_vol

        # Set up the figure
        fig = plt.figure(figsize=(18, 15))
        plt.suptitle(f"Visualization of VAE Dimension {dimension_idx} in Brain Space\nPatient Group: {sample_label}", fontsize=16)

        # Define anatomically relevant slices
        slices = {
            'axial': 32,       # Axial z=32
            'coronal': 50,     # Coronal y=50
            'sagittal1': 55,   # Sagittal x=55
            'sagittal2': 70    # Sagittal x=70
        }

        # Create a custom colormap for difference maps
        diff_cmap = plt.cm.RdBu_r  # Red-Blue colormap with red for negative, blue for positive

        # Row 1: Original reconstruction
        plt.subplot(5, 4, 1)
        plt.imshow(original_vol[slices['axial']], cmap='gray', vmin=0, vmax=3)
        plt.title(f"Original (Axial z={slices['axial']})")
        plt.axis('off')

        plt.subplot(5, 4, 2)
        plt.imshow(original_vol[:, slices['coronal'], :], cmap='gray', vmin=0, vmax=3)
        plt.title(f"Original (Coronal y={slices['coronal']})")
        plt.axis('off')

        plt.subplot(5, 4, 3)
        plt.imshow(original_vol[:, :, slices['sagittal1']], cmap='gray', vmin=0, vmax=3)
        plt.title(f"Original (Sagittal x={slices['sagittal1']})")
        plt.axis('off')

        plt.subplot(5, 4, 4)
        plt.imshow(original_vol[:, :, slices['sagittal2']], cmap='gray', vmin=0, vmax=3)
        plt.title(f"Original (Sagittal x={slices['sagittal2']})")
        plt.axis('off')

        # Row 2: Increased dimension
        plt.subplot(5, 4, 5)
        plt.imshow(plus_vol[slices['axial']], cmap='gray', vmin=0, vmax=3)
        plt.title(f"Dim {dimension_idx} + {alpha}")
        plt.axis('off')

        plt.subplot(5, 4, 6)
        plt.imshow(plus_vol[:, slices['coronal'], :], cmap='gray', vmin=0, vmax=3)
        plt.title(f"Dim {dimension_idx} + {alpha}")
        plt.axis('off')

        plt.subplot(5, 4, 7)
        plt.imshow(plus_vol[:, :, slices['sagittal1']], cmap='gray', vmin=0, vmax=3)
        plt.title(f"Dim {dimension_idx} + {alpha}")
        plt.axis('off')

        plt.subplot(5, 4, 8)
        plt.imshow(plus_vol[:, :, slices['sagittal2']], cmap='gray', vmin=0, vmax=3)
        plt.title(f"Dim {dimension_idx} + {alpha}")
        plt.axis('off')

        # Row 3: Decreased dimension
        plt.subplot(5, 4, 9)
        plt.imshow(minus_vol[slices['axial']], cmap='gray', vmin=0, vmax=3)
        plt.title(f"Dim {dimension_idx} - {alpha}")
        plt.axis('off')

        plt.subplot(5, 4, 10)
        plt.imshow(minus_vol[:, slices['coronal'], :], cmap='gray', vmin=0, vmax=3)
        plt.title(f"Dim {dimension_idx} - {alpha}")
        plt.axis('off')

        plt.subplot(5, 4, 11)
        plt.imshow(minus_vol[:, :, slices['sagittal1']], cmap='gray', vmin=0, vmax=3)
        plt.title(f"Dim {dimension_idx} - {alpha}")
        plt.axis('off')

        plt.subplot(5, 4, 12)
        plt.imshow(minus_vol[:, :, slices['sagittal2']], cmap='gray', vmin=0, vmax=3)
        plt.title(f"Dim {dimension_idx} - {alpha}")
        plt.axis('off')

        # Row 4: Difference map (increased - original)
        plt.subplot(5, 4, 13)
        im1 = plt.imshow(plus_diff[slices['axial']], cmap=diff_cmap, vmin=-0.5, vmax=0.5)
        plt.title(f"Difference (+)")
        plt.axis('off')

        plt.subplot(5, 4, 14)
        plt.imshow(plus_diff[:, slices['coronal'], :], cmap=diff_cmap, vmin=-0.5, vmax=0.5)
        plt.title(f"Difference (+)")
        plt.axis('off')

        plt.subplot(5, 4, 15)
        plt.imshow(plus_diff[:, :, slices['sagittal1']], cmap=diff_cmap, vmin=-0.5, vmax=0.5)
        plt.title(f"Difference (+)")
        plt.axis('off')

        plt.subplot(5, 4, 16)
        plt.imshow(plus_diff[:, :, slices['sagittal2']], cmap=diff_cmap, vmin=-0.5, vmax=0.5)
        plt.title(f"Difference (+)")
        plt.axis('off')

        # Row 5: Difference map (decreased - original)
        plt.subplot(5, 4, 17)
        im2 = plt.imshow(minus_diff[slices['axial']], cmap=diff_cmap, vmin=-0.5, vmax=0.5)
        plt.title(f"Difference (-)")
        plt.axis('off')

        plt.subplot(5, 4, 18)
        plt.imshow(minus_diff[:, slices['coronal'], :], cmap=diff_cmap, vmin=-0.5, vmax=0.5)
        plt.title(f"Difference (-)")
        plt.axis('off')

        plt.subplot(5, 4, 19)
        plt.imshow(minus_diff[:, :, slices['sagittal1']], cmap=diff_cmap, vmin=-0.5, vmax=0.5)
        plt.title(f"Difference (-)")
        plt.axis('off')

        plt.subplot(5, 4, 20)
        plt.imshow(minus_diff[:, :, slices['sagittal2']], cmap=diff_cmap, vmin=-0.5, vmax=0.5)
        plt.title(f"Difference (-)")
        plt.axis('off')

        # Add colorbar for difference maps
        cbar_ax = fig.add_axes([0.93, 0.15, 0.02, 0.3])
        cbar = plt.colorbar(im1, cax=cbar_ax)
        cbar.set_label('Difference Intensity')

        plt.tight_layout()
        plt.subplots_adjust(top=0.92, right=0.9)
        plt.show()

        # Return both the sample info and reconstructions for further analysis
        return {
            'label': sample_label,
            'path': sample_path,
            'original': original_vol,
            'plus': plus_vol,
            'minus': minus_vol,
            'plus_diff': plus_diff,
            'minus_diff': minus_diff
        }

# Run VAE evaluation
if __name__ == "__main__":
    try:
        print("=== Starting VAE Evaluation ===")

        # Load trained VAE model
        print("\nLoading trained VAE model...")
        vae_model, vae_metadata = load_trained_vae('checkpoints', 'vae_model_v2', latent_dim=256)

        # 1. Plot training history
        print("\nPlotting VAE training history...")
        plot_vae_training_history(vae_metadata)

        # 2. Visualize reconstructions
        print("\nVisualizing VAE reconstructions...")
        visualize_vae_reconstructions(vae_model, val_loader, num_samples=3)

        # 3. Extract latent vectors from VAE
        print("\nExtracting latent vectors from VAE...")
        latent_means, latent_logvars, labels, paths = extract_vae_latent_vectors(vae_model, val_loader, max_samples=200)

        # 4. Visualize latent space with t-SNE and PCA
        print("\nVisualizing latent space with t-SNE...")
        tsne_result = visualize_latent_space(latent_means, labels, method='tsne')

        print("\nVisualizing latent space with PCA...")
        pca_result = visualize_latent_space(latent_means, labels, method='pca')

        # 5. Analyze discriminative dimensions
        print("\nAnalyzing discriminative dimensions in latent space...")
        top_dims = plot_latent_dimension_activation(latent_means, labels)

        # 6. Compute and visualize reconstruction error
        print("\nComputing reconstruction error metrics...")
        avg_mse, error_by_label, kl_by_label = compute_reconstruction_error(vae_model, val_loader)

        # 7. Find and visualize outliers
        print("\nFinding and visualizing outliers...")
        outliers, all_errors, all_paths, all_labels = find_vae_outliers(vae_model, val_loader, threshold_std=2.5)
        visualize_vae_outliers(vae_model, outliers)

        # 8. Visualize latent dimensions in brain space
        print("\nVisualizing discriminative dimensions in brain space...")
        for dim in [63, 34, 123]:  # Specified dimensions
            visualize_vae_latent_dimension(vae_model, val_loader, dimension_idx=dim, alpha=5.0)

        print("\nVAE Evaluation completed successfully!")

    except Exception as e:
        print(f"Error during VAE evaluation: {str(e)}")
        import traceback
        traceback.print_exc()

# Cell 19: VAE Feature Importance Maps and Uncertainty Visualization
import torch
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm
import gc
from matplotlib.colors import LinearSegmentedColormap

def generate_vae_feature_importance_map(model, dataloader, dimension_idx, group=None, num_samples=5):
    """
    Generate feature importance map for a specific VAE latent dimension
    by aggregating effects across multiple samples using anatomically relevant slices.

    Parameters:
        model: Trained VAE model
        dataloader: DataLoader containing samples
        dimension_idx: The latent dimension to analyze
        group: Optional filter for specific patient group
        num_samples: Number of samples to aggregate
    """
    device = next(model.parameters()).device
    model.eval()

    # Storage for aggregated results
    aggregated_plus_diff = None
    aggregated_minus_diff = None
    sample_count = 0

    # Storage for encoding statistics
    mu_values = []
    logvar_values = []

    # Find samples (optionally from specific group)
    for batch in dataloader:
        volumes = batch['volume']
        labels = batch['label']

        if group is not None:
            # Find samples from the specified group
            group_indices = [i for i, label in enumerate(labels) if label == group]
            indices = group_indices
        else:
            # Use all samples in batch
            indices = range(len(volumes))

        for idx in indices:
            if sample_count >= num_samples:
                break

            sample = volumes[idx:idx+1].to(device)

            with torch.no_grad():
                # Encode the sample to get mean and log_var
                mu, log_var = model.encode(sample)

                # Store these for uncertainty analysis
                mu_values.append(mu[0, dimension_idx].cpu().item())
                logvar_values.append(log_var[0, dimension_idx].cpu().item())

                # Use mean as the latent vector (no sampling in evaluation)
                z = mu.clone()

                # Create modified latent vectors
                z_plus = z.clone()
                z_minus = z.clone()

                # Modify the specific dimension
                z_plus[0, dimension_idx] += 5.0
                z_minus[0, dimension_idx] -= 5.0

                # Decode the vectors
                original_reconstruction = model.decoder(z)
                plus_reconstruction = model.decoder(z_plus)
                minus_reconstruction = model.decoder(z_minus)

                # Calculate the difference maps
                plus_diff = (plus_reconstruction - original_reconstruction).cpu().squeeze().numpy()
                minus_diff = (minus_reconstruction - original_reconstruction).cpu().squeeze().numpy()

                # Aggregate the difference maps
                if aggregated_plus_diff is None:
                    aggregated_plus_diff = plus_diff
                    aggregated_minus_diff = minus_diff
                else:
                    aggregated_plus_diff += plus_diff
                    aggregated_minus_diff += minus_diff

                sample_count += 1

        if sample_count >= num_samples:
            break

    # Average the difference maps
    aggregated_plus_diff /= sample_count
    aggregated_minus_diff /= sample_count

    # Compute absolute importance map (average of plus and minus effects)
    importance_map = (np.abs(aggregated_plus_diff) + np.abs(aggregated_minus_diff)) / 2

    # Define anatomically relevant slices
    axial_slice = 32      # Axial view - slice 32
    coronal_slice = 50    # Coronal view - slice 50
    sagittal_slice1 = 55  # Sagittal view - slice 55
    sagittal_slice2 = 70  # Sagittal view - slice 70

    # Visualize the importance map
    fig = plt.figure(figsize=(16, 8))
    plt.suptitle(f"VAE Feature Importance Map for Dimension {dimension_idx}" +
                (f" (Group: {group})" if group else ""), fontsize=16)

    # Plot axial, coronal, and sagittal views
    plt.subplot(2, 4, 1)
    plt.imshow(importance_map[axial_slice], cmap='hot')
    plt.title(f"Axial (z={axial_slice})")
    plt.colorbar(fraction=0.046, pad=0.04)
    plt.axis('off')

    plt.subplot(2, 4, 2)
    plt.imshow(importance_map[:, coronal_slice, :], cmap='hot')
    plt.title(f"Coronal (y={coronal_slice})")
    plt.colorbar(fraction=0.046, pad=0.04)
    plt.axis('off')

    plt.subplot(2, 4, 3)
    plt.imshow(importance_map[:, :, sagittal_slice1], cmap='hot')
    plt.title(f"Sagittal (x={sagittal_slice1})")
    plt.colorbar(fraction=0.046, pad=0.04)
    plt.axis('off')

    plt.subplot(2, 4, 4)
    plt.imshow(importance_map[:, :, sagittal_slice2], cmap='hot')
    plt.title(f"Sagittal (x={sagittal_slice2})")
    plt.colorbar(fraction=0.046, pad=0.04)
    plt.axis('off')

    # Plot activating direction (positive difference)
    plt.subplot(2, 4, 5)
    plt.imshow(aggregated_plus_diff[axial_slice], cmap='bwr', vmin=-0.2, vmax=0.2)
    plt.title(f"Activating Direction - Axial")
    plt.colorbar(fraction=0.046, pad=0.04)
    plt.axis('off')

    plt.subplot(2, 4, 6)
    plt.imshow(aggregated_plus_diff[:, coronal_slice, :], cmap='bwr', vmin=-0.2, vmax=0.2)
    plt.title(f"Activating Direction - Coronal")
    plt.colorbar(fraction=0.046, pad=0.04)
    plt.axis('off')

    plt.subplot(2, 4, 7)
    plt.imshow(aggregated_plus_diff[:, :, sagittal_slice1], cmap='bwr', vmin=-0.2, vmax=0.2)
    plt.title(f"Activating Direction - Sagittal 1")
    plt.colorbar(fraction=0.046, pad=0.04)
    plt.axis('off')

    plt.subplot(2, 4, 8)
    plt.imshow(aggregated_plus_diff[:, :, sagittal_slice2], cmap='bwr', vmin=-0.2, vmax=0.2)
    plt.title(f"Activating Direction - Sagittal 2")
    plt.colorbar(fraction=0.046, pad=0.04)
    plt.axis('off')

    plt.tight_layout()
    plt.subplots_adjust(top=0.9)
    plt.show()

    # Return values for further analysis
    mean_mu = np.mean(mu_values)
    std_mu = np.std(mu_values)
    mean_var = np.exp(np.mean(logvar_values))

    print(f"Dimension {dimension_idx} statistics:")
    print(f"  Mean activation: {mean_mu:.4f} ± {std_mu:.4f}")
    print(f"  Average variance: {mean_var:.4f}")

    return importance_map, aggregated_plus_diff, aggregated_minus_diff

def visualize_vae_uncertainty(model, dataloader, group=None, num_samples=20):
    """
    Visualize uncertainty in VAE's reconstructions by sampling
    multiple times from the latent distribution.

    Parameters:
        model: Trained VAE model
        dataloader: DataLoader containing samples
        group: Optional filter for specific patient group
        num_samples: Number of samples to encode
    """
    device = next(model.parameters()).device
    model.eval()

    # Find a suitable sample from the specified group
    sample_vol = None
    sample_label = None

    for batch in dataloader:
        volumes = batch['volume']
        labels = batch['label']

        if group is not None:
            # Find first sample from specified group
            group_indices = [i for i, label in enumerate(labels) if label == group]
            if group_indices:
                idx = group_indices[0]
                sample_vol = volumes[idx:idx+1].to(device)
                sample_label = labels[idx]
                break
        else:
            # Use the first sample
            sample_vol = volumes[0:1].to(device)
            sample_label = labels[0]
            break

    if sample_vol is None:
        print(f"No samples found for group {group}")
        return

    # Generate multiple reconstructions to visualize uncertainty
    reconstructions = []

    with torch.no_grad():
        # Encode the sample to get mean and log_var
        mu, log_var = model.encode(sample_vol)

        # Generate multiple reconstructions by sampling from latent space
        for _ in range(20):  # Generate 20 different reconstructions
            # Sample from latent distribution
            std = torch.exp(0.5 * log_var)
            eps = torch.randn_like(std)
            z = mu + eps * std

            # Decode sample
            recon = model.decoder(z)
            reconstructions.append(recon.cpu().squeeze().numpy())

    # Define anatomically relevant slices
    axial_slice = 32      # Axial view - slice 32
    coronal_slice = 50    # Coronal view - slice 50
    sagittal_slice1 = 55  # Sagittal view - slice 55
    sagittal_slice2 = 70  # Sagittal view - slice 70

    # Convert sample to numpy for visualization
    sample_np = sample_vol.cpu().squeeze().numpy()

    # Calculate statistics over reconstructions
    recon_mean = np.mean(reconstructions, axis=0)
    recon_std = np.std(reconstructions, axis=0)

    # Visualize uncertainty
    fig = plt.figure(figsize=(16, 12))
    plt.suptitle(f"VAE Reconstruction Uncertainty - Group: {sample_label}", fontsize=16)

    # Row 1: Original sample
    plt.subplot(3, 4, 1)
    plt.imshow(sample_np[axial_slice], cmap='gray', vmin=0, vmax=3)
    plt.title(f"Original - Axial (z={axial_slice})")
    plt.axis('off')

    plt.subplot(3, 4, 2)
    plt.imshow(sample_np[:, coronal_slice, :], cmap='gray', vmin=0, vmax=3)
    plt.title(f"Original - Coronal (y={coronal_slice})")
    plt.axis('off')

    plt.subplot(3, 4, 3)
    plt.imshow(sample_np[:, :, sagittal_slice1], cmap='gray', vmin=0, vmax=3)
    plt.title(f"Original - Sagittal (x={sagittal_slice1})")
    plt.axis('off')

    plt.subplot(3, 4, 4)
    plt.imshow(sample_np[:, :, sagittal_slice2], cmap='gray', vmin=0, vmax=3)
    plt.title(f"Original - Sagittal (x={sagittal_slice2})")
    plt.axis('off')

    # Row 2: Mean reconstruction
    plt.subplot(3, 4, 5)
    plt.imshow(recon_mean[axial_slice], cmap='gray', vmin=0, vmax=3)
    plt.title(f"Mean Recon - Axial")
    plt.axis('off')

    plt.subplot(3, 4, 6)
    plt.imshow(recon_mean[:, coronal_slice, :], cmap='gray', vmin=0, vmax=3)
    plt.title(f"Mean Recon - Coronal")
    plt.axis('off')

    plt.subplot(3, 4, 7)
    plt.imshow(recon_mean[:, :, sagittal_slice1], cmap='gray', vmin=0, vmax=3)
    plt.title(f"Mean Recon - Sagittal 1")
    plt.axis('off')

    plt.subplot(3, 4, 8)
    plt.imshow(recon_mean[:, :, sagittal_slice2], cmap='gray', vmin=0, vmax=3)
    plt.title(f"Mean Recon - Sagittal 2")
    plt.axis('off')

    # Row 3: Standard deviation (uncertainty)
    plt.subplot(3, 4, 9)
    plt.imshow(recon_std[axial_slice], cmap='viridis')
    plt.title(f"Uncertainty - Axial")
    plt.colorbar(fraction=0.046, pad=0.04)
    plt.axis('off')

    plt.subplot(3, 4, 10)
    plt.imshow(recon_std[:, coronal_slice, :], cmap='viridis')
    plt.title(f"Uncertainty - Coronal")
    plt.colorbar(fraction=0.046, pad=0.04)
    plt.axis('off')

    plt.subplot(3, 4, 11)
    plt.imshow(recon_std[:, :, sagittal_slice1], cmap='viridis')
    plt.title(f"Uncertainty - Sagittal 1")
    plt.colorbar(fraction=0.046, pad=0.04)
    plt.axis('off')

    plt.subplot(3, 4, 12)
    plt.imshow(recon_std[:, :, sagittal_slice2], cmap='viridis')
    plt.title(f"Uncertainty - Sagittal 2")
    plt.colorbar(fraction=0.046, pad=0.04)
    plt.axis('off')

    plt.tight_layout()
    plt.subplots_adjust(top=0.9)
    plt.show()

    return recon_mean, recon_std

def generate_full_brain_uncertainty_map(model, dataloader, group=None, num_samples=10):
    """
    Generate uncertainty maps across the entire brain by analyzing the
    variability in reconstructions from multiple samples.

    Parameters:
        model: Trained VAE model
        dataloader: DataLoader containing samples
        group: Optional filter for specific patient group
        num_samples: Number of samples to analyze
    """
    device = next(model.parameters()).device
    model.eval()

    # Store reconstructions and their uncertainties
    all_uncertainty_maps = []
    sample_count = 0

    # Process samples
    for batch in tqdm(dataloader, desc="Generating uncertainty maps"):
        volumes = batch['volume']
        labels = batch['label']

        if group is not None:
            # Filter samples for the specified group
            group_indices = [i for i, label in enumerate(labels) if label == group]
            batch_indices = group_indices
        else:
            # Use all samples in batch
            batch_indices = range(len(volumes))

        for idx in batch_indices:
            if sample_count >= num_samples:
                break

            sample = volumes[idx:idx+1].to(device)

            # Generate multiple reconstructions for this sample
            with torch.no_grad():
                # Encode the sample
                mu, log_var = model.encode(sample)

                # Generate reconstructions by sampling 10 times
                sample_recons = []
                for _ in range(10):
                    # Sample from latent distribution
                    std = torch.exp(0.5 * log_var)
                    eps = torch.randn_like(std)
                    z = mu + eps * std

                    # Decode sample
                    recon = model.decoder(z)
                    sample_recons.append(recon.cpu().squeeze().numpy())

                # Calculate standard deviation across reconstructions
                uncertainty_map = np.std(sample_recons, axis=0)
                all_uncertainty_maps.append(uncertainty_map)

                sample_count += 1

        # Memory cleanup
        gc.collect()
        torch.cuda.empty_cache()

        if sample_count >= num_samples:
            break

    # Average uncertainty across samples
    avg_uncertainty = np.mean(all_uncertainty_maps, axis=0)

    # Define anatomically relevant slices
    axial_slice = 32      # Axial view - slice 32
    coronal_slice = 50    # Coronal view - slice 50
    sagittal_slice1 = 55  # Sagittal view - slice 55
    sagittal_slice2 = 70  # Sagittal view - slice 70

    # Visualize average uncertainty across all samples
    fig = plt.figure(figsize=(16, 8))
    plt.suptitle(f"VAE Full Brain Uncertainty Map" +
                (f" - Group: {group}" if group else ""), fontsize=16)

    # Top row: Uncertainty views with viridis colormap
    plt.subplot(2, 4, 1)
    plt.imshow(avg_uncertainty[axial_slice], cmap='viridis')
    plt.title(f"Uncertainty - Axial (z={axial_slice})")
    plt.colorbar(fraction=0.046, pad=0.04)
    plt.axis('off')

    plt.subplot(2, 4, 2)
    plt.imshow(avg_uncertainty[:, coronal_slice, :], cmap='viridis')
    plt.title(f"Uncertainty - Coronal (y={coronal_slice})")
    plt.colorbar(fraction=0.046, pad=0.04)
    plt.axis('off')

    plt.subplot(2, 4, 3)
    plt.imshow(avg_uncertainty[:, :, sagittal_slice1], cmap='viridis')
    plt.title(f"Uncertainty - Sagittal (x={sagittal_slice1})")
    plt.colorbar(fraction=0.046, pad=0.04)
    plt.axis('off')

    plt.subplot(2, 4, 4)
    plt.imshow(avg_uncertainty[:, :, sagittal_slice2], cmap='viridis')
    plt.title(f"Uncertainty - Sagittal (x={sagittal_slice2})")
    plt.colorbar(fraction=0.046, pad=0.04)
    plt.axis('off')

    # Calculate high uncertainty regions (top 10%)
    threshold = np.percentile(avg_uncertainty, 90)
    high_uncertainty_mask = avg_uncertainty > threshold

    # Bottom row: Thresholded uncertainty highlighting highest uncertainty regions
    plt.subplot(2, 4, 5)
    plt.imshow(avg_uncertainty[axial_slice], cmap='gray')
    plt.imshow(high_uncertainty_mask[axial_slice], cmap='hot', alpha=0.7)
    plt.title(f"High Uncertainty - Axial")
    plt.axis('off')

    plt.subplot(2, 4, 6)
    plt.imshow(avg_uncertainty[:, coronal_slice, :], cmap='gray')
    plt.imshow(high_uncertainty_mask[:, coronal_slice, :], cmap='hot', alpha=0.7)
    plt.title(f"High Uncertainty - Coronal")
    plt.axis('off')

    plt.subplot(2, 4, 7)
    plt.imshow(avg_uncertainty[:, :, sagittal_slice1], cmap='gray')
    plt.imshow(high_uncertainty_mask[:, :, sagittal_slice1], cmap='hot', alpha=0.7)
    plt.title(f"High Uncertainty - Sagittal 1")
    plt.axis('off')

    plt.subplot(2, 4, 8)
    plt.imshow(avg_uncertainty[:, :, sagittal_slice2], cmap='gray')
    plt.imshow(high_uncertainty_mask[:, :, sagittal_slice2], cmap='hot', alpha=0.7)
    plt.title(f"High Uncertainty - Sagittal 2")
    plt.axis('off')

    plt.tight_layout()
    plt.subplots_adjust(top=0.9)
    plt.show()

    return avg_uncertainty, high_uncertainty_mask

def compare_group_uncertainty(model, dataloader, groups=['PD', 'Control', 'SWEDD'], num_samples=5):
    """
    Compare uncertainty patterns between different patient groups.

    Parameters:
        model: Trained VAE model
        dataloader: DataLoader containing samples
        groups: List of groups to compare
        num_samples: Number of samples per group
    """
    uncertainty_maps = {}

    # Generate uncertainty maps for each group
    for group in groups:
        print(f"Generating uncertainty maps for group: {group}")
        avg_uncertainty, _ = generate_full_brain_uncertainty_map(
            model, dataloader, group=group, num_samples=num_samples)
        uncertainty_maps[group] = avg_uncertainty

    # Define anatomically relevant slices
    axial_slice = 32      # Axial view - slice 32
    coronal_slice = 50    # Coronal view - slice 50
    sagittal_slice1 = 55  # Sagittal view - slice 55
    sagittal_slice2 = 70  # Sagittal view - slice 70

    # Visualize uncertainty differences between groups
    fig = plt.figure(figsize=(16, 12))
    plt.suptitle(f"VAE Uncertainty Comparison Between Groups", fontsize=16)

    # Row for each group
    for i, group in enumerate(groups):
        plt.subplot(len(groups) + 1, 4, i*4 + 1)
        plt.imshow(uncertainty_maps[group][axial_slice], cmap='viridis')
        plt.title(f"{group} - Axial")
        plt.colorbar(fraction=0.046, pad=0.04)
        plt.axis('off')

        plt.subplot(len(groups) + 1, 4, i*4 + 2)
        plt.imshow(uncertainty_maps[group][:, coronal_slice, :], cmap='viridis')
        plt.title(f"{group} - Coronal")
        plt.colorbar(fraction=0.046, pad=0.04)
        plt.axis('off')

        plt.subplot(len(groups) + 1, 4, i*4 + 3)
        plt.imshow(uncertainty_maps[group][:, :, sagittal_slice1], cmap='viridis')
        plt.title(f"{group} - Sagittal 1")
        plt.colorbar(fraction=0.046, pad=0.04)
        plt.axis('off')

        plt.subplot(len(groups) + 1, 4, i*4 + 4)
        plt.imshow(uncertainty_maps[group][:, :, sagittal_slice2], cmap='viridis')
        plt.title(f"{group} - Sagittal 2")
        plt.colorbar(fraction=0.046, pad=0.04)
        plt.axis('off')

    # Bottom row: Calculate difference between first two groups (if applicable)
    if len(groups) >= 2:
        diff_map = uncertainty_maps[groups[0]] - uncertainty_maps[groups[1]]
        vmax = max(abs(diff_map.min()), abs(diff_map.max()))
        vmin = -vmax

        plt.subplot(len(groups) + 1, 4, len(groups)*4 + 1)
        plt.imshow(diff_map[axial_slice], cmap='bwr', vmin=vmin, vmax=vmax)
        plt.title(f"Difference ({groups[0]} - {groups[1]}) - Axial")
        plt.colorbar(fraction=0.046, pad=0.04)
        plt.axis('off')

        plt.subplot(len(groups) + 1, 4, len(groups)*4 + 2)
        plt.imshow(diff_map[:, coronal_slice, :], cmap='bwr', vmin=vmin, vmax=vmax)
        plt.title(f"Difference ({groups[0]} - {groups[1]}) - Coronal")
        plt.colorbar(fraction=0.046, pad=0.04)
        plt.axis('off')

        plt.subplot(len(groups) + 1, 4, len(groups)*4 + 3)
        plt.imshow(diff_map[:, :, sagittal_slice1], cmap='bwr', vmin=vmin, vmax=vmax)
        plt.title(f"Difference ({groups[0]} - {groups[1]}) - Sagittal 1")
        plt.colorbar(fraction=0.046, pad=0.04)
        plt.axis('off')

        plt.subplot(len(groups) + 1, 4, len(groups)*4 + 4)
        plt.imshow(diff_map[:, :, sagittal_slice2], cmap='bwr', vmin=vmin, vmax=vmax)
        plt.title(f"Difference ({groups[0]} - {groups[1]}) - Sagittal 2")
        plt.colorbar(fraction=0.046, pad=0.04)
        plt.axis('off')

    plt.tight_layout()
    plt.subplots_adjust(top=0.95)
    plt.show()

    return uncertainty_maps

# Run the VAE feature importance and uncertainty visualization
if __name__ == "__main__":
    try:
        print("=== Starting VAE Feature Importance and Uncertainty Analysis ===")

        # Load trained VAE model (using the load_trained_vae function from Cell 18)
        print("\nLoading trained VAE model...")
        vae_model, _ = load_trained_vae('checkpoints', 'vae_model_v2', latent_dim=256)

        # 1. Generate feature importance maps for specified dimensions
        print("\nGenerating feature importance maps...")
        important_dims = [63, 34, 123]  # The dimensions specified by the user

        for dim in important_dims:
            print(f"\nAnalyzing dimension {dim}...")

            # For PD group
            print(f"Feature map for dimension {dim} - PD group")
            importance_map_pd, _, _ = generate_vae_feature_importance_map(
                vae_model, val_loader, dimension_idx=dim, group='PD', num_samples=8)

            # For Control group
            print(f"Feature map for dimension {dim} - Control group")
            importance_map_control, _, _ = generate_vae_feature_importance_map(
                vae_model, val_loader, dimension_idx=dim, group='Control', num_samples=8)

        # 2. Visualize uncertainty in VAE reconstructions
        print("\nVisualizing VAE reconstruction uncertainty...")

        # For a single sample (PD)
        print("Single sample uncertainty visualization - PD")
        recon_mean_pd, recon_std_pd = visualize_vae_uncertainty(
            vae_model, val_loader, group='PD')

        # For a single sample (Control)
        print("Single sample uncertainty visualization - Control")
        recon_mean_control, recon_std_control = visualize_vae_uncertainty(
            vae_model, val_loader, group='Control')

        # 3. Generate full brain uncertainty maps
        print("\nGenerating full brain uncertainty maps...")

        # For all samples
        print("Full brain uncertainty map - All groups")
        avg_uncertainty, high_uncertainty_mask = generate_full_brain_uncertainty_map(
            vae_model, val_loader, num_samples=15)

        # 4. Compare uncertainty between groups
        print("\nComparing uncertainty between groups...")
        uncertainty_maps = compare_group_uncertainty(
            vae_model, val_loader, groups=['PD', 'Control', 'SWEDD'], num_samples=5)

        print("\nVAE Feature Importance and Uncertainty Analysis completed successfully!")

    except Exception as e:
        print(f"Error during analysis: {str(e)}")
        import traceback
        traceback.print_exc()

"""# Performance"""

# Cell 20: VAE Latent Space Analysis through KL Divergence

import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import scipy.stats as stats
from tqdm import tqdm
import gc

def analyze_vae_kl_dimensions(model, dataloader, max_samples=150):
    """
    Analyze the KL divergence contribution of each dimension in the VAE latent space.

    KL divergence in VAEs measures how much the encoded distribution differs from
    the prior (standard normal) distribution. Dimensions with higher KL divergence
    are more "active" and potentially encode more meaningful features.
    """
    device = next(model.parameters()).device
    model.eval()

    # Storage for per-dimension KL and sample metadata
    kl_per_dim = []
    labels = []
    paths = []

    sample_count = 0
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Analyzing KL divergence per dimension"):
            volumes = batch['volume'].to(device)
            batch_labels = batch['label']
            batch_paths = batch['path']

            # Encode samples to get mu and log_var
            mu, log_var = model.encode(volumes)

            # Calculate KL divergence per dimension: 0.5 * (mu^2 + exp(log_var) - log_var - 1)
            kl_dims = 0.5 * (mu.pow(2) + log_var.exp() - log_var - 1)

            # Store results
            kl_per_dim.append(kl_dims.cpu().numpy())
            labels.extend(batch_labels)
            paths.extend(batch_paths)

            sample_count += volumes.shape[0]

            # Memory cleanup
            del volumes, mu, log_var, kl_dims
            torch.cuda.empty_cache()

            if sample_count >= max_samples:
                break

    # Stack arrays and trim to max_samples if needed
    kl_per_dim = np.vstack(kl_per_dim)
    if max_samples and len(labels) > max_samples:
        kl_per_dim = kl_per_dim[:max_samples]
        labels = labels[:max_samples]
        paths = paths[:max_samples]

    # Analyze overall KL divergence per dimension
    mean_kl_per_dim = np.mean(kl_per_dim, axis=0)

    # Find most active dimensions (highest KL)
    top_dims_idx = np.argsort(mean_kl_per_dim)[-20:][::-1]
    top_dims_kl = mean_kl_per_dim[top_dims_idx]

    # Plot overall KL per dimension (sorted)
    plt.figure(figsize=(14, 6))
    plt.subplot(1, 2, 1)
    plt.bar(range(len(mean_kl_per_dim)), sorted(mean_kl_per_dim, reverse=True))
    plt.axhline(y=np.mean(mean_kl_per_dim), color='r', linestyle='--', label=f'Mean KL: {np.mean(mean_kl_per_dim):.4f}')
    plt.title('KL Divergence per Dimension (Sorted)')
    plt.xlabel('Dimension Index (Sorted)')
    plt.ylabel('Mean KL Divergence')
    plt.grid(alpha=0.3)
    plt.legend()

    # Plot top 20 dimensions with actual indices
    plt.subplot(1, 2, 2)
    plt.bar(top_dims_idx, top_dims_kl)
    plt.title('Top 20 Most Active Dimensions')
    plt.xlabel('Dimension Index')
    plt.ylabel('Mean KL Divergence')
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.show()

    # Analyze distribution of top dimensions across patient groups
    print(f"\nAnalyzing top 5 most active dimensions across patient groups:")

    # Get unique labels
    unique_labels = list(set(labels))

    # Prepare data for violin plots
    top5_dims = top_dims_idx[:5]

    plt.figure(figsize=(15, 8))
    for i, dim in enumerate(top5_dims):
        plt.subplot(1, 5, i+1)

        # Create a dataframe for this dimension
        dim_df = pd.DataFrame({
            'KL': kl_per_dim[:, dim],
            'Group': labels
        })

        # Create violin plot
        sns.violinplot(x='Group', y='KL', data=dim_df, palette='viridis')
        plt.title(f'Dimension {dim}')
        plt.ylabel('KL Divergence' if i == 0 else '')
        plt.grid(alpha=0.3)

        # Add ANOVA p-value to see if groups are significantly different
        groups_data = [dim_df[dim_df['Group'] == group]['KL'].values for group in unique_labels]
        f_val, p_val = stats.f_oneway(*groups_data)
        plt.annotate(f'p={p_val:.4f}', xy=(0.5, 0.9), xycoords='axes fraction', ha='center')

    plt.tight_layout()
    plt.show()

    # Test predictive power of top dimensions using a simple classifier
    print("\nTesting predictive power of top KL dimensions:")

    # Prepare train/test split (70/30)
    np.random.seed(42)
    indices = np.random.permutation(len(labels))
    train_idx, test_idx = indices[:int(0.7*len(indices))], indices[int(0.7*len(indices)):]

    # Convert labels to numeric for the classifier
    label_map = {label: i for i, label in enumerate(unique_labels)}
    y = np.array([label_map[label] for label in labels])

    # Train a random forest on different sets of dimensions
    for n_dims in [5, 10, 20, 50, 100]:
        if n_dims <= len(top_dims_idx):
            # Get top n dimensions
            selected_dims = top_dims_idx[:n_dims]
            X_selected = kl_per_dim[:, selected_dims]

            # Train model
            clf = RandomForestClassifier(n_estimators=100, random_state=42)
            clf.fit(X_selected[train_idx], y[train_idx])

            # Test model
            y_pred = clf.predict(X_selected[test_idx])
            accuracy = accuracy_score(y[test_idx], y_pred)

            print(f"Accuracy using top {n_dims} KL dimensions: {accuracy:.4f}")

            # Feature importance
            if n_dims <= 20:  # Only show for smaller sets
                importances = clf.feature_importances_
                indices = np.argsort(importances)[::-1]

                print("  Top 5 most important dimensions:")
                for i in range(min(5, n_dims)):
                    print(f"    Dim {selected_dims[indices[i]]}: {importances[indices[i]]:.4f}")

    return kl_per_dim, top_dims_idx, labels, paths

# Load the trained VAE model from previous cells
print("Loading trained VAE model...")
# Using the load_trained_vae function from Cell 18
vae_model, _ = load_trained_vae('checkpoints', 'vae_model_v2', latent_dim=256)

# Analyze VAE KL divergence per dimension
print("\nAnalyzing VAE latent dimensions through KL divergence...")
kl_per_dim, top_dims, labels, paths = analyze_vae_kl_dimensions(
    vae_model, val_loader, max_samples=150)

print("KL Divergence Analysis completed!")

# Cell 21: Visualization of Top KL Dimensions in Brain Space

# Now let's visualize what the top dimensions represent in the brain space
print("Visualizing top KL dimensions in brain space...")

# Select the top 3 dimensions with highest KL divergence
top_3_dims = top_dims[:3]
print(f"Analyzing dimensions: {top_3_dims}")

# For each dimension, visualize its effect on reconstruction
for dim_idx in top_3_dims:
    print(f"\nVisualizing dimension {dim_idx}...")

    # Use the existing function from Cell 15
    visualize_vae_latent_dimension(vae_model, val_loader, dimension_idx=dim_idx, alpha=5.0)

    # Generate feature importance map
    print(f"Generating feature importance map for dimension {dim_idx}")
    for group in ['PD', 'Control']:
        generate_vae_feature_importance_map(
            vae_model, val_loader, dimension_idx=dim_idx, group=group, num_samples=8)

print("Top dimensions visualization completed!")

# Cell 22: Metadata Dimension Analysis with Improved Visualization

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from tqdm import tqdm
import torch
import re
import scipy.stats as stats

def analyze_metadata_dimensions(ae_model, vae_model, dataloader, metadata_path="dicom_metadata.csv", max_samples=150):
    """
    Analyze how specific dimensions in the latent spaces correlate with metadata.
    """
    print("\nAnalyzing correlation between latent dimensions and metadata...")

    # Load metadata CSV
    if not os.path.exists(metadata_path):
        print(f"Error: Metadata file {metadata_path} not found!")
        return None

    # Load with proper type handling
    metadata_df = pd.read_csv(metadata_path)
    print(f"Loaded metadata with {len(metadata_df)} entries and columns: {metadata_df.columns.tolist()}")

    # Check data types and missing values
    print("\nData types in metadata:")
    print(metadata_df.dtypes)

    print("\nMissing values in metadata:")
    print(metadata_df.isnull().sum())

    # Extract latent vectors from models
    device = next(ae_model.parameters()).device
    ae_model.eval()
    vae_model.eval()

    sample_paths = []
    sample_labels = []
    ae_latent_vecs = []
    vae_latent_means = []

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Extracting latent vectors"):
            volumes = batch['volume'].to(device)
            batch_labels = batch['label']
            batch_paths = batch['path']

            # Get latent vectors
            ae_z = ae_model.encode(volumes)
            vae_mu, _ = vae_model.encode(volumes)

            # Store results
            ae_latent_vecs.append(ae_z.cpu().numpy())
            vae_latent_means.append(vae_mu.cpu().numpy())
            sample_paths.extend(batch_paths)
            sample_labels.extend(batch_labels)

            # Memory cleanup
            del volumes, ae_z, vae_mu
            torch.cuda.empty_cache()

            if len(sample_paths) >= max_samples:
                break

    # Stack arrays
    ae_latent_vecs = np.vstack(ae_latent_vecs)
    vae_latent_means = np.vstack(vae_latent_means)

    # Create dataframe with sample info
    sample_df = pd.DataFrame({
        'file_path': sample_paths,
        'group': sample_labels
    })

    # Normalize paths for matching
    sample_df['file_path_norm'] = sample_df['file_path'].apply(
        lambda x: os.path.basename(x.replace('\\', '/'))
    )
    metadata_df['file_path_norm'] = metadata_df['file_path'].apply(
        lambda x: os.path.basename(x.replace('\\', '/'))
    )

    # Extract patient IDs as a fallback matching strategy
    def extract_patient_id(path):
        path = path.replace('\\', '/')
        match = re.search(r'PPMI_(\d+)_', path)
        if match:
            return match.group(1)
        match = re.search(r'PPMI_Images_\w+/(\d+)/', path)
        if match:
            return match.group(1)
        return None

    sample_df['patient_id'] = sample_df['file_path'].apply(extract_patient_id)
    metadata_df['patient_id'] = metadata_df['file_path'].apply(extract_patient_id)

    # Try to match by filename first
    merged_df = pd.merge(sample_df, metadata_df, on='file_path_norm', how='inner', suffixes=('_sample', '_meta'))

    # If that doesn't work well, try matching by patient ID
    if len(merged_df) < 10:
        print("Filename matching failed, trying patient ID matching...")
        merged_df = pd.merge(sample_df, metadata_df, on='patient_id', how='inner', suffixes=('_sample', '_meta'))

    print(f"Successfully matched {len(merged_df)} samples with metadata")

    if len(merged_df) < 10:
        print("Error: Insufficient matches for analysis. Check file paths.")
        print("Sample patient IDs:", sample_df['patient_id'].dropna().unique()[:10])
        print("Metadata patient IDs:", metadata_df['patient_id'].dropna().unique()[:10])
        return None

    # Get indices of matched samples to extract latent vectors
    matched_indices = [sample_paths.index(path) for path in merged_df['file_path_sample']]
    ae_matched = ae_latent_vecs[matched_indices]
    vae_matched = vae_latent_means[matched_indices]

    # Analyze categorical metadata fields
    categorical_fields = ['PatientSex', 'StudyDescription', 'Manufacturer', 'ManufacturerModelName']

    # Results storage
    dimension_analysis = {
        'ae': {field: {} for field in categorical_fields},
        'vae': {field: {} for field in categorical_fields}
    }

    # Analyze each metadata field
    for field in categorical_fields:
        field_meta = field + '_meta' if field + '_meta' in merged_df.columns else field

        if field_meta not in merged_df.columns:
            print(f"Warning: Field '{field}' not found in metadata")
            continue

        print(f"\nAnalyzing field '{field}'...")

        # Check data type and handle properly
        print(f"Data type for {field_meta}: {merged_df[field_meta].dtype}")

        # Handle nulls and convert to string for consistent processing
        field_values = merged_df[field_meta].fillna('Unknown').astype(str).values

        # Now get unique values safely
        unique_values = np.unique(field_values)
        print(f"Unique values: {unique_values}")

        # Skip if less than 2 categories
        if len(unique_values) < 2:
            print(f"Skipping '{field}' - only one unique value found")
            continue

        # Count samples per category
        value_counts = pd.Series(field_values).value_counts()
        print(f"Counts per category: {dict(value_counts)}")

        # Skip categories with too few samples
        keep_values = value_counts[value_counts >= 5].index.tolist()
        if len(keep_values) < 2:
            print(f"Skipping '{field}' - insufficient samples per category")
            continue

        # Filter to keep only samples with sufficient category representation
        keep_mask = np.array([v in keep_values for v in field_values])
        if keep_mask.sum() < 10:
            print(f"Skipping '{field}' - insufficient samples after filtering")
            continue

        filtered_values = field_values[keep_mask]
        ae_filtered = ae_matched[keep_mask]
        vae_filtered = vae_matched[keep_mask]

        print(f"Analyzing with {len(filtered_values)} samples across {len(keep_values)} categories")

        # Calculate dimension scores
        ae_scores = np.zeros(ae_filtered.shape[1])
        vae_scores = np.zeros(vae_filtered.shape[1])
        ae_p_values = np.ones(ae_filtered.shape[1])
        vae_p_values = np.ones(vae_filtered.shape[1])

        # Convert categorical values to numeric for analysis
        value_map = {val: i for i, val in enumerate(keep_values)}
        y = np.array([value_map[val] for val in filtered_values])

        # DIMENSION SELECTION METHOD:
        # Using ANOVA F-statistic to measure how well each dimension separates different categories
        for dim in range(ae_filtered.shape[1]):
            try:
                # Group data by category for ANOVA
                groups = [ae_filtered[filtered_values == val, dim] for val in keep_values]
                # Some groups might be empty or have only one sample
                if all(len(g) > 1 for g in groups):
                    f_val, p_val = stats.f_oneway(*groups)
                    ae_scores[dim] = f_val
                    ae_p_values[dim] = p_val
                else:
                    ae_scores[dim] = 0
                    ae_p_values[dim] = 1.0
            except Exception as e:
                print(f"Error analyzing AE dimension {dim}: {e}")
                ae_scores[dim] = 0
                ae_p_values[dim] = 1.0

        for dim in range(vae_filtered.shape[1]):
            try:
                groups = [vae_filtered[filtered_values == val, dim] for val in keep_values]
                if all(len(g) > 1 for g in groups):
                    f_val, p_val = stats.f_oneway(*groups)
                    vae_scores[dim] = f_val
                    vae_p_values[dim] = p_val
                else:
                    vae_scores[dim] = 0
                    vae_p_values[dim] = 1.0
            except Exception as e:
                print(f"Error analyzing VAE dimension {dim}: {e}")
                vae_scores[dim] = 0
                vae_p_values[dim] = 1.0

        # Find top dimensions (highest F-statistic and p < 0.05)
        ae_signif_dims = np.where(ae_p_values < 0.05)[0]
        vae_signif_dims = np.where(vae_p_values < 0.05)[0]

        ae_top_dims = ae_signif_dims[np.argsort(ae_scores[ae_signif_dims])[::-1][:10]]
        vae_top_dims = vae_signif_dims[np.argsort(vae_scores[vae_signif_dims])[::-1][:10]]

        print(f"\nTop 10 AE dimensions for {field}:")
        for i, dim in enumerate(ae_top_dims):
            print(f"  Dimension {dim}: F-statistic={ae_scores[dim]:.2f}, p-value={ae_p_values[dim]:.4f}")

        print(f"\nTop 10 VAE dimensions for {field}:")
        for i, dim in enumerate(vae_top_dims):
            print(f"  Dimension {dim}: F-statistic={vae_scores[dim]:.2f}, p-value={vae_p_values[dim]:.4f}")

        # Visualize top dimensions with separate plots for AE and VAE
        if len(ae_top_dims) > 0 or len(vae_top_dims) > 0:
            # Abbreviate labels if needed
            labels = []
            for val in keep_values:
                if len(val) > 20:  # If label is too long
                    shortened = val[:17] + "..."
                else:
                    shortened = val
                labels.append(shortened)

            # AE DIMENSIONS PLOT (if we have significant dimensions)
            if len(ae_top_dims) > 0:
                plt.figure(figsize=(20, 8))
                plt.suptitle(f"AE Dimensions Correlated with {field}", fontsize=18)

                # Plot up to 3 top AE dimensions
                for i, dim in enumerate(ae_top_dims[:3]):
                    if i < 3:  # Only plot first 3
                        plt.subplot(1, 3, i+1)

                        # Create boxplot with clear labels (no colors)
                        data = []
                        for val in keep_values:
                            mask = filtered_values == val
                            data.append(ae_filtered[mask, dim])

                        # Standard boxplot without colors
                        plt.boxplot(data)

                        plt.title(f'AE Dimension {dim}', fontsize=14)
                        plt.ylabel('Dimension Value', fontsize=12)

                        # Set x-ticks with increased spacing
                        plt.xticks(range(1, len(labels)+1), labels, rotation=90, fontsize=10)
                        plt.tick_params(axis='x', which='major', pad=15)  # Add padding for better readability
                        plt.grid(axis='y', alpha=0.3)

                # Ensure there's plenty of space
                plt.tight_layout()
                plt.subplots_adjust(bottom=0.4, wspace=0.3)
                plt.show()

            # VAE DIMENSIONS PLOT (if we have significant dimensions)
            if len(vae_top_dims) > 0:
                plt.figure(figsize=(20, 8))
                plt.suptitle(f"VAE Dimensions Correlated with {field}", fontsize=18)

                # Plot up to 3 top VAE dimensions
                for i, dim in enumerate(vae_top_dims[:3]):
                    if i < 3:  # Only plot first 3
                        plt.subplot(1, 3, i+1)

                        # Create boxplot with clear labels (no colors)
                        data = []
                        for val in keep_values:
                            mask = filtered_values == val
                            data.append(vae_filtered[mask, dim])

                        # Standard boxplot without colors
                        plt.boxplot(data)

                        plt.title(f'VAE Dimension {dim}', fontsize=14)
                        plt.ylabel('Dimension Value', fontsize=12)

                        # Set x-ticks with increased spacing
                        plt.xticks(range(1, len(labels)+1), labels, rotation=90, fontsize=10)
                        plt.tick_params(axis='x', which='major', pad=15)  # Add padding for better readability
                        plt.grid(axis='y', alpha=0.3)

                # Ensure there's plenty of space
                plt.tight_layout()
                plt.subplots_adjust(bottom=0.4, wspace=0.3)
                plt.show()

        # Store results
        dimension_analysis['ae'][field] = {
            'scores': ae_scores,
            'p_values': ae_p_values,
            'top_dims': ae_top_dims.tolist()
        }

        dimension_analysis['vae'][field] = {
            'scores': vae_scores,
            'p_values': vae_p_values,
            'top_dims': vae_top_dims.tolist()
        }

    return dimension_analysis

# Load both models
print("Loading trained models...")
ae_model, _ = load_trained_model('checkpoints', 'autoencoder_v1', latent_dim=256)
vae_model, _ = load_trained_vae('checkpoints', 'vae_model_v2', latent_dim=256)

# Run the dimension analysis
print("\nAnalyzing which dimensions correlate with metadata...")
dimension_results = analyze_metadata_dimensions(
    ae_model, vae_model, val_loader,
    metadata_path="dicom_metadata.csv", max_samples=150)

print("\nDimension analysis completed!")

# Cell 22: Metadata Dimension Analysis with Standardized Y-Axis Scales

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from tqdm import tqdm
import torch
import re
import scipy.stats as stats

def analyze_metadata_dimensions(ae_model, vae_model, dataloader, metadata_path="dicom_metadata.csv", max_samples=150):
    """
    Analyze how specific dimensions in the latent spaces correlate with metadata.
    """
    print("\nAnalyzing correlation between latent dimensions and metadata...")

    # Load metadata CSV
    if not os.path.exists(metadata_path):
        print(f"Error: Metadata file {metadata_path} not found!")
        return None

    # Load with proper type handling
    metadata_df = pd.read_csv(metadata_path)
    print(f"Loaded metadata with {len(metadata_df)} entries and columns: {metadata_df.columns.tolist()}")

    # Check data types and missing values
    print("\nData types in metadata:")
    print(metadata_df.dtypes)

    print("\nMissing values in metadata:")
    print(metadata_df.isnull().sum())

    # Extract latent vectors from models
    device = next(ae_model.parameters()).device
    ae_model.eval()
    vae_model.eval()

    sample_paths = []
    sample_labels = []
    ae_latent_vecs = []
    vae_latent_means = []

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Extracting latent vectors"):
            volumes = batch['volume'].to(device)
            batch_labels = batch['label']
            batch_paths = batch['path']

            # Get latent vectors
            ae_z = ae_model.encode(volumes)
            vae_mu, _ = vae_model.encode(volumes)

            # Store results
            ae_latent_vecs.append(ae_z.cpu().numpy())
            vae_latent_means.append(vae_mu.cpu().numpy())
            sample_paths.extend(batch_paths)
            sample_labels.extend(batch_labels)

            # Memory cleanup
            del volumes, ae_z, vae_mu
            torch.cuda.empty_cache()

            if len(sample_paths) >= max_samples:
                break

    # Stack arrays
    ae_latent_vecs = np.vstack(ae_latent_vecs)
    vae_latent_means = np.vstack(vae_latent_means)

    # Create dataframe with sample info
    sample_df = pd.DataFrame({
        'file_path': sample_paths,
        'group': sample_labels
    })

    # Normalize paths for matching
    sample_df['file_path_norm'] = sample_df['file_path'].apply(
        lambda x: os.path.basename(x.replace('\\', '/'))
    )
    metadata_df['file_path_norm'] = metadata_df['file_path'].apply(
        lambda x: os.path.basename(x.replace('\\', '/'))
    )

    # Extract patient IDs as a fallback matching strategy
    def extract_patient_id(path):
        path = path.replace('\\', '/')
        match = re.search(r'PPMI_(\d+)_', path)
        if match:
            return match.group(1)
        match = re.search(r'PPMI_Images_\w+/(\d+)/', path)
        if match:
            return match.group(1)
        return None

    sample_df['patient_id'] = sample_df['file_path'].apply(extract_patient_id)
    metadata_df['patient_id'] = metadata_df['file_path'].apply(extract_patient_id)

    # Try to match by filename first
    merged_df = pd.merge(sample_df, metadata_df, on='file_path_norm', how='inner', suffixes=('_sample', '_meta'))

    # If that doesn't work well, try matching by patient ID
    if len(merged_df) < 10:
        print("Filename matching failed, trying patient ID matching...")
        merged_df = pd.merge(sample_df, metadata_df, on='patient_id', how='inner', suffixes=('_sample', '_meta'))

    print(f"Successfully matched {len(merged_df)} samples with metadata")

    if len(merged_df) < 10:
        print("Error: Insufficient matches for analysis. Check file paths.")
        print("Sample patient IDs:", sample_df['patient_id'].dropna().unique()[:10])
        print("Metadata patient IDs:", metadata_df['patient_id'].dropna().unique()[:10])
        return None

    # Get indices of matched samples to extract latent vectors
    matched_indices = [sample_paths.index(path) for path in merged_df['file_path_sample']]
    ae_matched = ae_latent_vecs[matched_indices]
    vae_matched = vae_latent_means[matched_indices]

    # Analyze categorical metadata fields
    categorical_fields = ['PatientSex', 'StudyDescription', 'Manufacturer', 'ManufacturerModelName']

    # Results storage
    dimension_analysis = {
        'ae': {field: {} for field in categorical_fields},
        'vae': {field: {} for field in categorical_fields}
    }

    # Analyze each metadata field
    for field in categorical_fields:
        field_meta = field + '_meta' if field + '_meta' in merged_df.columns else field

        if field_meta not in merged_df.columns:
            print(f"Warning: Field '{field}' not found in metadata")
            continue

        print(f"\nAnalyzing field '{field}'...")

        # Check data type and handle properly
        print(f"Data type for {field_meta}: {merged_df[field_meta].dtype}")

        # Handle nulls and convert to string for consistent processing
        field_values = merged_df[field_meta].fillna('Unknown').astype(str).values

        # Now get unique values safely
        unique_values = np.unique(field_values)
        print(f"Unique values: {unique_values}")

        # Skip if less than 2 categories
        if len(unique_values) < 2:
            print(f"Skipping '{field}' - only one unique value found")
            continue

        # Count samples per category
        value_counts = pd.Series(field_values).value_counts()
        print(f"Counts per category: {dict(value_counts)}")

        # Skip categories with too few samples
        keep_values = value_counts[value_counts >= 5].index.tolist()
        if len(keep_values) < 2:
            print(f"Skipping '{field}' - insufficient samples per category")
            continue

        # Filter to keep only samples with sufficient category representation
        keep_mask = np.array([v in keep_values for v in field_values])
        if keep_mask.sum() < 10:
            print(f"Skipping '{field}' - insufficient samples after filtering")
            continue

        filtered_values = field_values[keep_mask]
        ae_filtered = ae_matched[keep_mask]
        vae_filtered = vae_matched[keep_mask]

        print(f"Analyzing with {len(filtered_values)} samples across {len(keep_values)} categories")

        # Calculate dimension scores
        ae_scores = np.zeros(ae_filtered.shape[1])
        vae_scores = np.zeros(vae_filtered.shape[1])
        ae_p_values = np.ones(ae_filtered.shape[1])
        vae_p_values = np.ones(vae_filtered.shape[1])

        # Convert categorical values to numeric for analysis
        value_map = {val: i for i, val in enumerate(keep_values)}
        y = np.array([value_map[val] for val in filtered_values])

        # DIMENSION SELECTION METHOD:
        # Using ANOVA F-statistic to measure how well each dimension separates different categories
        for dim in range(ae_filtered.shape[1]):
            try:
                # Group data by category for ANOVA
                groups = [ae_filtered[filtered_values == val, dim] for val in keep_values]
                # Some groups might be empty or have only one sample
                if all(len(g) > 1 for g in groups):
                    f_val, p_val = stats.f_oneway(*groups)
                    ae_scores[dim] = f_val
                    ae_p_values[dim] = p_val
                else:
                    ae_scores[dim] = 0
                    ae_p_values[dim] = 1.0
            except Exception as e:
                print(f"Error analyzing AE dimension {dim}: {e}")
                ae_scores[dim] = 0
                ae_p_values[dim] = 1.0

        for dim in range(vae_filtered.shape[1]):
            try:
                groups = [vae_filtered[filtered_values == val, dim] for val in keep_values]
                if all(len(g) > 1 for g in groups):
                    f_val, p_val = stats.f_oneway(*groups)
                    vae_scores[dim] = f_val
                    vae_p_values[dim] = p_val
                else:
                    vae_scores[dim] = 0
                    vae_p_values[dim] = 1.0
            except Exception as e:
                print(f"Error analyzing VAE dimension {dim}: {e}")
                vae_scores[dim] = 0
                vae_p_values[dim] = 1.0

        # Find top dimensions (highest F-statistic and p < 0.05)
        ae_signif_dims = np.where(ae_p_values < 0.05)[0]
        vae_signif_dims = np.where(vae_p_values < 0.05)[0]

        ae_top_dims = ae_signif_dims[np.argsort(ae_scores[ae_signif_dims])[::-1][:10]]
        vae_top_dims = vae_signif_dims[np.argsort(vae_scores[vae_signif_dims])[::-1][:10]]

        print(f"\nTop 10 AE dimensions for {field}:")
        for i, dim in enumerate(ae_top_dims):
            print(f"  Dimension {dim}: F-statistic={ae_scores[dim]:.2f}, p-value={ae_p_values[dim]:.4f}")

        print(f"\nTop 10 VAE dimensions for {field}:")
        for i, dim in enumerate(vae_top_dims):
            print(f"  Dimension {dim}: F-statistic={vae_scores[dim]:.2f}, p-value={vae_p_values[dim]:.4f}")

        # Visualize top dimensions with separate plots for AE and VAE
        if len(ae_top_dims) > 0 or len(vae_top_dims) > 0:
            # Abbreviate labels if needed
            labels = []
            for val in keep_values:
                if len(val) > 20:  # If label is too long
                    shortened = val[:17] + "..."
                else:
                    shortened = val
                labels.append(shortened)

            # STANDARDIZED Y-AXIS RANGES
            # Calculate global min/max for all top AE dimensions to be plotted
            ae_data_for_plot = []
            for dim in ae_top_dims[:3]:
                for val in keep_values:
                    mask = filtered_values == val
                    ae_data_for_plot.extend(ae_filtered[mask, dim])

            # Calculate global min/max for all top VAE dimensions to be plotted
            vae_data_for_plot = []
            for dim in vae_top_dims[:3]:
                for val in keep_values:
                    mask = filtered_values == val
                    vae_data_for_plot.extend(vae_filtered[mask, dim])

            # Determine y-axis ranges with 10% padding
            if ae_data_for_plot:
                ae_min = np.min(ae_data_for_plot)
                ae_max = np.max(ae_data_for_plot)
                ae_range = ae_max - ae_min
                ae_y_min = ae_min - 0.1 * ae_range
                ae_y_max = ae_max + 0.1 * ae_range

            if vae_data_for_plot:
                vae_min = np.min(vae_data_for_plot)
                vae_max = np.max(vae_data_for_plot)
                vae_range = vae_max - vae_min
                vae_y_min = vae_min - 0.1 * vae_range
                vae_y_max = vae_max + 0.1 * vae_range

            # AE DIMENSIONS PLOT (if we have significant dimensions)
            if len(ae_top_dims) > 0:
                plt.figure(figsize=(20, 8))
                plt.suptitle(f"AE Dimensions Correlated with {field}", fontsize=18)

                # Plot up to 3 top AE dimensions
                for i, dim in enumerate(ae_top_dims[:3]):
                    if i < 3:  # Only plot first 3
                        plt.subplot(1, 3, i+1)

                        # Create boxplot with clear labels (no colors)
                        data = []
                        for val in keep_values:
                            mask = filtered_values == val
                            data.append(ae_filtered[mask, dim])

                        # Standard boxplot without colors
                        plt.boxplot(data)

                        plt.title(f'AE Dimension {dim}', fontsize=14)
                        plt.ylabel('Dimension Value', fontsize=12)

                        # Set x-ticks with increased spacing
                        plt.xticks(range(1, len(labels)+1), labels, rotation=90, fontsize=10)
                        plt.tick_params(axis='x', which='major', pad=15)
                        plt.grid(axis='y', alpha=0.3)

                        # Set consistent y-axis limits
                        plt.ylim(ae_y_min, ae_y_max)

                # Ensure there's plenty of space
                plt.tight_layout()
                plt.subplots_adjust(bottom=0.4, wspace=0.3)
                plt.show()

            # VAE DIMENSIONS PLOT (if we have significant dimensions)
            if len(vae_top_dims) > 0:
                plt.figure(figsize=(20, 8))
                plt.suptitle(f"VAE Dimensions Correlated with {field}", fontsize=18)

                # Plot up to 3 top VAE dimensions
                for i, dim in enumerate(vae_top_dims[:3]):
                    if i < 3:  # Only plot first 3
                        plt.subplot(1, 3, i+1)

                        # Create boxplot with clear labels (no colors)
                        data = []
                        for val in keep_values:
                            mask = filtered_values == val
                            data.append(vae_filtered[mask, dim])

                        # Standard boxplot without colors
                        plt.boxplot(data)

                        plt.title(f'VAE Dimension {dim}', fontsize=14)
                        plt.ylabel('Dimension Value', fontsize=12)

                        # Set x-ticks with increased spacing
                        plt.xticks(range(1, len(labels)+1), labels, rotation=90, fontsize=10)
                        plt.tick_params(axis='x', which='major', pad=15)
                        plt.grid(axis='y', alpha=0.3)

                        # Set consistent y-axis limits
                        plt.ylim(vae_y_min, vae_y_max)

                # Ensure there's plenty of space
                plt.tight_layout()
                plt.subplots_adjust(bottom=0.4, wspace=0.3)
                plt.show()

        # Store results
        dimension_analysis['ae'][field] = {
            'scores': ae_scores,
            'p_values': ae_p_values,
            'top_dims': ae_top_dims.tolist()
        }

        dimension_analysis['vae'][field] = {
            'scores': vae_scores,
            'p_values': vae_p_values,
            'top_dims': vae_top_dims.tolist()
        }

    return dimension_analysis

# Load both models
print("Loading trained models...")
ae_model, _ = load_trained_model('checkpoints', 'autoencoder_v1', latent_dim=256)
vae_model, _ = load_trained_vae('checkpoints', 'vae_model_v2', latent_dim=256)

# Run the dimension analysis
print("\nAnalyzing which dimensions correlate with metadata...")
dimension_results = analyze_metadata_dimensions(
    ae_model, vae_model, val_loader,
    metadata_path="dicom_metadata.csv", max_samples=150)

print("\nDimension analysis completed!")